<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>技术博客 on</title><link>https://openksc.github.io/zh/blogs/</link><description>Recent content in 技术博客 on</description><generator>Hugo</generator><language>zh-CN</language><atom:link href="https://openksc.github.io/zh/blogs/index.xml" rel="self" type="application/rss+xml"/><item><title>33 张高清大图，带你玩转 KubeSphere 4.1.2 部署与扩展组件安装</title><link>https://openksc.github.io/zh/blogs/kubesphere-4.1.2-deployment-and-extension-installation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-4.1.2-deployment-and-extension-installation/</guid><description>&lt;p>备受瞩目的 KubeSphere 4.1.2 已经正式官宣发布，该版本带来了一个重大优化：增加默认的扩展组件仓库。&lt;/p>
&lt;p>这一优化改进，让采用全新的 KubeSphere LuBan 架构的 KubeSphere，真正实现了自由打造高度可扩展和可配置的云原生底座。&lt;/p>
&lt;p>KubeSphere 用户仅需要在 K8s 之上，默认安装清爽简洁、最小化的 KubeSphere Core。后续，基于扩展机制，将常用的功能拆解、封装成一个个可配置和可扩展的组件式应用模块。根据业务场景自由选择组合合适的 KubeSphere 扩展组件。同时，还能将自己的应用无缝融入到 KubeSphere 控制台。最终实现云原生容器云管理平台千人千面的效果。&lt;/p>
&lt;p>本文为您提供一份详尽的实战手册，通过逐步操作，助您完成以下核心任务：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>安装 K8s 集群&lt;/strong>：使用 KubeKey，从零开始构建高可用的 K8s 集群。&lt;/li>
&lt;li>&lt;strong>集成 NFS 持久化存储&lt;/strong>：无缝对接网络文件系统 (NFS)，为您的集群提供持久化存储解决方案。&lt;/li>
&lt;li>&lt;strong>部署 KubeSphere 4.1.2&lt;/strong>：在您的集群上安装最新版本的 KubeSphere，解锁强大的多租户容器云管理平台。&lt;/li>
&lt;li>&lt;strong>配置扩展组件&lt;/strong>：安装并配置所需扩展组件，增强 K8s 集群的功能。&lt;/li>
&lt;/ul>
&lt;p>通过本指南，您将学会构建一个基于 KubeSphere、可视化管理的高功能、可扩展、高可用的 K8s 集群。清晰的步骤和详尽的图解让每一步都易于操作，确保您能够轻松掌握并成功实施。&lt;/p>
&lt;p>&lt;strong>实战服务器配置(架构 1:1 复刻小规模生产环境，只是配置略有不同)&lt;/strong>&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.161&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.162&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.163&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage&lt;/td>
 &lt;td style="text-align: center">192.168.9.164&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">500&lt;/td>
 &lt;td style="text-align: center">NFS-Storage&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">26&lt;/td>
 &lt;td style="text-align: center">52&lt;/td>
 &lt;td style="text-align: center">160&lt;/td>
 &lt;td style="text-align: center">800&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>实战环境涉及软件版本信息&lt;/strong>&lt;/p></description></item><item><title>59 张高清大图，带你实战入门 KubeSphere DevOps</title><link>https://openksc.github.io/zh/blogs/kubesphere-devops-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-devops-guide/</guid><description>&lt;p>KubeSphere 基于 &lt;a href="https://jenkins.io/" target="_blank" rel="noopener noreferrer">Jenkins&lt;/a> 的 DevOps 系统是专为 Kubernetes 中的 CI/CD 工作流设计的，它提供了一站式的解决方案，帮助开发和运维团队用非常简单的方式构建、测试和发布应用到 Kubernetes。它还具有插件管理、&lt;a href="https://kubesphere.com.cn/docs/v3.3/project-user-guide/image-builder/binary-to-image/" target="_blank" rel="noopener noreferrer">Binary-to-Image (B2I)&lt;/a>、&lt;a href="https://kubesphere.com.cn/docs/v3.3/project-user-guide/image-builder/source-to-image/" target="_blank" rel="noopener noreferrer">Source-to-Image (S2I)&lt;/a>、代码依赖缓存、代码质量分析、流水线日志等功能。&lt;/p>
&lt;p>DevOps 系统为用户提供了一个自动化的环境，应用可以自动发布到同一个平台。它还兼容第三方私有镜像仓库（如 Harbor）和代码库（如 GitLab/GitHub/SVN/BitBucket）。它为用户提供了全面的、可视化的 CI/CD 流水线，打造了极佳的用户体验，而且这种兼容性强的流水线能力在离线环境中非常有用。&lt;/p>
&lt;p>本文档旨在成为您的技术指南，逐步引导您开启 KubeSphere 的 DevOps 之旅。我们将深入探索如何开启 DevOps 插件，如何规划设计一个完整的 DevOps 流水线并编写 Jenkins 流水线配置文件。通过本文档的实战案例，您将能够掌握从理论到实践的全过程，为您的项目带来持续集成和持续部署的自动化体验。&lt;/p>
&lt;ul>
&lt;li>您将学习如何在 KubeSphere 上开启 DevOps 插件。&lt;/li>
&lt;li>通过实际案例，规划设计一个高效、自动化的 DevOps 流水线。&lt;/li>
&lt;li>我们将一起编写 Jenkinsfile，定义代码拉取、测试、编译、构建和部署的流程。&lt;/li>
&lt;li>最终，我们将完成一个实战项目，将理论知识转化为实际操作，让您对 KubeSphere DevOps 的应用有更深的理解。&lt;/li>
&lt;/ul>
&lt;p>随着 &lt;strong>59 张高清大图&lt;/strong>的辅助，本文档将确保您在每一个步骤中都能获得清晰的指导和深刻的见解。无论您是 DevOps 的新手还是希望在 KubeSphere 上实现 DevOps 流程的老手，本文档都将为您提供宝贵的知识和实践技巧。&lt;/p>
&lt;p>&lt;strong>实战服务器配置(架构 1:1 复刻小规模生产环境，配置略有不同)&lt;/strong>&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ksp-registry&lt;/td>
 &lt;td style="text-align: center">192.168.9.90&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Harbor 镜像仓库&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.94&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker/CI&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">400+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/Longhorn/Ceph/NFS&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.98&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">300+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/Longhorn/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.99&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">300+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/Longhorn/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gpu-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.101&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker(GPU NVIDIA Tesla M40 24G)&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gpu-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.102&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker(GPU NVIDIA Tesla P100 16G)&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gateway-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.103&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;td style="text-align: center">自建应用服务代理网关/VIP：192.168.9.100&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gateway-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.104&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;td style="text-align: center">自建应用服务代理网关/VIP：192.168.9.100&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-mid&lt;/td>
 &lt;td style="text-align: center">192.168.9.105&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">部署在 k8s 集群之外的服务节点（Gitlab 等）&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">15&lt;/td>
 &lt;td style="text-align: center">68&lt;/td>
 &lt;td style="text-align: center">152&lt;/td>
 &lt;td style="text-align: center">600&lt;/td>
 &lt;td style="text-align: center">2100+&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>实战环境涉及软件版本信息&lt;/strong>&lt;/p></description></item><item><title>ARM 版 Kylin V10 部署 KubeSphere 3.4.0 不完全指南</title><link>https://openksc.github.io/zh/blogs/deploy-kubesphere-on-arm-kylin-v10/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deploy-kubesphere-on-arm-kylin-v10/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;h3 id="知识点">知识点&lt;/h3>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>KubeKey 安装部署 ARM 版 KubeSphere 和 Kubernetes&lt;/li>
&lt;li>ARM 版麒麟 V10 安装部署 KubeSphere 和 Kubernetes 常见问题&lt;/li>
&lt;/ul>
&lt;h3 id="实战服务器配置-个人云上测试服务器">实战服务器配置 (个人云上测试服务器)&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ksp-master-1&lt;/td>
 &lt;td style="text-align: center">172.16.33.16&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-master-2&lt;/td>
 &lt;td style="text-align: center">172.16.33.22&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-master-3&lt;/td>
 &lt;td style="text-align: center">172.16.33.23&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">3&lt;/td>
 &lt;td style="text-align: center">24&lt;/td>
 &lt;td style="text-align: center">48&lt;/td>
 &lt;td style="text-align: center">150&lt;/td>
 &lt;td style="text-align: center">600&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h3 id="实战环境涉及软件版本信息">实战环境涉及软件版本信息&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>服务器芯片：&lt;strong>Kunpeng-920&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>操作系统：&lt;strong>麒麟 V10 SP2 aarch64&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>KubeSphere：&lt;strong>v3.4.0&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Kubernetes：&lt;strong>v1.26.5&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Containerd：&lt;strong>1.6.4&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>KubeKey: &lt;strong>v3.0.13&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="1-本文简介">1. 本文简介&lt;/h2>
&lt;p>本文介绍了如何在 &lt;strong>麒麟 V10 aarch64&lt;/strong> 架构服务器上部署 KubeSphere 和 Kubernetes 集群。我们将使用 KubeSphere 开发的 KubeKey 工具实现自动化部署，在三台服务器上实现高可用模式最小化部署 Kubernetes 集群和 KubeSphere。&lt;/p></description></item><item><title>ARM 版 openEuler 22.03 部署 KubeSphere 3.4.0 不完全指南续篇</title><link>https://openksc.github.io/zh/blogs/deploy-kubesphere-v3.4.0-on-arm-openeuler-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deploy-kubesphere-v3.4.0-on-arm-openeuler-2/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;h3 id="知识点">知识点&lt;/h3>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>KubeKey 安装部署 ARM 版 KubeSphere 和 Kubernetes&lt;/li>
&lt;li>ARM 版 KubeSphere 和 Kubernetes 常见问题&lt;/li>
&lt;/ul>
&lt;h3 id="实战服务器配置-个人云上测试服务器">实战服务器配置 (个人云上测试服务器)&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-1&lt;/td>
 &lt;td style="text-align: center">172.16.33.16&lt;/td>
 &lt;td style="text-align: center">6&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-2&lt;/td>
 &lt;td style="text-align: center">172.16.33.22&lt;/td>
 &lt;td style="text-align: center">6&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-3&lt;/td>
 &lt;td style="text-align: center">172.16.33.23&lt;/td>
 &lt;td style="text-align: center">6&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">10&lt;/td>
 &lt;td style="text-align: center">18&lt;/td>
 &lt;td style="text-align: center">48&lt;/td>
 &lt;td style="text-align: center">150&lt;/td>
 &lt;td style="text-align: center">600+&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h3 id="实战环境涉及软件版本信息">实战环境涉及软件版本信息&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>服务器芯片：&lt;strong>Kunpeng-920&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>操作系统：&lt;strong>openEuler 22.03 LTS SP2 aarch64&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>KubeSphere：&lt;strong>v3.4.0&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Kubernetes：&lt;strong>v1.26.5&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Containerd：&lt;strong>1.6.4&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>KubeKey: &lt;strong>v3.0.10&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="1-本文简介">1. 本文简介&lt;/h2>
&lt;p>本文是 &lt;a href="https://kubesphere.io/zh/blogs/deploy-kubesphere-v3.4.0-on-arm-openeuler/" target="_blank" rel="noopener noreferrer">ARM 版 openEuler 22.03 部署 KubeSphere 3.4.0 不完全指南&lt;/a> 的续集，受限于字符数量限制，将完整的文档拆成了两篇。&lt;/p></description></item><item><title>ARM 版 openEuler 22.03 部署 KubeSphere v3.4.0 不完全指南</title><link>https://openksc.github.io/zh/blogs/deploy-kubesphere-v3.4.0-on-arm-openeuler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deploy-kubesphere-v3.4.0-on-arm-openeuler/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;h3 id="知识点">知识点&lt;/h3>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>KubeKey 安装部署 ARM 版 KubeSphere 和 Kubernetes&lt;/li>
&lt;li>ARM 版 KubeSphere 和 Kubernetes 常见问题&lt;/li>
&lt;/ul>
&lt;h3 id="实战服务器配置-个人云上测试服务器">实战服务器配置 (个人云上测试服务器)&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-1&lt;/td>
 &lt;td style="text-align: center">172.16.33.16&lt;/td>
 &lt;td style="text-align: center">6&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-2&lt;/td>
 &lt;td style="text-align: center">172.16.33.22&lt;/td>
 &lt;td style="text-align: center">6&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-3&lt;/td>
 &lt;td style="text-align: center">172.16.33.23&lt;/td>
 &lt;td style="text-align: center">6&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">10&lt;/td>
 &lt;td style="text-align: center">18&lt;/td>
 &lt;td style="text-align: center">48&lt;/td>
 &lt;td style="text-align: center">150&lt;/td>
 &lt;td style="text-align: center">600+&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h3 id="实战环境涉及软件版本信息">实战环境涉及软件版本信息&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>服务器芯片：&lt;strong>Kunpeng-920&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>操作系统：&lt;strong>openEuler 22.03 LTS SP2 aarch64&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>KubeSphere：&lt;strong>v3.4.0&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Kubernetes：&lt;strong>v1.26.5&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Containerd：&lt;strong>1.6.4&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>KubeKey: &lt;strong>v3.0.10&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="1-本文简介">1. 本文简介&lt;/h2>
&lt;p>本文介绍了如何在 &lt;strong>openEuler 22.03 LTS SP2 aarch64&lt;/strong> 架构服务器上部署 KubeSphere 和 Kubernetes 集群。我们将使用 KubeSphere 开发的 KubeKey 工具实现自动化部署，在三台服务器上实现高可用模式最小化部署 Kubernetes 集群和 KubeSphere。&lt;/p></description></item><item><title>Calico 路由反射模式权威指南</title><link>https://openksc.github.io/zh/blogs/calico-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/calico-guide/</guid><description>&lt;h2 id="概述">概述&lt;/h2>
&lt;p>作为 Kubernetes 最长使用的一种网络插件，Calico 具有很强的扩展性，较优的资源利用和较少的依赖，相较于 Flannel 插件采用 Overlay 的网络，Calico 可以通过三层路由的方式采用性能更佳的 Underlay 网络，Calico 网络插件的转发效率是所有方案中较高的。&lt;/p>
&lt;p>在使用 Calico 网络插件的实际生产环境当中，为了提高网络的性能和灵活性，需要将 K8s 的工作节点和物理网络中的 leaf 交换机建立 BGP 邻居关系，同步 BGP 路由信息，可以将 pod 网络的路由发布到物理网络中。Calico 给出了三种类型的 BGP 互联方案，分别是 &lt;strong>Full-mesh&lt;/strong>、&lt;strong>Route reflectors&lt;/strong> 和 &lt;strong>Top of Rack (ToR)&lt;/strong>。&lt;/p>
&lt;h3 id="full-mesh">Full-mesh&lt;/h3>
&lt;p>全互联模式，启用了 BGP 之后，Calico 的默认行为是在每个节点彼此对等的情况下创建完整的内部 BGP（iBGP）连接，这使 Calico 可以在任何 L2 网络（无论是公有云还是私有云）上运行，或者说（如果配了 IPIP）可以在任何不禁止 IPIP 流量的网络上作为 Overlay 运行。对于 vxlan overlay，Calico 不使用 BGP。&lt;/p>
&lt;p>Full-mesh 模式对于 100 个以内的工作节点或更少节点的中小规模部署非常有用，但是在较大的规模上，Full-mesh 模式效率会降低，较大规模情况下，Calico 官方建议使用 Route reflectors。&lt;/p>
&lt;h3 id="route-reflectors">Route reflectors&lt;/h3>
&lt;p>如果想构建内部 BGP（iBGP）大规模集群，可以使用 BGP 路由反射器来减少每个节点上使用 BGP 对等体的数量。在此模型中，某些节点充当路由反射器，并配置为在它们之间建立完整的网格。然后，将其他节点配置为与这些路由反射器的子集（通常为冗余，通常为 2 个）进行对等，从而与全网格相比减少了 BGP 对等连接的总数。&lt;/p></description></item><item><title>Cilium 1.11 发布，带来内核级服务网格、拓扑感知路由....</title><link>https://openksc.github.io/zh/blogs/cilium-1.11-release/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/cilium-1.11-release/</guid><description>&lt;blockquote>
&lt;p>原文链接： &lt;a href="https://isovalent.com/blog/post/2021-12-release-111" target="_blank" rel="noopener noreferrer">https://isovalent.com/blog/post/2021-12-release-111&lt;/a>&lt;/p>&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>作者：Cilium 母公司 Isovalent 团队&lt;/strong>
&lt;strong>译者：范彬，狄卫华，米开朗基杨&lt;/strong>&lt;/p>
&lt;p>注：本文已取得作者本人的翻译授权！&lt;/p>&lt;/blockquote>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202112141058286.png" alt="">&lt;/p>
&lt;p>Cilium 项目已逐渐成为万众瞩目之星，我们很自豪能够成为该项目的核心人员。几天前，我们发布了具有诸多新功能的 Cilium 1.11 版本，这是一个令人兴奋的版本。诸多新功能中也包括了万众期待的 Cilium Service Mesh 的 Beta 版本。在本篇文章中，我们将深入探讨其中的部分新功能。&lt;/p>
&lt;h2 id="service-mesh-测试版本beta">Service Mesh 测试版本（Beta）&lt;/h2>
&lt;p>在探讨 1.11 版本之前，让我们先了解一下 Cilium 社区宣布的 Service Mesh 的新功能。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202112141059276.png" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>基于 eBPF 技术的 Service Mesh （Beta）版本&lt;/strong>： 定义了新的服务网格能力，这包括 L7 流量管理和负载均衡、TLS 终止、金丝雀发布、追踪等诸多能力。&lt;/li>
&lt;li>&lt;strong>集成了 Kubernetes Ingress (Beta) 功能&lt;/strong>： 通过将 eBPF 和 Envoy 的强势联合，实现了对 Kubernetes Ingress 的支持。&lt;/li>
&lt;/ul>
&lt;p>Cilium 网站的一篇文章详细介绍了&lt;a href="https://cilium.io/blog/2021/12/01/cilium-service-mesh-beta" target="_blank" rel="noopener noreferrer">Service Mesh Beta 版本&lt;/a>，其中也包括了如何参与到该功能的开发。当前，这些 Beta 功能是 Cilium 项目中的一部分，在单独&lt;a href="https://github.com/cilium/cilium/tree/beta/service-mesh" target="_blank" rel="noopener noreferrer">分支&lt;/a>进行开发，可独立进行测试、反馈和修改，我们期待在 2022 年初 Cilium 1.12 版本发布之前合入到 Cilium 主分支。&lt;/p></description></item><item><title>CKS 认证备考指南</title><link>https://openksc.github.io/zh/blogs/cks-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/cks-guide/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>CKA 和 CKS 是 Linux 基金会联合 CNCF 社区组织的云原生技术领域权威的技术水平认证考试，考试采用实操方式进行。CKS 全称是 Certified Kubernetes Security Specialist，它在一个模拟真实的环境中测试考生对 Kubernetes 和云安全的知识。在参加 CKS 考试之前，必须已经通过 CKA（Kubernetes 管理员认证），在获得 CKA 证书之后才可以预约 CKS 考试。CKS 的考试难度相对于 CKA 提高了很多，2 个小时的考试时间很紧张，因为考试是在外网上进行，这两个考试又是实操考试，网络条件不好，很影响效率，如果不抓紧的话，很可能做不完所有实操题。提醒备考的同学善用考试软件提供的 notepad 功能，先把 yaml 文件或命令写到 notepad 里，再粘贴到 Terminal 里。&lt;/p>
&lt;p>我因为上次 CKA 考试还是比较顺利，所以这次的 CKS 考试有点疏忽了，搞忘带身份证和护照，CKA/CKS 考试需要身份证 + 护照/信用卡，因此跟监考老师沟通了很久时间，最后修改了考试人姓名为中文，是用驾驶证完成的考试。意外之喜是 CKS 给我的证书是中文名的。&lt;/p>
&lt;p>我这次考试的 Kubernetes 版本是 1.22，特意记录了一下考试会考到的知识点，分享给需要的同学。&lt;/p>
&lt;blockquote>
&lt;p>补充：Kubernetes 1.25 开始，正式废止了 PSP，这个部分可以参考本文的记录。&lt;/p>&lt;/blockquote>
&lt;h2 id="networkpolicy">NetworkPolicy&lt;/h2>
&lt;p>通常使用标签选择器来选择 Pod，控制流量。所以要对 kubectl label 的使用方法熟悉起来。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl label &lt;span style="color:#f92672">[&lt;/span>--overwrite&lt;span style="color:#f92672">]&lt;/span> &lt;span style="color:#f92672">(&lt;/span>-f FILENAME | TYPE NAME&lt;span style="color:#f92672">)&lt;/span> KEY_1&lt;span style="color:#f92672">=&lt;/span>VAL_1 ... KEY_N&lt;span style="color:#f92672">=&lt;/span>VAL_N &lt;span style="color:#f92672">[&lt;/span>--resource-version&lt;span style="color:#f92672">=&lt;/span>version&lt;span style="color:#f92672">]&lt;/span> &lt;span style="color:#f92672">[&lt;/span>options&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>网络策略的使用方法见注释：&lt;/p></description></item><item><title>CNI 基准测试：Cilium 网络性能分析</title><link>https://openksc.github.io/zh/blogs/cilium-cni-benchmark/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/cilium-cni-benchmark/</guid><description>&lt;blockquote>
&lt;p>原文链接：&lt;a href="https://cilium.io/blog/2021/05/11/cni-benchmark" target="_blank" rel="noopener noreferrer">https://cilium.io/blog/2021/05/11/cni-benchmark&lt;/a>&lt;/p>&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>作者：Thomas Graf&lt;/strong>&lt;br />
&lt;strong>译者：罗煜、张亮，均来自KubeSphere 团队&lt;/strong>&lt;br />
Thomas Graf 是 Cilium 的联合创始人，同时也是 Cilium 母公司 &lt;a href="https://isovalent.com" title="Isovalent" target="_blank" rel="noopener noreferrer">Isovalent&lt;/a> 的 CTO 和联合创始人。此前 Thomas 曾先后在 &lt;a href="https://kernel.org" title="Linux 内核" target="_blank" rel="noopener noreferrer">Linux 内核&lt;/a>的网络、安全和 eBPF 领域从事了 15 年的开发工作。&lt;br />
&lt;strong>注：本文已取得作者本人的翻译授权&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;p>大家好！👋&lt;/p>
&lt;p>随着越来越多的关键负载被迁移到 Kubernetes 上，网络性能基准测试正在成为选择 Kubernetes 网络方案的重要参考。在这篇文章中，我们将基于过去几周进行的大量基准测试的结果探讨 Cilium 的性能特点。应广大用户的要求，我们也将展示 Calico 的测试结果，以便进行直接对比。&lt;/p>
&lt;p>除了展示测试的结果数据外，我们还将对容器网络基准测试这一课题进行更深入的研究，并探讨以下几个方面的问题：&lt;/p>
&lt;ul>
&lt;li>吞吐量基准测试&lt;/li>
&lt;li>容器网络是否会增加开销&lt;/li>
&lt;li>打破常规：eBPF 主机路由（Host Routing）&lt;/li>
&lt;li>测量延迟：每秒请求数&lt;/li>
&lt;li>Cilium eBPF 和 Calico eBPF 的 CPU 火焰图对比&lt;/li>
&lt;li>新连接处理速率&lt;/li>
&lt;li>WireGuard 与 IPsec 加密开销对比&lt;/li>
&lt;li>测试环境&lt;/li>
&lt;/ul>
&lt;h2 id="测试结果汇总">测试结果汇总&lt;/h2>
&lt;p>在详细分析基准测试及其数据之前，我们先展示汇总的测试结论。如果您希望直接了解测试细节并得出自己的结论，也可以跳过这一节的内容。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>eBPF 起决定性作用&lt;/strong>：Cilium 在某些方面优于 Calico 的 eBPF 数据路径（Data Path），例如在 &lt;code>TCP_RR&lt;/code> 和 &lt;code>TCP_CRR&lt;/code> 基准测试中观察到的延迟。此外，更重要的结论是 eBPF 明显优于 iptables。在允许使用 eBPF 绕过 iptables 的配置环境中，Cilium 和 Calico 的性能都明显优于不能绕过 iptables 的情况。&lt;/p></description></item><item><title>Curve 分布式存储在 KubeSphere 中的实践</title><link>https://openksc.github.io/zh/blogs/kubesphere-curve/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-curve/</guid><description>&lt;h2 id="curve-介绍">Curve 介绍&lt;/h2>
&lt;p>Curve 是网易开发的现代存储系统，目前支持文件存储 (CurveFS) 和块存储 (CurveBS)。现在它作为一个沙盒项目托管在 CNCF。
Curve 是一个高性能、轻量级操作、本地云的开源分布式存储系统。Curve 可以应用于 :&lt;/p>
&lt;ol>
&lt;li>主流云本地基础设施平台 OpenStack 和 Kubernetes;&lt;/li>
&lt;li>云本地数据库的高性能存储 ;&lt;/li>
&lt;li>使用与 s3 兼容的对象存储作为数据存储的云存储中间件。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202301051306995.png" alt="">&lt;/p>
&lt;h2 id="实现效果">实现效果&lt;/h2>
&lt;p>通过部署 CurveFS 集群、Helm 部署 Curve-csi 插件创建 SC 来达到声明 PVC 时自动创建 PV 的效果&lt;/p>
&lt;h2 id="开始部署">开始部署&lt;/h2>
&lt;p>K8s 环境可以通过安装 KubeSphere 进行部署 , 我使用的是高可用方案。
在公有云上安装 KubeSphere 参考文档：&lt;a href="https://v3-1.docs.kubesphere.io/zh/docs/installing-on-linux/public-cloud/install-kubesphere-on-huaweicloud-ecs/" title="多节点安装" target="_blank" rel="noopener noreferrer">多节点安装&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@k8s-master ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># kubectl get node&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STATUS ROLES AGE VERSION
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>k8s-master Ready control-plane,master 79d v1.23.8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>k8s-node1 Ready worker 79d v1.23.8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>k8s-node2 Ready worker 79d v1.23.8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>k8s-node3 Ready worker 79d v1.23.8
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202301051306259.png" alt="">&lt;/p></description></item><item><title>DevOps 流水线如何去除 Docker 依赖</title><link>https://openksc.github.io/zh/blogs/devops-pipeline-remove-docker-dependencies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/devops-pipeline-remove-docker-dependencies/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>在 1.20 版本之后, Kubernetes 社区放弃了对 Docker 的支持, 但并不是说未来 Docker 将无法使用。本文主要是针对非 Docker 驱动的 Kubernetes 集群下，给出一个可行的 CI/CD 方案。如果你对非 Docker 环境下进行 CI/CD 也有需求，欢迎一起讨论方案。&lt;/p>
&lt;p>阅读本文需要一些基础，这些包括:&lt;/p>
&lt;ul>
&lt;li>熟悉 KubeSphere Devops，能独立阅读文档进行自定义配置&lt;/li>
&lt;li>熟练使用流水线功能&lt;/li>
&lt;/ul>
&lt;p>不建议刚接触 DevOps 的人阅读本文。由于这种变更对用户影响较大，后面的版本中，希望能达成统一的实践，再集成到产品中，预期会支持 Containerd、cri-o、Kind 等。&lt;/p>
&lt;h2 id="测试环境描述">测试环境描述&lt;/h2>
&lt;p>安装非 Docker 集群可以参考&lt;a href="https://ask.kubesphere.io/forum/d/3054-dockerkubernetes" target="_blank" rel="noopener noreferrer">文档&lt;/a> ，执行如下命令, 查看 Kubernetes 版本:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl version
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Client Version: version.Info&lt;span style="color:#f92672">{&lt;/span>Major:&lt;span style="color:#e6db74">&amp;#34;1&amp;#34;&lt;/span>, Minor:&lt;span style="color:#e6db74">&amp;#34;17&amp;#34;&lt;/span>, GitVersion:&lt;span style="color:#e6db74">&amp;#34;v1.17.9&amp;#34;&lt;/span>, GitCommit:&lt;span style="color:#e6db74">&amp;#34;4fb7ed12476d57b8437ada90b4f93b17ffaeed99&amp;#34;&lt;/span>, GitTreeState:&lt;span style="color:#e6db74">&amp;#34;clean&amp;#34;&lt;/span>, BuildDate:&lt;span style="color:#e6db74">&amp;#34;2020-07-15T16:18:16Z&amp;#34;&lt;/span>, GoVersion:&lt;span style="color:#e6db74">&amp;#34;go1.13.9&amp;#34;&lt;/span>, Compiler:&lt;span style="color:#e6db74">&amp;#34;gc&amp;#34;&lt;/span>, Platform:&lt;span style="color:#e6db74">&amp;#34;linux/amd64&amp;#34;&lt;/span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Server Version: version.Info&lt;span style="color:#f92672">{&lt;/span>Major:&lt;span style="color:#e6db74">&amp;#34;1&amp;#34;&lt;/span>, Minor:&lt;span style="color:#e6db74">&amp;#34;17&amp;#34;&lt;/span>, GitVersion:&lt;span style="color:#e6db74">&amp;#34;v1.17.9&amp;#34;&lt;/span>, GitCommit:&lt;span style="color:#e6db74">&amp;#34;4fb7ed12476d57b8437ada90b4f93b17ffaeed99&amp;#34;&lt;/span>, GitTreeState:&lt;span style="color:#e6db74">&amp;#34;clean&amp;#34;&lt;/span>, BuildDate:&lt;span style="color:#e6db74">&amp;#34;2020-07-15T16:10:45Z&amp;#34;&lt;/span>, GoVersion:&lt;span style="color:#e6db74">&amp;#34;go1.13.9&amp;#34;&lt;/span>, Compiler:&lt;span style="color:#e6db74">&amp;#34;gc&amp;#34;&lt;/span>, Platform:&lt;span style="color:#e6db74">&amp;#34;linux/amd64&amp;#34;&lt;/span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>执行如下命令, 查看 containerd 版本:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>containerd --version
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>containerd github.com/containerd/containerd v1.4.3 269548fa27e0089a8b8278fc4
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="去除-kubesphere-devops-对-docker-的依赖">去除 KubeSphere DevOps 对 Docker 的依赖&lt;/h2>
&lt;p>主要有两点:&lt;/p></description></item><item><title>Docker 出局？你还有 iSula、Containerd、CRI-O</title><link>https://openksc.github.io/zh/blogs/dockershim-out-of-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/dockershim-out-of-kubernetes/</guid><description>&lt;h2 id="docker-真的要被-kubernetes-社区剔除了吗">Docker 真的要被 Kubernetes 社区剔除了吗？&lt;/h2>
&lt;p>最近，Kubernetes 在 1.20 版本中 的 ChangeLog 提到将在未来的版本中废弃 Docker 作为容器运行时，这个事情在全球都闹得沸沸扬扬。&lt;/p>
&lt;p>那么，Kubernetes 要在 v1.20 开始弃用 docker 了？其实是 Kubernetes 弃用 kubelet 中集成的 dockershim 模块，也就是说不再将 docker 作为默认的 Container Runtime，不过 Kubernetes 应该还是可以通过外接方式使用 Docker 的，感兴趣的同学可以通过以下链接了解个中缘由。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Dockershim Deprecation FAQ&lt;/strong>: &lt;a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq" target="_blank" rel="noopener noreferrer">https://kubernetes.io/blog/2020/12/02/dockershim-faq&lt;/a>&lt;/li>
&lt;li>&lt;strong>Don't Panic: Kubernetes and Docker&lt;/strong>: &lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker" target="_blank" rel="noopener noreferrer">https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="对于已经使用-docker-的-kubesphere-用户有没有影响">对于已经使用 Docker 的 KubeSphere 用户有没有影响&lt;/h2>
&lt;p>该消息一出，社区里就有很多用户小伙伴问我们对 KubeSphere 有没有影响，土耳其的合作伙伴也联系我们希望提供一个官方声明。&lt;/p>
&lt;p>&lt;img src="https://ap3.qingstor.com/kubesphere-website/docs/20201208233955.png" alt="社区用户提问">&lt;/p>
&lt;p>dockershim 一直都是 Kubernetes 社区为了能让 Docker 成为其支持的容器运行时，所维护的一个兼容程序。本次所谓的废弃，也仅仅是 Kubernetes 要放弃对现在 Kubernetes 代码仓库中的 dockershim 的维护支持。 以便其可以像开始时计划的那样，仅负责维护其 CRI ，任何兼容 CRI 的运行时，皆可作为 Kubernetes 的 runtime，例如 Isula、CRI-O、Containerd 等。&lt;/p></description></item><item><title>eBPF 概述，第 1 部分：介绍</title><link>https://openksc.github.io/zh/blogs/ebpf-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/ebpf-guide/</guid><description>&lt;blockquote>
&lt;p>原文链接： &lt;a href="https://www.collabora.com/news-and-blog/blog/2019/04/05/an-ebpf-overview-part-1-introduction/" target="_blank" rel="noopener noreferrer">https://www.collabora.com/news-and-blog/blog/2019/04/05/an-ebpf-overview-part-1-introduction/&lt;/a>&lt;/p>&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>作者：Adrian Ratiu&lt;/strong>&lt;br />
&lt;strong>译者：狄卫华&lt;/strong>&lt;br />
&lt;strong>注：本文已取得作者本人的翻译授权&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;h2 id="1-前言">1. 前言&lt;/h2>
&lt;p>&lt;strong>有兴趣了解更多关于 eBPF 技术的底层细节？那么请继续移步，我们将深入研究 eBPF 的底层细节，从其虚拟机机制和工具，到在远程资源受限的嵌入式设备上运行跟踪。&lt;/strong>&lt;/p>
&lt;p>注意：本系列博客文章将集中在 eBPF 技术，因此对于我们来讲，文中 BPF 和 eBPF 等同，可相互使用。BPF 名字/缩写已经没有太大的意义，因为这个项目的发展远远超出了它最初的范围。BPF 和 eBPF 在该系列中会交替使用。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.collabora.com/news-and-blog/blog/2019/04/05/an-ebpf-overview-part-1-introduction/" target="_blank" rel="noopener noreferrer">第 1 部分&lt;/a>和&lt;a href="https://www.collabora.com/news-and-blog/blog/2019/04/15/an-ebpf-overview-part-2-machine-and-bytecode/" target="_blank" rel="noopener noreferrer">第 2 部分&lt;/a> 为新人或那些希望通过深入了解 eBPF 技术栈的底层技术来进一步了解 eBPF 技术的人提供了深入介绍。&lt;/li>
&lt;li>&lt;a href="https://www.collabora.com/news-and-blog/blog/2019/04/26/an-ebpf-overview-part-3-walking-up-the-software-stack/" target="_blank" rel="noopener noreferrer">第 3 部分&lt;/a>是对用户空间工具的概述，旨在提高生产力，建立在第 1 部分和第 2 部分中介绍的底层虚拟机机制之上。&lt;/li>
&lt;li>&lt;a href="https://www.collabora.com/news-and-blog/blog/2019/05/06/an-ebpf-overview-part-4-working-with-embedded-systems/" target="_blank" rel="noopener noreferrer">第 4 部分&lt;/a>侧重于在资源有限的嵌入式系统上运行 eBPF 程序，在嵌入式系统中完整的工具链技术栈（BCC/LLVM/python 等）是不可行的。我们将使用占用资源较小的嵌入式工具在 32 位 ARM 上交叉编译和运行 eBPF 程序。只对该部分感兴趣的读者可选择跳过其他部分。&lt;/li>
&lt;li>&lt;a href="https://www.collabora.com/news-and-blog/blog/2019/05/14/an-ebpf-overview-part-5-tracing-user-processes/" target="_blank" rel="noopener noreferrer">第 5 部分&lt;/a>是关于用户空间追踪。到目前为止，我们的努力都集中在内核追踪上，所以是时候我们关注一下用户进程了。&lt;/li>
&lt;/ul>
&lt;p>如有疑问时，可使用该流程图：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/eBPF-flowchart.png" alt="">&lt;/p>
&lt;h2 id="2-ebpf-是什么">2. eBPF 是什么？&lt;/h2>
&lt;p>eBPF 是一个基于寄存器的虚拟机，使用自定义的 64 位 RISC 指令集，能够在 Linux 内核内运行即时本地编译的 &amp;quot;BPF 程序&amp;quot;，并能访问内核功能和内存的一个子集。这是一个完整的虚拟机实现，不要与基于内核的虚拟机（KVM）相混淆，后者是一个模块，目的是使 Linux 能够作为其他虚拟机的管理程序。eBPF 也是主线内核的一部分，所以它不像其他框架那样需要任何第三方模块（&lt;a href="https://lttng.org/docs/v2.10/#doc-lttng-modules" target="_blank" rel="noopener noreferrer">LTTng&lt;/a> 或 &lt;a href="https://kernelnewbies.org/SystemTap" target="_blank" rel="noopener noreferrer">SystemTap&lt;/a>），而且几乎所有的 Linux 发行版都默认启用。熟悉 DTrace 的读者可能会发现 &lt;a href="https://www.brendangregg.com/blog/2018-10-08/dtrace-for-linux-2018.html" target="_blank" rel="noopener noreferrer">DTrace/BPFtrace 对比&lt;/a>非常有用。&lt;/p></description></item><item><title>eBPF 概述，第 2 部分：机器和字节码</title><link>https://openksc.github.io/zh/blogs/ebpf-machine-bytecode/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/ebpf-machine-bytecode/</guid><description>&lt;blockquote>
&lt;p>原文链接： &lt;a href="https://www.collabora.com/news-and-blog/blog/2019/04/15/an-ebpf-overview-part-2-machine-and-bytecode/" target="_blank" rel="noopener noreferrer">https://www.collabora.com/news-and-blog/blog/2019/04/15/an-ebpf-overview-part-2-machine-and-bytecode/&lt;/a>&lt;/p>&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>作者：Adrian Ratiu&lt;/strong>&lt;br />
&lt;strong>译者：狄卫华&lt;/strong>&lt;br />
&lt;strong>注：本文已取得作者本人的翻译授权&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;p>在我们的&lt;a href="https://kubesphere.com.cn/blogs/ebpf-guide/" target="_blank" rel="noopener noreferrer">第一篇文章&lt;/a>中，我们介绍了 eBPF VM、它刻意的设计限制以及如何从用户空间进程与其交互。如果您还没有阅读它，您可能需要在继续阅读本篇文章之前阅读上一篇文章，因为如果没有适当了解，直接从机器和字节码细节开始学习可能会很困难。如有疑问，请参阅第一部分开头的流程图。&lt;/p>
&lt;p>本系列的第二部分更深入地研究了第一部分中研究的 eBPF VM 和程序。掌握这种底层知识不是强制性的，但对于本系列的其余部分来说是非常有用的基础，我们将在其中检查建立在这些机制之上的更高级别的工具。&lt;/p>
&lt;h2 id="虚拟机">虚拟机&lt;/h2>
&lt;p>eBPF 是一个 RISC 寄存器机，共有 &lt;a href="https://github.com/torvalds/linux/blob/v4.20/include/uapi/linux/bpf.h#L45" target="_blank" rel="noopener noreferrer">11 个 64 位寄存器&lt;/a>，一个程序计数器和一个 512 字节固定大小的堆栈。九个寄存器是通用读写的，一个是只读堆栈指针，程序计数器是隐式的，即我们只能跳转到计数器的某个偏移量。VM 寄存器始终为 64 位宽（即使在 32 位 ARM 处理器内核中运行！）并且如果最高有效的 32 位为零，则支持 32 位子寄存器寻址 - 这将在第四部分在嵌入式设备上交叉编译和运行 eBPF 程序非常有用。&lt;/p>
&lt;p>这些寄存器是：&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>&lt;/th>
 &lt;th>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>r0：&lt;/td>
 &lt;td>存储函数调用和当前程序退出代码的返回值&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>r1 - r5：&lt;/td>
 &lt;td>作为函数调用的参数，在程序开始时 r1 包含 “上下文” 参数指针&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>r6 - r9：&lt;/td>
 &lt;td>这些在内核函数调用之间被保留&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>r10：&lt;/td>
 &lt;td>每个 eBPF 程序512字节堆栈的只读指针&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>在加载时提供的 eBPF &lt;a href="https://github.com/torvalds/linux/blob/v4.20/include/uapi/linux/bpf.h#L136" target="_blank" rel="noopener noreferrer">程序类型&lt;/a>准确地决定了哪些内核函数子集可以调用，以及在程序启动时通过 r1 提供的 “上下文” 参数。r0 中存储的程序退出值的含义也是由程序类型决定的。&lt;/p></description></item><item><title>eBPF 概述，第 3 部分：软件开发生态</title><link>https://openksc.github.io/zh/blogs/ebpf-software-stack/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/ebpf-software-stack/</guid><description>&lt;blockquote>
&lt;p>原文链接： &lt;a href="https://www.collabora.com/news-and-blog/blog/2019/04/26/an-ebpf-overview-part-3-walking-up-the-software-stack/" target="_blank" rel="noopener noreferrer">https://www.collabora.com/news-and-blog/blog/2019/04/26/an-ebpf-overview-part-3-walking-up-the-software-stack/&lt;/a>&lt;/p>&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>作者：Adrian Ratiu&lt;/strong>&lt;br />
&lt;strong>译者：狄卫华&lt;/strong>&lt;br />
&lt;strong>注：本文已取得作者本人的翻译授权&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;h2 id="1-前言">1. 前言&lt;/h2>
&lt;p>在本系列的&lt;a href="https://kubesphere.com.cn/blogs/ebpf-guide/" target="_blank" rel="noopener noreferrer">第 1 部分&lt;/a>和&lt;a href="https://kubesphere.com.cn/blogs/ebpf-machine-bytecode/" target="_blank" rel="noopener noreferrer">第 2 部分&lt;/a>中，我们对 eBPF 虚拟机进行了简洁的深入研究。阅读上述部分并不是理解第 3 部分的必修课，尽管很好地掌握了低级别的基础知识确实有助于更好地理解高级别的工具。为了理解这些工具是如何工作的，我们先定义一下 eBPF 程序的高层次组件：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>后端&lt;/strong>：这是在内核中加载和运行的 eBPF 字节码。它将数据写入内核 map 和环形缓冲区的&lt;strong>数据结构&lt;/strong>中。&lt;/li>
&lt;li>&lt;strong>加载器：&lt;strong>它将字节码&lt;/strong>后端&lt;/strong>加载到内核中。通常情况下，当加载器进程终止时，字节码会被内核自动卸载。&lt;/li>
&lt;li>&lt;strong>前端：&lt;strong>从&lt;/strong>数据结构&lt;/strong>中读取数据（由&lt;strong>后端&lt;/strong>写入）并将其显示给用户。&lt;/li>
&lt;li>&lt;strong>数据结构&lt;/strong>：这些是&lt;strong>后端&lt;/strong>和&lt;strong>前端&lt;/strong>之间的通信手段。它们是由内核管理的 map 和环形缓冲区，可以通过文件描述符访问，并需要在&lt;strong>后端&lt;/strong>被加载之前创建。它们会持续存在，直到没有更多的&lt;strong>后端&lt;/strong>或&lt;strong>前端&lt;/strong>进行读写操作。&lt;/li>
&lt;/ul>
&lt;p>在第 1 部分和第 2 部分研究的 &lt;a href="https://github.com/torvalds/linux/blob/v4.20/samples/bpf/sock_example.c" target="_blank" rel="noopener noreferrer">sock_example.c&lt;/a> 中，所有的组件都被放置在一个 C 文件中，所有的动作都由用户进程完成。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/torvalds/linux/blob/v4.20/samples/bpf/sock_example.c#L40-L45" target="_blank" rel="noopener noreferrer">第 40-45 行&lt;/a>创建 map&lt;strong>数据结构&lt;/strong>。&lt;/li>
&lt;li>&lt;a href="https://github.com/torvalds/linux/blob/v4.20/samples/bpf/sock_example.c#L47-L61" target="_blank" rel="noopener noreferrer">第 47-61 行&lt;/a>定义&lt;strong>后端&lt;/strong>。&lt;/li>
&lt;li>&lt;a href="https://github.com/torvalds/linux/blob/v4.20/samples/bpf/sock_example.c#L63-L76" target="_blank" rel="noopener noreferrer">第 63-76 行&lt;/a>在内核中&lt;strong>加载&lt;/strong>后端&lt;/li>
&lt;li>&lt;a href="https://github.com/torvalds/linux/blob/v4.20/samples/bpf/sock_example.c#L78-L91" target="_blank" rel="noopener noreferrer">第 78-91 行&lt;/a>是&lt;strong>前端&lt;/strong>，负责将从 map 文件描述符中读取的数据打印给用户。&lt;/li>
&lt;/ul>
&lt;p>eBPF 程序可以更加复杂：多个&lt;strong>后端&lt;/strong>可以由一个（或单独的多个！）&lt;strong>加载器&lt;/strong>进程加载，写入多个&lt;strong>数据结构&lt;/strong>，然后由多个&lt;strong>前端&lt;/strong>进程读取，所有这些都可以发生在一个跨越多个进程的用户 eBPF 应用程序中。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/eBPF-Part3-Diagram1.png" alt="">&lt;/p>
&lt;h2 id="2-层级-1容易编写的后端llvm-ebpf-编译器">2. 层级 1：容易编写的后端：LLVM eBPF 编译器&lt;/h2>
&lt;p>我们在前面的文章中看到，在内核中编写原始的 eBPF 字节码是不仅困难而且低效，这非常像用处理器的汇编语言编写程序，所以很自然地开发了一个能够将 LLVM 中间表示编译成 eBPF 程序的模块，并从 2015 年的 v3.7 开始发布（GCC 到现在为止仍然不支持 eBPF）。这使得多种高级语言如 C、Go 或 Rust 的子集可以被编译到 eBPF。最成熟和最流行的是基于 C 语言编写的方式，因为内核也是用 C 写的，这样就更容易复用现有的内核头文件。&lt;/p></description></item><item><title>eBPF 概述，第 4 部分：在嵌入式系统运行</title><link>https://openksc.github.io/zh/blogs/ebpf-working-with-embedded-systems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/ebpf-working-with-embedded-systems/</guid><description>&lt;blockquote>
&lt;p>原文链接： &lt;a href="https://www.collabora.com/news-and-blog/blog/2019/05/06/an-ebpf-overview-part-4-working-with-embedded-systems/" target="_blank" rel="noopener noreferrer">https://www.collabora.com/news-and-blog/blog/2019/05/06/an-ebpf-overview-part-4-working-with-embedded-systems/&lt;/a>&lt;/p>&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>作者：Adrian Ratiu&lt;/strong>&lt;br />
&lt;strong>译者：狄卫华&lt;/strong>&lt;br />
&lt;strong>注：本文已取得作者本人的翻译授权&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;h2 id="1-前言">1. 前言&lt;/h2>
&lt;p>在本系列的&lt;a href="https://kubesphere.com.cn/blogs/ebpf-guide/" target="_blank" rel="noopener noreferrer">第 1 部分&lt;/a>和&lt;a href="https://kubesphere.com.cn/blogs/ebpf-machine-bytecode/" target="_blank" rel="noopener noreferrer">第 2 部分&lt;/a>，我们介绍了 eBPF 虚拟机内部工作原理，在&lt;a href="https://kubesphere.com.cn/blogs/ebpf-software-stack/" target="_blank" rel="noopener noreferrer">第 3 部分&lt;/a>我们研究了基于底层虚拟机机制之上开发和使用 eBPF 程序的主流方式。&lt;/p>
&lt;p>在这一部分中，我们将从另外一个视角来分析项目，尝试解决嵌入式 Linux 系统所面临的一些独特的问题：如需要非常小的自定义操作系统镜像，不能容纳完整的 BCC LLVM 工具链/python 安装，或试图避免同时维护主机的交叉编译（本地）工具链和交叉编译的目标编译器工具链，以及其相关的构建逻辑，即使在使用像 OpenEmbedded/Yocto 这样的高级构建系统时也很重要。&lt;/p>
&lt;h2 id="2-关于可移植性">2. 关于可移植性&lt;/h2>
&lt;p>在第 3 部分研究的运行 eBPF/BCC 程序的主流方式中，可移植性并不是像在嵌入式设备上面临的问题那么大：eBPF 程序是在被加载的同一台机器上编译的，使用已经运行的内核，而且头文件很容易通过发行包管理器获得。嵌入式系统通常运行不同的 Linux 发行版和不同的处理器架构，与开发人员的计算机相比，有时具有重度修改或上游分歧的内核，在构建配置上也有很大的差异，或还可能使用了只有二进制的模块。&lt;/p>
&lt;p>eBPF 虚拟机的字节码是通用的（并未与特定机器相关），所以一旦编译好 eBPF 字节码，将其从 x86_64 移动到 ARM 设备上并不会引起太多问题。当字节码探测内核函数和数据结构时，问题就开始了，这些函数和数据结构可能与目标设备的内核不同或者会不存在，所以至少目标设备的内核头文件必须存在于构建 eBPF 程序字节码的主机上。新的功能或 eBPF 指令也可能被添加到以后的内核中，这可以使 eBPF 字节码向前兼容，但不能在内核版本之间向后兼容（参见&lt;a href="https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md" target="_blank" rel="noopener noreferrer">内核版本与 eBPF 功能&lt;/a>）。建议将 eBPF 程序附加到稳定的内核 ABI 上，如跟踪点（tracepoint），这可以缓解常见的可移植性。&lt;/p>
&lt;p>最近一个重要的工作已经开始，通过在 LLVM 生成的 eBPF 对象代码中嵌入数据类型信息，通过增加 BTF（BTF 类型格式）数据，以增加 eBPF 程序的可移植性（CO-RE 一次编译，到处运行）。更多信息见这里的&lt;a href="https://lwn.net/Articles/750695/" target="_blank" rel="noopener noreferrer">补丁&lt;/a>和&lt;a href="https://lwn.net/Articles/773198/" target="_blank" rel="noopener noreferrer">文章&lt;/a>。这很重要，因为 BTF 涉及到 eBPF 软件技术栈的所有部分（内核虚拟机和验证器、clang/LLVM 编译器、BCC 等），但这种方式可带来很大的便利，允许重复使用现有的 BCC 工具，而不需要特别的 eBPF 交叉编译和在嵌入式设备上安装 LLVM 或运行 BPFd。截至目前，CO-RE BTF 工作仍处于早期开发阶段，还需要付出相当多的工作才能可用【译者注：当前在高版本内核已经可以使用或者编译内核时启用了 BTF 编译选项】。也许我们会在其完全可用后再发表一篇博文。&lt;/p></description></item><item><title>eBPF 概述，第 5 部分：跟踪用户进程</title><link>https://openksc.github.io/zh/blogs/ebpf-tracing-user-processes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/ebpf-tracing-user-processes/</guid><description>&lt;blockquote>
&lt;p>原文链接： &lt;a href="https://www.collabora.com/news-and-blog/blog/2019/05/14/an-ebpf-overview-part-5-tracing-user-processes/" target="_blank" rel="noopener noreferrer">https://www.collabora.com/news-and-blog/blog/2019/05/14/an-ebpf-overview-part-5-tracing-user-processes/&lt;/a>&lt;/p>&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>作者：Adrian Ratiu&lt;/strong>&lt;br />
&lt;strong>译者：狄卫华&lt;/strong>&lt;br />
&lt;strong>注：本文已取得作者本人的翻译授权&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;h2 id="1-前言">1. 前言&lt;/h2>
&lt;p>在之前的部分中，我们专注于 Linux 内核跟踪，在我们看来，基于 eBPF 的项目是最安全、最广泛可用和最有效的方法（eBPF 在 Linux 中完全是上游支持的，保证稳定的 ABI，在几乎所有的发行版中都默认启用，并可与所有其他跟踪机制集成）。 eBPF 成为内核工作的不二之选。 然而，到目前为止，我们故意避免深入讨论用户空间跟踪，因为它值得特别对待，因此我们在第 5 部分中专门讨论。&lt;/p>
&lt;p>首先，我们将讨论为什么使用，然后我们将 eBPF 用户跟踪分为静态和动态两类分别讨论。&lt;/p>
&lt;h2 id="2-为什么要在用户空间使用-ebpf">2. 为什么要在用户空间使用 eBPF？&lt;/h2>
&lt;p>最重要的用户问题是，既然有这么多其他的调试器/性能分析器/跟踪器，这么多针对特定语言或操作系统的工具为同样的任务而开发，为什么还要使用 eBPF 来跟踪用户空间进程？答案并不简单，根据不同的使用情况，eBPF 可能不是最佳解决方案；在庞大的用户空间生态系统中，并没有一个适合所有情况的调试/跟踪的项目。&lt;/p>
&lt;p>eBPF 跟踪具有以下优势：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>它为内核和用户空间提供了一个统一的跟踪接口，与其他工具（[k,u]probe, (dtrace)tracepoint 等）使用的机制兼容。2015 年的文章&lt;a href="https://www.brendangregg.com/blog/2015-07-08/choosing-a-linux-tracer.html" target="_blank" rel="noopener noreferrer">选择 linux 跟踪器&lt;/a>虽然有些过时，但其提供了很好的见解，说明使用所有不同的工具有多困难，要花多少精力。有一个统一的、强大的、安全的、可广泛使用的框架来满足大多数跟踪的需要，是非常有价值的。一些更高级别的工具，如 Perf/SystemTap/DTrace，正在 eBPF 的基础上重写（成为 eBPF 的前端），所以了解 eBPF 有助于使用它们。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>eBPF 是完全可编程的。Perf/ftrace 和其他工具都需要在事后处理数据，而 eBPF 可直接在内核/应用程序中运行自定义的高级本地编译的 C/Python/Go 检测代码。它可以在多个 eBPF 事件运行之间存储数据，例如以基于函数状态/参数计算每个函数调用统计数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>eBPF 可以跟踪系统中的一切，它并不局限于特定的应用程序。例如可以在共享库上设置 uprobes 并跟踪链接和调用它的所有进程。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>很多调试器需要暂停程序来观察其状态或降低运行时性能，从而难以进行实时分析，尤其是在生产工作负载上。因为 eBPF 附加了 JIT 的本地编译的检测代码，它的性能影响是最小的，不需要长时间暂停执行。&lt;/p></description></item><item><title>Fluent Operator 入门教程</title><link>https://openksc.github.io/zh/blogs/fluent-operator-logging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/fluent-operator-logging/</guid><description>&lt;h2 id="fluent-operator-介绍">Fluent Operator​ 介绍&lt;/h2>
&lt;p>随着云原生技术的快速发展，技术的不断迭代，对于日志的采集、处理及转发提出了更高的要求。云原生架构下的日志方案相比基于物理机或者是虚拟机场景的日志架构设计存在很大差别。作为 CNCF 的毕业项目，Fluent Bit 无疑为解决云环境中的日志记录问题的首选解决方案之一。但是在 Kubernetes 中安装部署以及配置 Fluent Bit 都具有一定的门槛，加大了用户的使用成本。&lt;/p>
&lt;p>&lt;strong>2019 年 1 月 21 日&lt;/strong>，KubeSphere 社区为了满足以云原生的方式管理 Fluent Bit 的需求开发了 &lt;strong>Fluentbit Operator&lt;/strong>，并在 2020 年 2 月 17 日发布了 v0.1.0 版本。此后产品不断迭代，在 &lt;strong>2021 年 8 月 4 日&lt;/strong> 正式将 Fluentbit Operator 捐献给 Fluent 社区。&lt;/p>
&lt;p>Fluentbit Operator 降低了 Fluent Bit 的使用门槛，能高效、快捷的处理日志信息，但是 Fluent Bit 处理日志的能力稍弱，我们还没有集成日志处理工具，比如 Fluentd，它有更多的插件可供使用。基于以上需求，Fluentbit Operator 集成了 Fluentd，旨在将 Fluentd 集成为一个可选的日志聚合和转发层，并重新命名为 &lt;strong>Fluent Operator（GitHub 地址： &lt;a href="https://github.com/fluent/fluent-operator" target="_blank" rel="noopener noreferrer">https://github.com/fluent/fluent-operator&lt;/a>）&lt;/strong>。在 2022 年 3 月 25 日 Fluent Operator 发布了 v1.0.0 版本，并将继续迭代 Fluentd Operator，预计在 2022 年第 2 季度发布 v1.1.0 版本，增加更多的功能与亮点。&lt;/p></description></item><item><title>FluxCD 多集群应用的设计与实现</title><link>https://openksc.github.io/zh/blogs/fluxcd-design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/fluxcd-design/</guid><description>&lt;blockquote>
&lt;p>作者：程乐齐，西电通院研二，方向是图像工程，目前专注于云原生领域。&lt;/p>&lt;/blockquote>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>FluxCD 是 CNCF 的孵化项目，可以让我们以 GitOps 的方式轻松地交付应用。和另一个同类的 CNCF 孵化项目 ArgoCD 不同，FluxCD 是许多 toolkit 的集合，天然松耦合并且有良好的扩展性，用户可按需取用。我们希望通过集成 FluxCD 这样一个优秀的 GitOps 项目来为用户提供更多的选择。&lt;/p>
&lt;p>我们综合考虑了以下三大要素：&lt;/p>
&lt;ul>
&lt;li>为还没有接触过 GitOps 的用户提供易上手的体验；&lt;/li>
&lt;li>为使用过 FluxCD 的用户提供无缝切换的体验；&lt;/li>
&lt;li>为已经使用过 KubeSphere GitOps 功能的用户提供熟悉感的同时突出 FluxCD 的优势和特性。&lt;/li>
&lt;/ul>
&lt;p>多次重新设计了前端界面和后端实现，最终完成了一个还算比较满意的版本。&lt;/p>
&lt;h2 id="ks-devops">ks-devops&lt;/h2>
&lt;h3 id="设计">设计&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>模板复用&lt;/strong>：FluxCD 提供了 &lt;a href="https://fluxcd.io/flux/components/source/helmcharts/" target="_blank" rel="noopener noreferrer">HelmChart&lt;/a> 类型的 CRD，但是 HelmRelease 无法直接引用 HelmChart，我们希望添加模板的功能，这样许多配置就可以复用。&lt;/li>
&lt;li>&lt;strong>多集群&lt;/strong>：我们希望 FluxApplication 是一个多集群应用，这样我们就可以用一套模板配置然后添加不同的配置去部署到多个集群中。&lt;/li>
&lt;/ul>
&lt;h3 id="crd">CRD&lt;/h3>
&lt;p>现有的 gitops.kubesphere.io/applications CRD 已经包含了 ArgoApplication。为了集成 FluxCD，我们将 FluxCD 中的 &lt;code>HelmRelease&lt;/code> 和 &lt;code>Kustomization&lt;/code> 组合抽象成一个 &lt;code>FluxApplication&lt;/code> 的概念放入 &lt;code>Application&lt;/code> 里并且 &lt;code>kind&lt;/code> 来标识用户启用了哪种 GitOps Engine。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202210211729793.png" alt="">&lt;/p>
&lt;p>一个完整的 GitOps 应用可以拆分成三大部分：&lt;/p></description></item><item><title>Istio 无法访问外部服务的故障排查</title><link>https://openksc.github.io/zh/blogs/debugging-envoy-and-istio/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/debugging-envoy-and-istio/</guid><description>&lt;h2 id="事故起因">事故起因&lt;/h2>
&lt;p>业务上新集群，本来以为&amp;quot;洒洒水&amp;quot;，11 点切，12 点就能在家睡觉了。流量切过来后，在验证过程中，发现网页能够正常打开，在登录时返回了 502，当场懵逼。在相关的容器日志发现一个高频的报错条目“7000 端口无法连接”，向业务组了解到这是 redis 集群中的一个端口，前后端是通过 redis 交互的，该集群同时还有 7001-7003 其它三个端口。&lt;/p>
&lt;p>用 nc 命令对 redis 集群进行连接测试：向服务端发送 &lt;code>keys *&lt;/code> 命令时，7000 端口返回的是 &lt;code>HTTP/1.1 400 Bad Request&lt;/code>，其他三个端口是 redis 返回的 &lt;code>-NOAUTH Authentication required&lt;/code>。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ nc 10.0.0.6 &lt;span style="color:#ae81ff">7000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>keys *
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>HTTP/1.1 &lt;span style="color:#ae81ff">400&lt;/span> Bad Request
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>content-length: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>connection: close
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ nc 10.0.0.6 &lt;span style="color:#ae81ff">7003&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>keys *
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-NOAUTH Authentication required
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>判断 7000 端口连接到了其他应用上，至少不是 redis。在宿主机上抓包发现没有抓到访问 7000 端口的流量，然后查看容器的 nf_conntrackb 表，发现 7000 端口的数据只有到本地的会话信息；7003 的有两条会话信息，一条到本机的，一条到目标服务器的。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ grep &lt;span style="color:#ae81ff">7000&lt;/span> /proc/net/nf_conntrack
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ipv4 &lt;span style="color:#ae81ff">2&lt;/span> tcp &lt;span style="color:#ae81ff">6&lt;/span> &lt;span style="color:#ae81ff">110&lt;/span> TIME_WAIT src&lt;span style="color:#f92672">=&lt;/span>10.64.192.14 dst&lt;span style="color:#f92672">=&lt;/span>10.0.0.6 sport&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">50498&lt;/span> dport&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">7000&lt;/span> src&lt;span style="color:#f92672">=&lt;/span>127.0.0.1 dst&lt;span style="color:#f92672">=&lt;/span>10.64.192.14 sport&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">15001&lt;/span> dport&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">50498&lt;/span> &lt;span style="color:#f92672">[&lt;/span>ASSURED&lt;span style="color:#f92672">]&lt;/span> mark&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span> zone&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span> use&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ grep &lt;span style="color:#ae81ff">7003&lt;/span> /proc/net/nf_conntrack
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ipv4 &lt;span style="color:#ae81ff">2&lt;/span> tcp &lt;span style="color:#ae81ff">6&lt;/span> &lt;span style="color:#ae81ff">104&lt;/span> TIME_WAIT src&lt;span style="color:#f92672">=&lt;/span>10.64.192.14 dst&lt;span style="color:#f92672">=&lt;/span>10.0.0.6 sport&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">38952&lt;/span> dport&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">7003&lt;/span> src&lt;span style="color:#f92672">=&lt;/span>127.0.0.1 dst&lt;span style="color:#f92672">=&lt;/span>10.64.192.14 sport&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">15001&lt;/span> dport&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">38952&lt;/span> &lt;span style="color:#f92672">[&lt;/span>ASSURED&lt;span style="color:#f92672">]&lt;/span> mark&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span> zone&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span> use&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ipv4 &lt;span style="color:#ae81ff">2&lt;/span> tcp &lt;span style="color:#ae81ff">6&lt;/span> &lt;span style="color:#ae81ff">104&lt;/span> TIME_WAIT src&lt;span style="color:#f92672">=&lt;/span>10.64.192.14 dst&lt;span style="color:#f92672">=&lt;/span>10.0.0.6 sport&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">38954&lt;/span> dport&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">7003&lt;/span> src&lt;span style="color:#f92672">=&lt;/span>10.0.0.6 dst&lt;span style="color:#f92672">=&lt;/span>10.64.192.14 sport&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">7003&lt;/span> dport&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">38954&lt;/span> &lt;span style="color:#f92672">[&lt;/span>ASSURED&lt;span style="color:#f92672">]&lt;/span> mark&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span> zone&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span> use&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>由此判断出 istio 没有代理转发出 7000 的流量，这突然就触及到了我的知识盲区，一大堆人看着，办公室 26 度的空调，一直在冒汗。没办法了，在与业务商量后，只能先关闭 istio 注入，优先恢复了业务。回去后恶补 istio 的相关资料。终于将问题解决。记录下相关信息，以供日后参考。&lt;/p></description></item><item><title>K8s 安全策略最佳实践</title><link>https://openksc.github.io/zh/blogs/k8s-security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/k8s-security/</guid><description>&lt;p>随着 K8s 在生产和测试环境中用的越来越多，对安全性的关注也会越来越多，所以本文主要是给大家分享以下内容：&lt;/p>
&lt;ul>
&lt;li>了解 K8s 环境面临的安全风险&lt;/li>
&lt;li>了解 K8s 提供的安全机制&lt;/li>
&lt;li>改善 K8s 安全状况的最佳实践&lt;/li>
&lt;/ul>
&lt;h2 id="1-k8s-安全风险">1. K8s 安全风险&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/k8s-security-1.png" alt="">&lt;/p>
&lt;p>这张图是 CNCF 金融用户小组总结的 K8s 信任边界图，它把在 K8s 环境中的信任边界划分成三大块儿。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>容器镜像相关部分&lt;/strong>，主要涉及到的安全攻击点就是镜像仓库和镜像本身。红色曲线可以被视为独立边界的系统。&lt;/li>
&lt;li>&lt;strong>K8s 控制平面相关部分&lt;/strong>，如果说一个攻击者攻击你的 K8s 集群的话，首先会攻击 K8s 的控制平面，中间涉及到的组件就是 K8s 的 apiserver、scheduler 和 controller-manager，所以说这些组件之间调用链的安全也需要去注意。&lt;/li>
&lt;li>&lt;strong>节点上运行时的安全&lt;/strong>，其中包括 kubelet、kube-proxy 和容器运行时环境也容易被攻击，要避免运行环境被渗透。&lt;/li>
&lt;/ul>
&lt;p>我们根据不同的攻击类型划分，首先最容易规避的就是来自外部的攻击。通常情况下，来自外部的攻击会有 2 种类型：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>一种是系统层面的漏洞，需要及时更新，及时跟进 K8s 社区和安全领域相关的最新消息，可以很好的规避。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>第二个是应用本身带来的渗透或者是提权的风险，业务部署在 K8s 之上，应用的漏洞可能造成容器越权或者容器逃逸之类的风险。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>借助恶意容器进行攻击也比较常见，在使用容器的过程种主要会面临以下风险：&lt;/p>
&lt;ol>
&lt;li>使用了不受信任的镜像仓库或者是使用了被篡改的容器镜像会导致恶意代码被执行。&lt;/li>
&lt;li>容器执行恶意代码存在提权或者逃逸的风险。&lt;/li>
&lt;li>即使容器运行时足够安全，无法提权或逃逸，内部暴露的服务也容易成为被攻击的点，造成数据被恶意访问。&lt;/li>
&lt;/ol>
&lt;p>K8s 集群的规模变大，运维人员与终端用户也会变多，安全凭证的泄露，会对整个集群的安全造成威胁。&lt;/p>
&lt;p>即使集群保护的非常好，在安全凭证没有泄漏的情况下，来自内部成员的恶意攻击也难以规避，即使是在测试环境也需要一定程度的租户隔离，避免来自内部的攻击、对数据的恶意访问。&lt;/p>
&lt;h2 id="2-k8s-安全机制">2. K8s 安全机制&lt;/h2>
&lt;p>在 K8s 社区，安全问题的关注度是非常高的，在 K8s 的设计中，各组件都有安全相关的特性。在 API 认证层面，控制平面中各个组件之间，需要开启 mTLS 进行组件之间的互认证。&lt;/p>
&lt;p>K8s 也支持丰富的认证、访问控制的机制，通常我们会借助 RBAC 对用户的权限进行限制。&lt;/p></description></item><item><title>KubeEdge 结合 KubeSphere 实现海量边缘节点与边缘设备管理</title><link>https://openksc.github.io/zh/blogs/kubesphere-kubeedge/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-kubeedge/</guid><description>&lt;h2 id="kubeedge-赋能-kubesphere-边缘节点管理">KubeEdge 赋能 KubeSphere 边缘节点管理&lt;/h2>
&lt;p>KubeSphere 是在 Kubernetes 之上构建的企业级分布式多租户的容器平台，在与 KubeEdge 集成中，扮演者着“云端控制面”的角色。&lt;/p>
&lt;p>下图中展示了边缘节点集成后，作为 Node 角色在 KubeSphere Console 上的展示效果，我们可以很方便的查看边缘节点容器日志和 Metrics。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/abc3947ada0084472d3d24fb43d95741.jpg" alt="">&lt;/p>
&lt;h2 id="为什么选择-kubeedge-集成边缘计算能力">为什么选择 KubeEdge 集成边缘计算能力？&lt;/h2>
&lt;p>首先 KubeEdge 本身的云边枢纽和架构，具有非常出色的云原生自治能力，支持边缘自治、消息与资源的可靠性同步、边缘节点的管理能力，边缘节点的 KubeFed 是极度轻量的，可按需裁剪定制。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/c4390108fc7f7f79447502c88cd43ed2.jpg" alt="">&lt;/p>
&lt;p>除了 KubeEdge 本身架构带来的特性外，我们集成 KubeEdge 的其他原因主要有：&lt;/p>
&lt;ul>
&lt;li>KubeEdge 是最早进入 CNCF 的边缘计算项目，项目成熟度比较高且社区比较活跃；&lt;/li>
&lt;li>KubeSphere v2.1.0/v3.0.0 起，社区用户陆续提出了边缘节点自动化安装部署、监控、日志、调试等方面的需求；&lt;/li>
&lt;li>KubeEdge 逐渐对边缘节点监控、日志、调试等有了更好的支持；&lt;/li>
&lt;li>补充 KubeEdge 边缘计算框架云端控制面。&lt;/li>
&lt;/ul>
&lt;p>在这样的背景下，KubeSphere 社区和 KubeEdge 社区紧密合作，从云端控制层面解决边缘节点纳管易用性和可观测性难题。KubeEdge 集成在 KubeSphere 容器平台后，可以补充 KubeSphere 的边缘计算能力，KubeSphere 则充当计算框架的一个云端控制面。&lt;/p>
&lt;h2 id="在集成过程中我们也遇到了一些挑战">在集成过程中，我们也遇到了一些挑战：&lt;/h2>
&lt;ul>
&lt;li>提供快速容器化部署方案&lt;/li>
&lt;li>实现边缘容器监控、日志依赖手动添加iptables规则，运维成本较高：&lt;code>iptables -t nat -A OUTPUT -p tcp --dport 10350 -j DNAT --to cloudcore ip:10003&lt;/code>&lt;/li>
&lt;li>提供边缘节点辅助验证服务&lt;/li>
&lt;li>边缘测部署配置项较多，希望一条脚本解决边缘节点加入云端组件&lt;/li>
&lt;/ul>
&lt;p>以上描述低版本的版本的场景，高版本的场景有所变化。&lt;/p></description></item><item><title>KubeKey 部署 K8s v1.28.8 实战</title><link>https://openksc.github.io/zh/blogs/using-kubekey-deploy-k8s-v1.28.8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/using-kubekey-deploy-k8s-v1.28.8/</guid><description>&lt;p>在某些生产环境下，我们仅需要一个原生的 K8s 集群，无需部署 &lt;strong>KubeSphere&lt;/strong> 这样的图形化管理控制台。在我们已有的技术栈里，已经习惯了利用 &lt;strong>KubeKey&lt;/strong> 部署 KubeSphere 和 K8s 集群。今天，我将为大家实战演示如何在 &lt;strong>openEuler 22.03 LTS SP3&lt;/strong> 上，利用 KubeKey 部署一套纯粹的 K8s 集群。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>实战服务器配置 (架构 1:1 复刻小规模生产环境，配置略有不同)&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ksp-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.131&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.132&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-master-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.133&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">3&lt;/td>
 &lt;td style="text-align: center">24&lt;/td>
 &lt;td style="text-align: center">48&lt;/td>
 &lt;td style="text-align: center">120&lt;/td>
 &lt;td style="text-align: center">300&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>&lt;strong>实战环境涉及软件版本信息&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;ul>
&lt;li>
&lt;p>操作系统：&lt;strong>openEuler 22.03 LTS SP3 x64&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>K8s：&lt;strong>v1.28.8&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Containerd：&lt;strong>1.7.13&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>KubeKey: &lt;strong>v3.1.1&lt;/strong>&lt;/p></description></item><item><title>KubeKey 离线部署 KubeSphere v3.4.1 和 K8s v1.26 实战指南</title><link>https://openksc.github.io/zh/blogs/deploying-kubesphere-and-k8s-offline-with-kubekey/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deploying-kubesphere-and-k8s-offline-with-kubekey/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;h3 id="知识点">知识点&lt;/h3>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>了解清单 (manifest) 和制品 (artifact) 的概念&lt;/li>
&lt;li>掌握 manifest 清单的编写方法&lt;/li>
&lt;li>根据 manifest 清单制作 artifact&lt;/li>
&lt;li>KubeKey 离线集群配置文件编写&lt;/li>
&lt;li>KubeKey 离线部署 Harbor&lt;/li>
&lt;li>KubeKey 离线部署 KubeSphere 和 K8s&lt;/li>
&lt;li>KubeKey 离线部署常见问题排查处理&lt;/li>
&lt;/ul>
&lt;h3 id="实战服务器配置">实战服务器配置&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ksp-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">离线环境 KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">离线环境 KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-master-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">离线环境 KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-registry&lt;/td>
 &lt;td style="text-align: center">192.168.9.90&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">离线环境镜像仓库节点（Harbor）&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-deploy&lt;/td>
 &lt;td style="text-align: center">192.168.9.89&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">联网主机用于制作离线包&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">32&lt;/td>
 &lt;td style="text-align: center">64&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">500&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h3 id="实战环境涉及软件版本信息">实战环境涉及软件版本信息&lt;/h3>
&lt;ul>
&lt;li>操作系统：&lt;strong>CentOS 7.9 x86_64&lt;/strong>&lt;/li>
&lt;li>KubeSphere：&lt;strong>v3.4.1&lt;/strong>&lt;/li>
&lt;li>K8s：&lt;strong>v1.26.5&lt;/strong>&lt;/li>
&lt;li>Containerd：&lt;strong>1.6.4&lt;/strong>&lt;/li>
&lt;li>KubeKey: &lt;strong>v3.0.13&lt;/strong>&lt;/li>
&lt;li>Harbor：&lt;strong>2.5.3&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="1-简介">1. 简介&lt;/h2>
&lt;p>KubeKey 从 v2.1.0 版开始新增了清单 (manifest) 和制品 (artifact) 的概念，为用户离线部署 KubeSphere 和 K8s 集群提供了一种简单便捷的解决方案。&lt;/p></description></item><item><title>KubeKey 升级 Kubernetes 次要版本实战指南</title><link>https://openksc.github.io/zh/blogs/upgrading-k8s-minor-versions-with-kubekey/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/upgrading-k8s-minor-versions-with-kubekey/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;h3 id="知识点">知识点&lt;/h3>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>KubeKey 如何升级 Kubernetes 次要版本&lt;/li>
&lt;li>Kubernetes 升级准备及验证&lt;/li>
&lt;li>KubeKey 升级 Kubernetes 的常见问题&lt;/li>
&lt;/ul>
&lt;h3 id="实战服务器配置-架构-11-复刻小规模生产环境配置略有不同">实战服务器配置 (架构 1:1 复刻小规模生产环境，配置略有不同)&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">k8s-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-master-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker/CI&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-worker-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-storage-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.81&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100/100/100/100/100&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS/Ceph-Rook/Longhorn/NFS/&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-storage-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.82&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100/100/100/100&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS/Ceph-Rook/Longhorn/&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-storage-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.83&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100/100/100/100&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS/Ceph-Rook/Longhorn/&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">registry&lt;/td>
 &lt;td style="text-align: center">192.168.9.80&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">Sonatype Nexus 3&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">10&lt;/td>
 &lt;td style="text-align: center">52&lt;/td>
 &lt;td style="text-align: center">152&lt;/td>
 &lt;td style="text-align: center">400&lt;/td>
 &lt;td style="text-align: center">2000&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h3 id="实战环境涉及软件版本信息">实战环境涉及软件版本信息&lt;/h3>
&lt;ul>
&lt;li>操作系统：&lt;strong>CentOS 7.9 x86_64&lt;/strong>&lt;/li>
&lt;li>KubeSphere：&lt;strong>v3.4.1&lt;/strong>&lt;/li>
&lt;li>Kubernetes：&lt;strong>v1.24.14&lt;/strong> to &lt;strong>v1.26.5&lt;/strong>&lt;/li>
&lt;li>Containerd：&lt;strong>1.6.4&lt;/strong>&lt;/li>
&lt;li>KubeKey: &lt;strong>v3.0.13&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="1-简介">1. 简介&lt;/h2>
&lt;p>上一期我们完成了 &lt;a href="https://kubesphere.io/zh/blogs/kubekey-upgrades-kubesphere-and-k8s-patch-versions/" target="_blank" rel="noopener noreferrer">KubeSphere 和 Kubernetes 补丁版本升级实战&lt;/a> , 本期我们实战如何利用 KubeKey 实现 Kubernetes 次要版本升级。&lt;/p></description></item><item><title>KubeKey 升级 KubeSphere 和 Kubernetes 补丁版本实战指南</title><link>https://openksc.github.io/zh/blogs/kubekey-upgrades-kubesphere-and-k8s-patch-versions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubekey-upgrades-kubesphere-and-k8s-patch-versions/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;h3 id="知识点">知识点&lt;/h3>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>KubeKey 如何升级 KubeSphere 补丁版本&lt;/li>
&lt;li>KubeKey 如何升级 Kubernetes 补丁版本&lt;/li>
&lt;li>KubeSphere 和 Kubernetes 升级准备及验证&lt;/li>
&lt;li>KubeKey 升级 KubeSphere 和 Kubernetes 的常见问题&lt;/li>
&lt;/ul>
&lt;h3 id="实战服务器配置-架构-11-复刻小规模生产环境配置略有不同">实战服务器配置 (架构 1:1 复刻小规模生产环境，配置略有不同)&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">k8s-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-master-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker/CI&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-worker-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-storage-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.81&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100/100/100/100/100&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS/Ceph-Rook/Longhorn/NFS/&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-storage-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.82&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100/100/100/100&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS/Ceph-Rook/Longhorn/&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">k8s-storage-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.83&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100/100/100/100&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS/Ceph-Rook/Longhorn/&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">registry&lt;/td>
 &lt;td style="text-align: center">192.168.9.80&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">Sonatype Nexus 3&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">10&lt;/td>
 &lt;td style="text-align: center">52&lt;/td>
 &lt;td style="text-align: center">152&lt;/td>
 &lt;td style="text-align: center">400&lt;/td>
 &lt;td style="text-align: center">2000&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h3 id="实战环境涉及软件版本信息">实战环境涉及软件版本信息&lt;/h3>
&lt;ul>
&lt;li>操作系统：&lt;strong>CentOS 7.9 x86_64&lt;/strong>&lt;/li>
&lt;li>KubeSphere：&lt;strong>v3.4.0&lt;/strong> to &lt;strong>v3.4.1&lt;/strong>&lt;/li>
&lt;li>Kubernetes：&lt;strong>v1.24.12&lt;/strong> to &lt;strong>v1.24.14&lt;/strong>&lt;/li>
&lt;li>Containerd：&lt;strong>1.6.4&lt;/strong>&lt;/li>
&lt;li>KubeKey: &lt;strong>v3.0.13&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="1-简介">1. 简介&lt;/h2>
&lt;h3 id="11-kubernetes-版本升级概述">1.1 Kubernetes 版本升级概述&lt;/h3>
&lt;p>KubeSphere v3.4.1 已于 2023 年 11 月 10 日正式发布，升级说明详见 &lt;a href="https://github.com/kubesphere/kubesphere/releases/tag/v3.4.1" target="_blank" rel="noopener noreferrer">Releases-v3.4.1 发布说明&lt;/a>。该发布版修复了 v3.4.0 中存在的许多问题，建议所有人更新。&lt;/p></description></item><item><title>KubeKey 在 AWS 安装部署 Kubernetes 高可用集群</title><link>https://openksc.github.io/zh/blogs/aws-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/aws-kubernetes/</guid><description>&lt;h3 id="介绍">介绍&lt;/h3>
&lt;p>对于生产环境，我们需要考虑 Kubernetes 集群的高可用性。本文教您部署如何在多台 AWS EC2 实例快速部署一套高可用的生产环境。要满足 Kubernetes 集群服务需要做到高可用，需要保证 kube-apiserver 的 HA ，可使用下列两种方式：&lt;/p>
&lt;ul>
&lt;li>AWS ELB（推荐）&lt;/li>
&lt;li>&lt;a href="https://ask.kubesphere.io/forum/d/1566-kubernetes-keepalived-haproxy" target="_blank" rel="noopener noreferrer">keepalived + haproxy&lt;/a> 对 kube-apiserver 进行负载均衡，实现高可用 Kubernetes 集群。&lt;/li>
&lt;/ul>
&lt;p>本教程重点介绍配置 AWS ELB 服务高可用安装。&lt;/p>
&lt;h3 id="前提条件">前提条件&lt;/h3>
&lt;ul>
&lt;li>考虑到数据的持久性，对于生产环境，我们不建议您使用存储OpenEBS，建议 NFS、GlusterFS、Ceph 等存储(需要提前准备)。文章为了进行开发和测试，集成了 OpenEBS 将 LocalPV 设置为默认的存储服务；&lt;/li>
&lt;li>SSH 可以访问所有节点；&lt;/li>
&lt;li>所有节点的时间同步；&lt;/li>
&lt;li>Red Hat 在其 Linux 发行版本中包括了 SELinux，建议关闭 SELinux 或者将 SELinux 的模式切换为 Permissive [宽容]工作模式。&lt;/li>
&lt;/ul>
&lt;h3 id="准备主机">准备主机&lt;/h3>
&lt;p>本示例创建 3 台 Ubuntu 18.04 server 64bit 的 EC2 云服务器，每台配置为 2 核 4 GB&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: left">主机IP&lt;/th>
 &lt;th style="text-align: left">主机名称&lt;/th>
 &lt;th style="text-align: left">角色&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: left">192.168.1.10&lt;/td>
 &lt;td style="text-align: left">master1&lt;/td>
 &lt;td style="text-align: left">master, node, etcd&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">192.168.1.11&lt;/td>
 &lt;td style="text-align: left">master2&lt;/td>
 &lt;td style="text-align: left">master, node, etcd&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">192.168.1.12&lt;/td>
 &lt;td style="text-align: left">master3&lt;/td>
 &lt;td style="text-align: left">master, node, etcd&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>注意:本教程仅作部署演示，在生产环境建议角色分离，单独部署 etcd 和 node 节点，提高稳定性。&lt;/p></description></item><item><title>Kubernetes client-go 源码分析 - Indexer &amp; ThreadSafeStore</title><link>https://openksc.github.io/zh/blogs/kubernetes-client-go-indexer-threadsafestore/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubernetes-client-go-indexer-threadsafestore/</guid><description>&lt;h2 id="概述">概述&lt;/h2>
&lt;blockquote>
&lt;p>源码版本信息&lt;/p>
&lt;ul>
&lt;li>Project: kubernetes&lt;/li>
&lt;li>Branch: master&lt;/li>
&lt;li>Last commit id: d25d741c&lt;/li>
&lt;li>Date: 2021-09-26&lt;/li>
&lt;/ul>&lt;/blockquote>
&lt;p>自定义控制器涉及到的 client-go 组件整体工作流程，大致如下图：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubernetes-client-go.png" alt="">&lt;/p>
&lt;p>Indexer 主要依赖于 ThreadSafeStore 实现，是 client-go 提供的一种缓存机制，通过检索本地缓存可以有效降低 apiserver 的压力，今天我们来详细看下 Indexer 和对应的 ThreadSafeStore 的实现。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/threadsafemap.png" alt="">&lt;/p>
&lt;h2 id="indexer-接口">Indexer 接口&lt;/h2>
&lt;p>Indexer 接口主要是在 Store 接口的基础上拓展了对象的检索功能：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>client-go/tools/cache/index.go:35&lt;/strong>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">Indexer&lt;/span> &lt;span style="color:#66d9ef">interface&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Store&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Index&lt;/span>(&lt;span style="color:#a6e22e">indexName&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>, &lt;span style="color:#a6e22e">obj&lt;/span> &lt;span style="color:#66d9ef">interface&lt;/span>{}) ([]&lt;span style="color:#66d9ef">interface&lt;/span>{}, &lt;span style="color:#66d9ef">error&lt;/span>) &lt;span style="color:#75715e">// 根据索引名和给定的对象返回符合条件的所有对象&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">IndexKeys&lt;/span>(&lt;span style="color:#a6e22e">indexName&lt;/span>, &lt;span style="color:#a6e22e">indexedValue&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>) ([]&lt;span style="color:#66d9ef">string&lt;/span>, &lt;span style="color:#66d9ef">error&lt;/span>) &lt;span style="color:#75715e">// 根据索引名和索引值返回符合条件的所有对象的 key&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ListIndexFuncValues&lt;/span>(&lt;span style="color:#a6e22e">indexName&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>) []&lt;span style="color:#66d9ef">string&lt;/span> &lt;span style="color:#75715e">// 列出索引函数计算出来的所有索引值&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ByIndex&lt;/span>(&lt;span style="color:#a6e22e">indexName&lt;/span>, &lt;span style="color:#a6e22e">indexedValue&lt;/span> &lt;span style="color:#66d9ef">string&lt;/span>) ([]&lt;span style="color:#66d9ef">interface&lt;/span>{}, &lt;span style="color:#66d9ef">error&lt;/span>) &lt;span style="color:#75715e">// 根据索引名和索引值返回符合条件的所有对象&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">GetIndexers&lt;/span>() &lt;span style="color:#a6e22e">Indexers&lt;/span> &lt;span style="color:#75715e">// 获取所有的 Indexers，对应 map[string]IndexFunc 类型&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">AddIndexers&lt;/span>(&lt;span style="color:#a6e22e">newIndexers&lt;/span> &lt;span style="color:#a6e22e">Indexers&lt;/span>) &lt;span style="color:#66d9ef">error&lt;/span> &lt;span style="color:#75715e">// 这个方法要在数据加入存储前调用，添加更多的索引方法，默认只通过 namespace 检索&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Indexer 的默认实现是 cache：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">cache&lt;/span> &lt;span style="color:#66d9ef">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">cacheStorage&lt;/span> &lt;span style="color:#a6e22e">ThreadSafeStore&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">keyFunc&lt;/span> &lt;span style="color:#a6e22e">KeyFunc&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>cache 对应两个方法体实现完全一样的 New 函数：&lt;/p></description></item><item><title>Kubernetes client-go 源码分析 - workqueue</title><link>https://openksc.github.io/zh/blogs/kubernetes-client-go-workqueue/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubernetes-client-go-workqueue/</guid><description>&lt;h2 id="概述">概述&lt;/h2>
&lt;blockquote>
&lt;p>源码版本信息&lt;/p>
&lt;ul>
&lt;li>Project: kubernetes&lt;/li>
&lt;li>Branch: master&lt;/li>
&lt;li>Last commit id: d25d741c&lt;/li>
&lt;li>Date: 2021-09-26&lt;/li>
&lt;/ul>&lt;/blockquote>
&lt;p>自定义控制器涉及到的 client-go 组件整体工作流程，大致如下图：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubernetes-client-go.png" alt="">&lt;/p>
&lt;p>今天我们来详细研究下 workqueue 相关代码。client-go 的 util/workqueue 包里主要有三个队列，分别是普通队列，延时队列，限速队列，后一个队列以前一个队列的实现为基础，层层添加新功能，我们按照 Queue、DelayingQueue、RateLimitingQueue 的顺序层层拨开来看限速队列是如何实现的。&lt;/p>
&lt;h2 id="queue">Queue&lt;/h2>
&lt;h3 id="接口和结构体">接口和结构体&lt;/h3>
&lt;p>先看接口定义：&lt;/p>
&lt;ul>
&lt;li>k8s.io/client-go/util/workqueue/queue.go:26&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">Interface&lt;/span> &lt;span style="color:#66d9ef">interface&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Add&lt;/span>(&lt;span style="color:#a6e22e">item&lt;/span> &lt;span style="color:#66d9ef">interface&lt;/span>{}) &lt;span style="color:#75715e">// 添加一个元素&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Len&lt;/span>() &lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#75715e">// 元素个数&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Get&lt;/span>() (&lt;span style="color:#a6e22e">item&lt;/span> &lt;span style="color:#66d9ef">interface&lt;/span>{}, &lt;span style="color:#a6e22e">shutdown&lt;/span> &lt;span style="color:#66d9ef">bool&lt;/span>) &lt;span style="color:#75715e">// 获取一个元素，第二个返回值和 channel 类似，标记队列是否关闭了&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Done&lt;/span>(&lt;span style="color:#a6e22e">item&lt;/span> &lt;span style="color:#66d9ef">interface&lt;/span>{}) &lt;span style="color:#75715e">// 标记一个元素已经处理完&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ShutDown&lt;/span>() &lt;span style="color:#75715e">// 关闭队列&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">ShuttingDown&lt;/span>() &lt;span style="color:#66d9ef">bool&lt;/span> &lt;span style="color:#75715e">// 是否正在关闭&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这个基础的队列接口定义很清晰，我们继续来看其实现的类型：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">type&lt;/span> &lt;span style="color:#a6e22e">Type&lt;/span> &lt;span style="color:#66d9ef">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">queue&lt;/span> []&lt;span style="color:#a6e22e">t&lt;/span> &lt;span style="color:#75715e">// 定义元素的处理顺序，里面所有元素都应该在 dirty set 中有，而不能出现在 processing set 中&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">dirty&lt;/span> &lt;span style="color:#a6e22e">set&lt;/span> &lt;span style="color:#75715e">// 标记所有需要被处理的元素&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">processing&lt;/span> &lt;span style="color:#a6e22e">set&lt;/span> &lt;span style="color:#75715e">// 当前正在被处理的元素，当处理完后需要检查该元素是否在 dirty set 中，如果有则添加到 queue 里&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">cond&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">sync&lt;/span>.&lt;span style="color:#a6e22e">Cond&lt;/span> &lt;span style="color:#75715e">// 条件锁&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">shuttingDown&lt;/span> &lt;span style="color:#66d9ef">bool&lt;/span> &lt;span style="color:#75715e">// 是否正在关闭&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">metrics&lt;/span> &lt;span style="color:#a6e22e">queueMetrics&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">unfinishedWorkUpdatePeriod&lt;/span> &lt;span style="color:#a6e22e">time&lt;/span>.&lt;span style="color:#a6e22e">Duration&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">clock&lt;/span> &lt;span style="color:#a6e22e">clock&lt;/span>.&lt;span style="color:#a6e22e">Clock&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Queue 的工作逻辑大致是这样，里面的三个属性 queue、dirty、processing 都保存 items，但是含义有所不同：&lt;/p></description></item><item><title>Kubernetes CNI 插件选型和应用场景探讨</title><link>https://openksc.github.io/zh/blogs/kubernetes-cni/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubernetes-cni/</guid><description>&lt;blockquote>
&lt;p>作者：马伟，青云科技容器顾问，云原生爱好者，目前专注于云原生技术，云原生领域技术栈涉及 Kubernetes、KubeSphere、KubeKey 等。&lt;/p>&lt;/blockquote>
&lt;p>本文介绍容器环境常见网络应用场景及对应场景的 Kubernetes CNI 插件功能实现。帮助搭建和使用云原生环境的小伙伴快速选择心仪的网络工具。&lt;/p>
&lt;h2 id="常见网络插件">常见网络插件&lt;/h2>
&lt;p>我们在学习容器网络的时候，肯定都听说过 Docker 的 bridge 网络，Vethpair，VxLAN 等术语，从 Docker 到 kubernetes 后，学习了 Flannel、Calico 等主流网络插件，分别代表了 Overlay 和 Underlay 的两种网络传输模式，也是很经典的两款 CNI 网络插件。那么，还有哪些好用的 CNI 插件呢 ? 我们看看 CNCF Landscape:&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202211021601323.png" alt="">&lt;/p>
&lt;p>抛去商业版 CNI，此次分享来聊聊几款热门开源 CNI 插件，分别为 &lt;a href="https://www.kube-ovn.io/" target="_blank" rel="noopener noreferrer">Kube-OVN&lt;/a>、&lt;a href="https://antrea.io/" target="_blank" rel="noopener noreferrer">Antrea&lt;/a>、&lt;a href="https://cilium.io/" target="_blank" rel="noopener noreferrer">Cilium&lt;/a>。Kube-OVN 和 Antrea 都是基于 OpenvSwitch 的项目，Cilium 使用 eBPF 这款革命性的技术作为数据路径，亦是这两年很火热的一个开源容器项目。&lt;/p>
&lt;p>那么，又回到学习新产品的第一步，如何快速部署 K8s 体验不同地 CNI 插件呢？还是交给我们亲爱的 Kubekey 吧。&lt;/p>
&lt;p>&lt;a href="https://github.com/kubesphere/kubekey" target="_blank" rel="noopener noreferrer">Kubekey&lt;/a> 作为一个开源的 Kubernetes 和 &lt;a href="https://kubesphere.com.cn/" target="_blank" rel="noopener noreferrer">KubeSphere&lt;/a> 集群部署工具，可以轻松的部署 Kubernetes 集群，提供节点管理、操作系统安全加固、容器运行时管理、网络存储安装、Etcd 管理等。Kubekey 支持一键部署 Calico / Flannel / Cilium / Kube-OVN 等网络插件，只需在 kk 的配置文件中注明 network 的 plugin 值即可：&lt;/p></description></item><item><title>Kubernetes Ingress 配置泛域名 TLS 证书</title><link>https://openksc.github.io/zh/blogs/kubesphere-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-ssl/</guid><description>&lt;blockquote>
&lt;p>作者：scwang18，主要负责技术架构，在容器云方向颇有研究。&lt;/p>&lt;/blockquote>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>KubeSphere 集群默认安装的证书是自签发证书，浏览器访问访问会发出安全提醒。本文记录了利用 &lt;code>let's encrytp&lt;/code> 泛域名证书实现 Kubernetes 集群外部服务自动证书配置和证书到期自动更新，支持 HTTPS 访问。我们还部署了证书自动分发组件，实现证书文件自动分发到其他 namespace 。&lt;/p>
&lt;h2 id="架构">架构&lt;/h2>
&lt;p>在 KubeSphere 集群中使用 HTTPS 协议，需要一个证书管理器、一个证书自动签发服务。&lt;/p>
&lt;p>cert-manager 是一个云原生证书管理开源项目，用于在 KubeSphere 集群中提供 HTTPS 证书并自动续期，支持 &lt;code>Let’s Encrypt&lt;/code>, &lt;code>HashiCorp Vault&lt;/code> 这些免费证书的签发。在 KubeSphere 集群中，我们可以通过 Kubernetes Ingress 和 &lt;code>Let’s Encrypt&lt;/code> 实现外部服务的自动化 HTTPS。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/high-level-overview.svg" alt="">&lt;/p>
&lt;p>&lt;strong>Issuers/ClusterIssuers&lt;/strong>：定义使用什么证书颁发机构 (CA) 来去颁发证书，Issuers 和 ClusterIssuers 区别是： issuers 是一个名称空间级别的资源，只能用来签发自己所在 namespace 下的证书，ClusterIssuer 是个集群级别的资源 可以签发任意 namespace 下的证书&lt;/p>
&lt;p>&lt;strong>Certificate&lt;/strong>：定义所需的 X.509 证书，该证书将更新并保持最新。Certificate 是一个命名空间资源，当 Certificate 被创建时，它会去创建相应的 CertificateRequest 资源来去申请证书。&lt;/p>
&lt;h2 id="安装证书管理器">安装证书管理器&lt;/h2>
&lt;p>安装证书管理器比较简单，直接执行以下脚本就可以了。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl create ns cert-manager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ helm uninstall cert-manager -n cert-manager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ helm install cert-manager jetstack/cert-manager &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -n cert-manager &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --version v1.8.0 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --set installCRDs&lt;span style="color:#f92672">=&lt;/span>true &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --set prometheus.enabled&lt;span style="color:#f92672">=&lt;/span>false &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --set &lt;span style="color:#e6db74">&amp;#39;extraArgs={--dns01-recursive-nameservers-only,--dns01-recursive-nameservers=119.29.29.29:53\,8.8.8.8:53}&amp;#39;&lt;/span> 
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="选择证书颁发者">选择证书颁发者&lt;/h2>
&lt;p>cert-manager 支持以下几种证书颁发者：&lt;/p></description></item><item><title>Kubernetes 安全及配置问题检测工具 KubeEye 使用教程</title><link>https://openksc.github.io/zh/blogs/k8s-kubeeye-security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/k8s-kubeeye-security/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>KubeEye 是一款 Kubernetes 安全及配置问题检测工具，针对部署在 K8s 集群中的业务应用进行配置检测使用 &lt;a href="https://github.com/open-policy-agent/opa" target="_blank" rel="noopener noreferrer">OPA&lt;/a>,针对集群部署的 Node 使用 &lt;a href="https://github.com/kubernetes/node-problem-detector" target="_blank" rel="noopener noreferrer">Node-Problem-Detector&lt;/a> 进行检测，同时除了系统内置有根据大多数业界常见场景的预定义规则，还支持用户自定义规则来进行集群检测。&lt;/p>
&lt;h2 id="架构">架构&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubeeye-architecture.png" alt="">&lt;/p>
&lt;p>KubeEye 通过调用 Kubernetes API，通过匹配资源中的关键字和容器语法的规则匹配来获取集群诊断数据，详见架构图。&lt;/p>
&lt;p>其中针对 Node 节点的检测，需要在被检测 Node 主机上安装。&lt;/p>
&lt;h2 id="特点">特点&lt;/h2>
&lt;h3 id="特性">特性&lt;/h3>
&lt;ul>
&lt;li>KubeEye 根据行业最佳实践审查你的工作负载 YAML 规范，帮助你使你的集群稳定。&lt;/li>
&lt;li>KubeEye 可以发现你的集群控制平面的问题，包括 kube-apiserver/kube-controller-manager/etcd 等。&lt;/li>
&lt;li>KubeEye 可以帮助你检测各种节点问题，包括内存/CPU/磁盘压力，意外的内核错误日志等。&lt;/li>
&lt;/ul>
&lt;h3 id="检查项">检查项&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>是/否&lt;/th>
 &lt;th>检查项&lt;/th>
 &lt;th>描述&lt;/th>
 &lt;th>级别&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>PrivilegeEscalationAllowed&lt;/td>
 &lt;td>允许特权升级&lt;/td>
 &lt;td>紧急&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>CanImpersonateUser&lt;/td>
 &lt;td>role/clusterrole 有伪装成其他用户权限&lt;/td>
 &lt;td>警告&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>CanDeleteResources&lt;/td>
 &lt;td>role/clusterrole 有删除 Kubernetes 资源权限&lt;/td>
 &lt;td>警告&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>CanModifyWorkloads&lt;/td>
 &lt;td>role/clusterrole 有修改 Kubernetes 资源权限&lt;/td>
 &lt;td>警告&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>NoCPULimits&lt;/td>
 &lt;td>资源没有设置 CPU 使用限制&lt;/td>
 &lt;td>紧急&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>NoCPURequests&lt;/td>
 &lt;td>资源没有设置预留 CPU&lt;/td>
 &lt;td>紧急&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>HighRiskCapabilities&lt;/td>
 &lt;td>开启了高危功能，例如 ALL/SYS_ADMIN/NET_ADMIN&lt;/td>
 &lt;td>紧急&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>HostIPCAllowed&lt;/td>
 &lt;td>开启了主机 IPC&lt;/td>
 &lt;td>紧急&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>HostNetworkAllowed&lt;/td>
 &lt;td>开启了主机网络&lt;/td>
 &lt;td>紧急&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>HostPIDAllowed&lt;/td>
 &lt;td>开启了主机 PID&lt;/td>
 &lt;td>紧急&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>HostPortAllowed&lt;/td>
 &lt;td>开启了主机端口&lt;/td>
 &lt;td>紧急&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>ImagePullPolicyNotAlways&lt;/td>
 &lt;td>镜像拉取策略不是 always&lt;/td>
 &lt;td>警告&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>ImageTagIsLatest&lt;/td>
 &lt;td>镜像标签是 latest&lt;/td>
 &lt;td>警告&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>ImageTagMiss&lt;/td>
 &lt;td>镜像没有标签&lt;/td>
 &lt;td>紧急&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>InsecureCapabilities&lt;/td>
 &lt;td>开启了不安全的功能，例如 KILL/SYS_CHROOT/CHOWN&lt;/td>
 &lt;td>警告&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>NoLivenessProbe&lt;/td>
 &lt;td>没有设置存活状态检查&lt;/td>
 &lt;td>警告&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>NoMemoryLimits&lt;/td>
 &lt;td>资源没有设置内存使用限制&lt;/td>
 &lt;td>紧急&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>NoMemoryRequests&lt;/td>
 &lt;td>资源没有设置预留内存&lt;/td>
 &lt;td>紧急&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>NoPriorityClassName&lt;/td>
 &lt;td>没有设置资源调度优先级&lt;/td>
 &lt;td>通知&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>PrivilegedAllowed&lt;/td>
 &lt;td>以特权模式运行资源&lt;/td>
 &lt;td>紧急&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>NoReadinessProbe&lt;/td>
 &lt;td>没有设置就绪状态检查&lt;/td>
 &lt;td>警告&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>NotReadOnlyRootFilesystem&lt;/td>
 &lt;td>没有设置根文件系统为只读&lt;/td>
 &lt;td>警告&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>NotRunAsNonRoot&lt;/td>
 &lt;td>没有设置禁止以 root 用户启动进程&lt;/td>
 &lt;td>警告&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>CertificateExpiredPeriod&lt;/td>
 &lt;td>将检查 API Server 证书的到期日期少于 30 天&lt;/td>
 &lt;td>紧急&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>EventAudit&lt;/td>
 &lt;td>事件检查&lt;/td>
 &lt;td>警告&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>NodeStatus&lt;/td>
 &lt;td>节点状态检查&lt;/td>
 &lt;td>警告&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>DockerStatus&lt;/td>
 &lt;td>Docker 状态检查&lt;/td>
 &lt;td>警告&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>✅&lt;/td>
 &lt;td>KubeletStatus&lt;/td>
 &lt;td>Kubelet 状态检查&lt;/td>
 &lt;td>警告&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h2 id="部署">部署&lt;/h2>
&lt;p>KubeEye 本身使用 Golang 编写，可使用编译好的二进制可执行文件进行相关组件安装。&lt;/p></description></item><item><title>Kubernetes 备份容灾服务产品体验教程</title><link>https://openksc.github.io/zh/blogs/k8s-backup-disater-recovery-service/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/k8s-backup-disater-recovery-service/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>Kubernetes 集群天生自带自愈功能，但是往往有些意外情况使自愈功能不起作用，比如：公司同事把某个 namespace 删除、存储对象被清理了、集群突然断电了、集群升级失败了等。如果没有好的备份工具及定时备份的习惯，不管对于开发环境还是生产环境来说无疑都是灾难性的，如果这个时候有一个可视化备份工具友好的帮助集群做定时备份，你的工作会事半功倍。下面就给大家推荐青云科技容器团队基于 Velero 开源备份工具研发的备份容灾服务。&lt;/p>
&lt;h2 id="云原生备份容灾服务简介">云原生备份容灾服务简介&lt;/h2>
&lt;p>KubeSphere Cloud 云原生备份容灾服务是 KubeSphere 团队针对混合云场景推出的 Kubernetes 备份容灾即服务产品。用户无需构建备份容灾的基础架构，基于原生的 Kubernetes API，提供了可视化界面，能够覆盖云原生数据保护的绝大多数重要场景，而且能够跨集群、跨云服务商、跨存储区域，轻松实现基础设施间多地、按需的备份恢复。登录 &lt;a href="https://kubesphere.cloud" target="_blank" rel="noopener noreferrer">KubeSphere Cloud&lt;/a> 即可对 Kubernetes 集群中的容器进行备份和恢复。&lt;/p>
&lt;h2 id="注册平台账号">注册平台账号&lt;/h2>
&lt;ol>
&lt;li>登录 KubeSphere Cloud 平台&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/0b57b576-8697-4359-82a0-77da35a3e95e.png" alt="">&lt;/p>
&lt;ol start="2">
&lt;li>创建账户&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/91fb46c3-8c89-49e3-8ede-ae2ab50b14e5.png" alt="">&lt;/p>
&lt;h2 id="准备集群">准备集群&lt;/h2>
&lt;ol>
&lt;li>进入首页找到【资源管理】选择【导入集群】&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/01936f12-55a3-4d56-bd99-3e2b9237d9eb.png" alt="">&lt;/p>
&lt;ol start="2">
&lt;li>填写集群相关信息，选择【直接连接 Kubernetes 集群】方式&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/042fc864-a519-4f1a-aa4b-10a2cf17b8c9.png" alt="">&lt;/p>
&lt;ol start="3">
&lt;li>获取 kubeconfig&lt;/li>
&lt;/ol>
&lt;p>方式一：托管 Kubernetes 集群&lt;/p>
&lt;p>请参考对应云厂商产品文档进行获取，如：阿里云、华为云、腾讯云等。&lt;/p>
&lt;p>方式二：自建 Kubernetes 集群&lt;/p>
&lt;p>（一）master 节点上执行&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat $HOME/.kube/config
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>(二)请确保 kubeconfig 中 cluser.server 字段的地址可以通过公网进行访问，或者同时勾选跳过 TLS 验证进行导入&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/d35d7d70-994f-4d8f-8b98-8d962f79d2c7.png" alt="">&lt;/p>
&lt;ol start="4">
&lt;li>验证集群连接状态&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/a913a41d-2cf5-4be0-8c75-a1d671473d8e.png" alt="">&lt;/p>
&lt;h2 id="添加对象存储仓库">添加对象存储仓库&lt;/h2>
&lt;ol>
&lt;li>选择【新建仓库】&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/7d1ca132-8950-4d0a-9212-c50912b92c47.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/8af1053c-dfe3-4d5c-9c04-2dc07693cc38.png" alt="">&lt;/p>
&lt;ol start="2">
&lt;li>查看仓库是否可用&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/9484d663-ec32-414c-ab73-d897b9e39748.png" alt="">&lt;/p>
&lt;h2 id="创建备份计划">创建备份计划&lt;/h2>
&lt;p>注意：备份的集群 namespace 里不能包含带有 error 的 PVC 或者 PV，否则无法恢复！！！&lt;/p></description></item><item><title>Kubernetes 持久化存储之 Rook Ceph 探究</title><link>https://openksc.github.io/zh/blogs/kubernetes-rook-ceph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubernetes-rook-ceph/</guid><description>&lt;p>在 Kubernetes 生态系统中，持久化存储是支撑业务应用稳定运行的基石，对于维护整个系统的健壮性至关重要。对于选择自主搭建 Kubernetes 集群的运维架构师来说，挑选合适的后端持久化存储解决方案是关键的选型决策。目前，Ceph、GlusterFS、NFS、Longhorn 和 openEBS 等解决方案已在业界得到广泛应用。&lt;/p>
&lt;p>为了丰富技术栈，并为容器云平台的持久化存储设计提供更广泛的灵活性和选择性，今天，我将带领大家一起探索，如何将 Ceph 集成到由 KubeSphere 管理的 Kubernetes 集群中。&lt;/p>
&lt;p>集成 Ceph 至 Kubernetes 集群主要有两种方案：&lt;/p>
&lt;ul>
&lt;li>利用 Rook Ceph 直接在 Kubernetes 集群上部署 Ceph 集群，这种方式更贴近云原生的应用特性。&lt;/li>
&lt;li>手动部署独立的 Ceph 集群，并配置 Kubernetes 集群与之对接，实现存储服务的集成。&lt;/li>
&lt;/ul>
&lt;p>本文将重点实战演示使用 Rook Ceph 在 Kubernetes 集群上直接部署 Ceph 集群的方法，让您体验到云原生环境下 Ceph 部署的便捷与强大。&lt;/p>
&lt;p>&lt;strong>实战服务器配置(架构 1:1 复刻小规模生产环境，配置略有不同)&lt;/strong>&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ksp-registry&lt;/td>
 &lt;td style="text-align: center">192.168.9.90&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Harbor 镜像仓库&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.94&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker/CI&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">400+&lt;/td>
 &lt;td style="text-align: center">Containerd、OpenEBS、ElasticSearch/Longhorn/Ceph/NFS&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.98&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">300+&lt;/td>
 &lt;td style="text-align: center">Containerd、OpenEBS、ElasticSearch/Longhorn/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.99&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">300+&lt;/td>
 &lt;td style="text-align: center">Containerd、OpenEBS、ElasticSearch/Longhorn/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gpu-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.101&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker(GPU NVIDIA Tesla M40 24G)&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gpu-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.102&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker(GPU NVIDIA Tesla P100 16G)&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gateway-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.103&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;td style="text-align: center">自建应用服务代理网关/VIP：192.168.9.100&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gateway-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.104&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;td style="text-align: center">自建应用服务代理网关/VIP：192.168.9.100&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-mid&lt;/td>
 &lt;td style="text-align: center">192.168.9.105&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">部署在 k8s 集群之外的服务节点（Gitlab 等）&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">15&lt;/td>
 &lt;td style="text-align: center">56&lt;/td>
 &lt;td style="text-align: center">152&lt;/td>
 &lt;td style="text-align: center">600&lt;/td>
 &lt;td style="text-align: center">2100+&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>实战环境涉及软件版本信息&lt;/strong>&lt;/p></description></item><item><title>Kubernetes 多行日志采集方案探索</title><link>https://openksc.github.io/zh/blogs/kubesphere-log-collection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-log-collection/</guid><description>&lt;blockquote>
&lt;p>作者：大飞哥，视源电子运维工程师，KubeSphere 用户委员会广州站站长&lt;/p>&lt;/blockquote>
&lt;h2 id="采集落盘日志">采集落盘日志&lt;/h2>
&lt;p>日志采集，通常使用 EFK 架构，即 &lt;code>ElasticSearch&lt;/code>,&lt;code>Filebeat&lt;/code>,&lt;code>Kibana&lt;/code>，这是在主机日志采集上非常成熟的方案，但在容器日志采集方面，整体方案就会复杂很多。我们现在面临的需求，就是要采集容器中的落盘日志。&lt;/p>
&lt;p>容器日志分为标准输出日志和落盘日志两种。应用将日志打印在容器标准输出 &lt;code>STDOUT&lt;/code> 中，由容器运行时(Docker 或 Containerd)把标准输出日志写入容器日志文件中，最终由采集器导出。这种日志打印采集是业界推荐方案。但对于不打印标准输出而直接将日志落盘的情况，业界最常用见的方案是，使用 &lt;code>Sidecar&lt;/code> 采集落盘日志，把落盘日志打印到容器标准输出中，再利用标准输出日志的采集方式输出。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/bb596565862f45f7852421c50d9e88b1~tplv-k3u1fbpfcp-watermark.png" alt="">&lt;/p>
&lt;p>对于 KubeSphere 用户，只需要两步即可：第一在项目中开启&lt;code>收集卷上日志&lt;/code>，第二在工作负载中配置落盘文件路径。具体操作见下图所示。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/e0b20d93902f4545826cacbc4541f054~tplv-k3u1fbpfcp-watermark.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/4d93e799977840628dd45d0a19493d9b~tplv-k3u1fbpfcp-watermark.png" alt="">&lt;/p>
&lt;p>上述两个步骤，会自动在容器中注入 &lt;code>Filebeat Sidecar&lt;/code> 作为 logging-agent，将落盘日志打印输出在容器标准输出中。Filebeat 配置可通过 ConfigMap 修改。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ kubectl get cm -n kubesphere-logging-system logsidecar-injector-configmap -o yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">## Filebeat 配置&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">filebeat.inputs&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">log&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">enabled&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">paths&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {{&lt;span style="color:#ae81ff">range .Paths}}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - {{&lt;span style="color:#ae81ff">.}}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {{&lt;span style="color:#ae81ff">end}}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">output.console&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">codec.format&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">string&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;%{[log.file.path]} %{[message]}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">logging.level&lt;/span>: &lt;span style="color:#ae81ff">warning&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="接入第三方日志服务">接入第三方日志服务&lt;/h2>
&lt;p>默认 KubeSphere 将日志采集到集群内置 Elasticsearch 中，数据保存周期为 7 天，这对于生产服务动辄 180 天的日志存储需求，显然无法满足。企业运维团队都会建立集中化的日志服务，将集群内日志接入到第三方日志服务中，已是必然选择。我们来看如何操作。&lt;/p>
&lt;p>上文说到，容器运行时会将标准输出日志，落盘写入到集群节点的日志文件中，Linux 系统默认在 &lt;code>/var/log/containers/*.log&lt;/code>。KubeSphere 使用 &lt;code>FluentBit&lt;/code> 以 &lt;code>DemonSet&lt;/code> 形式在各集群节点上采集日志，由 FluentBit 输出给 ElasticSearch 服务。具体配置可参考如下两个配置：&lt;/p></description></item><item><title>Kubernetes 集群中 Ingress 故障的根因诊断</title><link>https://openksc.github.io/zh/blogs/kubesphere-ingress-fault-diagnosis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-ingress-fault-diagnosis/</guid><description>&lt;blockquote>
&lt;p>作者：scwang18，主要负责技术架构，在容器云方向颇有研究。&lt;/p>&lt;/blockquote>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>KubeSphere 是青云开源的基于 Kubernetes 的云原生分布式操作系统，提供了比较炫酷的 Kubernetes 集群管理界面，我们团队用 KubeSphere 来作为开发平台。&lt;/p>
&lt;p>本文记录了一次 KubeSphere 环境下的网络故障的解决过程。&lt;/p>
&lt;h2 id="现象">现象&lt;/h2>
&lt;p>开发同学反馈自己搭建的 Harbor 仓库总是出问题，偶尔会报 &lt;code>net/http: TLS handshake timeout&lt;/code> ， 通过 curl 的方式访问 harbor.xxxx.cn ，也会随机频繁挂起。但是 ping 的反馈一切正常。&lt;/p>
&lt;h2 id="原因分析">原因分析&lt;/h2>
&lt;p>接到错误报障后，经过了多轮分析，才最终定位到原因，应该是安装 KubeSphere 时，使用了最新版的 Kubernetes 1.23.1 。&lt;/p>
&lt;p>虽然使用 &lt;code> ./kk version --show-supported-k8s&lt;/code> 可以看到 KubeSphere 3.2.1 可以支持 Kubernetes 1.23.1 ，但实际上只是试验性支持，有坑的。&lt;/p>
&lt;p>分析过程如下：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>出现 Harbor registry 访问问题，下意识以为是 Harbor 部署有问题，但是在检查 Harbor core 的日志的时候，没有看到异常时有相应错误信息，甚至 info 级别的日志信息都没有。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>又把目标放在 Harbor portal， 查看访问日志，一样没有发现异常信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>根据访问链，继续追查 kubesphere-router-kubesphere-system ， 即 KubeSphere 版的 nginx ingress controller ，同样没有发现异常日志。&lt;/p></description></item><item><title>Kubernetes 集群中流量暴露的几种方案</title><link>https://openksc.github.io/zh/blogs/kubesphere-loadbalance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-loadbalance/</guid><description>&lt;blockquote>
&lt;p>作者：KaliArch（薛磊），某 Cloud MSP 服务商产品负责人，熟悉企业级高可用 / 高并发架构，包括混合云架构、异地灾备，熟练企业 DevOps 改造优化，熟悉 Shell/Python/Go 等开发语言，熟悉 Kubernetes、 Docker、云原生、微服务架构等。&lt;/p>&lt;/blockquote>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>在业务使用 Kubernetes 进行编排管理时，针对业务的南北流量的接入，在 Kuberentes 中通常有几种方案，本文就接入的方案进行简单介绍。&lt;/p>
&lt;h2 id="流量接入方案">流量接入方案&lt;/h2>
&lt;p>Kuberentes 社区通过为集群增设入口点的方案，解决对外流量的管理。&lt;/p>
&lt;h3 id="通过-kube-proxy-进行代理">通过 kube-proxy 进行代理&lt;/h3>
&lt;p>通常在最简单的测试或个人开发环境，可以通过 &lt;code>kubectl port-forward&lt;/code> 来启动一个 kube-proxy 进程代理内部的服务至该命令执行的宿主机节点，如果该宿主机具备公网 IP，且转发监听端口为 &lt;code>0.0.0.0&lt;/code> 就可以实现公网访问该服务，该方式可以代理单个 Pod，或者 Deployment，或者 Servcie。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl port-forward -h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Forward one or more local ports to a pod. This command requires the node to have &lt;span style="color:#e6db74">&amp;#39;socat&amp;#39;&lt;/span> installed.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Use resource type/name such as deployment/mydeployment to &lt;span style="color:#66d9ef">select&lt;/span> a pod. Resource type defaults to &lt;span style="color:#e6db74">&amp;#39;pod&amp;#39;&lt;/span> &lt;span style="color:#66d9ef">if&lt;/span> omitted.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> If there are multiple pods matching the criteria, a pod will be selected automatically. The forwarding session ends
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>when the selected pod terminates, and rerun of the command is needed to resume forwarding.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Examples:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Listen on ports 5000 and 6000 locally, forwarding data to/from ports 5000 and 6000 in the pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubectl port-forward pod/mypod &lt;span style="color:#ae81ff">5000&lt;/span> &lt;span style="color:#ae81ff">6000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Listen on ports 5000 and 6000 locally, forwarding data to/from ports 5000 and 6000 in a pod selected by the&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>deployment
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubectl port-forward deployment/mydeployment &lt;span style="color:#ae81ff">5000&lt;/span> &lt;span style="color:#ae81ff">6000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Listen on port 8443 locally, forwarding to the targetPort of the service&amp;#39;s port named &amp;#34;https&amp;#34; in a pod selected by&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>the service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubectl port-forward service/myservice 8443:https
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Listen on port 8888 locally, forwarding to 5000 in the pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubectl port-forward pod/mypod 8888:5000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Listen on port 8888 on all addresses, forwarding to 5000 in the pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubectl port-forward --address 0.0.0.0 pod/mypod 8888:5000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Listen on port 8888 on localhost and selected IP, forwarding to 5000 in the pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubectl port-forward --address localhost,10.19.21.23 pod/mypod 8888:5000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Listen on a random port locally, forwarding to 5000 in the pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubectl port-forward pod/mypod :5000
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="nodeport-方式">NodePort 方式&lt;/h3>
&lt;p>其次较常用的为 NodePort 方式，将 K8s 中 service 的类型修改为 NodePort 方式，会得到一个端口范围在 30000-32767 端口范围内的宿主机端口，同样改宿主机具有公网 IP 就可以实现对服务的暴露，但是 NodePort 会占用宿主机端口，一个 Service 对应一个 NodePort，该方式仅为四层，无法实现 SSL 证书的卸载，如果将服务转发到单个 Node 节点的 NodePort 也无法实现高可用，一般需要在 NodePort 前搭配负载均衡来添加多个后端 NodePort 已实现高可用。&lt;/p></description></item><item><title>Kubernetes 跨 StorageClass 迁移 Persistent Volumes 完全指南</title><link>https://openksc.github.io/zh/blogs/manual-kubesphere-persistentvolumes-migration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/manual-kubesphere-persistentvolumes-migration/</guid><description>&lt;p>KubeSphere 3.3.0 （不出意外的话~）下周就要 GA 了，作为一名 KubeSphere 脑残粉，我迫不及待地先安装 &lt;a href="https://github.com/kubesphere/kubesphere/releases/tag/v3.3.0-rc.2" target="_blank" rel="noopener noreferrer">RC 版&lt;/a>尝尝鲜，一顿操作猛如虎开启所有组件，装完之后发现有点尴尬：我用错了持久化存储。&lt;/p>
&lt;p>我的 K8s 集群中有两个存储类（StorageClass），一个是 OpenEBS 提供的本地存储，另一个是 &lt;a href="https://github.com/yunify/qingcloud-csi" target="_blank" rel="noopener noreferrer">QingCloud CSI&lt;/a> 提供的分布式存储，而且默认的 StorageClass 是 OpenEBS 提供的 local-hostpath，所以 KubeSphere 的有状态组件默认便使用本地存储来保存数据。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202206162226279.png" alt="">&lt;/p>
&lt;p>失误失误，我本来是想用分布式存储作为默认存储的，但是我忘记将 csi-qingcloud 设置为默认的 StorageClass 了，反正不管怎样，就这么稀里糊涂地搞错了。&lt;strong>虽然重装可以解决 99% 的问题，但作为一名成熟的 YAML 工程师，重装是不可能的，必须在不重装的情况下解决这个问题，才能体现出我的气质！&lt;/strong>&lt;/p>
&lt;p>事实上不止我一个人遇到过这种情况，很多人都会稀里糊涂地装完一整套产品之后发现 StorageClass 用错了，这时候再想改回去恐怕就没那么容易了。这不巧了么这不是，本文就是来帮助大家解决这个问题的。&lt;/p>
&lt;h2 id="思路">思路&lt;/h2>
&lt;p>我们先来思考一下换 StorageClass 需要做哪几件事情。首先需要将应用的副本数缩减为 0，然后创建一个新的 PVC，将旧 PV 的数据复制到新 PV，然后让应用使用新的 PV，并将副本扩展到原来的数量，最后再将旧 PV 删除。在这整个过程中还要防止删除 PVC 时 Kubernetes 将 PV 也删除了。&lt;/p>
&lt;p>当然，有些 CSI 驱动或者存储后端可能会有更便利的数据迁移技巧，但是本文提供的是一种更加通用的方案，不管后端是什么存储都可以。&lt;/p>
&lt;p>KubeSphere 3.3.0 开启所有组件之后使用的持久卷声明（PVC）如下：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202206162257307.jpg" alt="">&lt;/p>
&lt;p>本文就以 Elasticsearch 为例，演示如何将 Elasticsearch 的存储从本地存储替换为分布式存储。&lt;/p>
&lt;h2 id="备份-pvc-和-pv">备份 PVC 和 PV&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202206171525327.png" alt="">&lt;/p></description></item><item><title>Kubernetes 升级不弃 Docker：KubeKey 的丝滑之道</title><link>https://openksc.github.io/zh/blogs/upgrading-k8s-with-kubekey/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/upgrading-k8s-with-kubekey/</guid><description>&lt;blockquote>
&lt;p>作者：尹珉，KubeSphere Ambaasador&amp;amp;Contributor，KubeSphere 社区用户委员会杭州站站长。&lt;/p>&lt;/blockquote>
&lt;h2 id="引言">引言&lt;/h2>
&lt;p>随着 Kubernetes 社区的不断发展，即将迎来 Kubernetes 1.30 版本的迭代。在早先的 1.24 版本中，社区作出一个重要决策：不再默认集成 Docker 作为容器运行时，即取消了对 Docker 的默认支持。这就像咱们家厨房换了个新灶头，虽然厨艺的本质没变，但用起来感觉肯定不一样。这篇文章就带你摸透这个变化，直击 Kubernetes 1.24+ 版本抛弃 Docker 后的影响，同时手把手教你如何借助 KubeKey 这个神器，让你在给 Kubernetes “装修升级”的过程中既稳又顺，还能把 Docker 那些贴心好用的功能保留下来。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/image-20240409-0.webp" alt="">&lt;/p>
&lt;h2 id="docker-移除带来的潜在风险分析">Docker 移除带来的潜在风险分析&lt;/h2>
&lt;h3 id="工具链与生态兼容性">工具链与生态兼容性&lt;/h3>
&lt;ol>
&lt;li>对于大量使用 Jenkins 等 CI/CD 工具的企业而言，原先基于 Docker 的镜像构建、推送和拉取流程可能需要重构。Jenkinsfile 中的 Docker 构建步骤需调整为兼容 containerd 的方式进行，否则可能造成流水线中断。&lt;/li>
&lt;li>监控系统和其他依赖于 Docker API 的周边工具需要进行改造或更换，以适应新的容器运行时环境，这涉及到了大量的验证工作和可能的二次开发成本。&lt;/li>
&lt;/ol>
&lt;h3 id="开发环境一致性">开发环境一致性&lt;/h3>
&lt;p>开发者们习惯了在本地使用 Docker 进行快速迭代和测试，移除 Docker 后，需要重新适应 containerd 或寻找兼容 Docker API 的替代方案，以保持开发环境与生产环境的一致性。&lt;/p>
&lt;h3 id="现有运维脚本失效">现有运维脚本失效&lt;/h3>
&lt;p>许多自动化脚本、运维命令和 Helm Chart 等资源文件可能直接引用了 Docker 命令或依赖于 Docker 的特定行为，这些都需要逐步审查和适配。&lt;/p>
&lt;h2 id="升集群时手动保留-docker-特性的成本分析">升集群时手动保留 Docker 特性的成本分析&lt;/h2>
&lt;h3 id="运维复杂度增加">运维复杂度增加&lt;/h3>
&lt;ol>
&lt;li>需要在 Kubernetes 集群中手动集成第三方插件或其他兼容方案以模拟 Docker 的运行时环境，这要求运维团队具备更高的技术水平和对 Kubernetes 内部机制的深入了解。&lt;/li>
&lt;li>需要密切关注 Kubernetes 更新与 Docker 兼容性之间的差异，每次升级 Kubernetes 都可能导致与 Docker 集成的部分出现问题，需要额外的时间和精力进行维护和调试。&lt;/li>
&lt;/ol>
&lt;h3 id="集群规模操作成本剧增">集群规模操作成本剧增&lt;/h3>
&lt;ol>
&lt;li>假设面临如 100 个节点的集群时，每个节点上的容器运行时切换都需要单独进行，这意味着至少需要分别在 100 个节点上执行启停容器运行时的操作，耗费巨大的人力和时间成本。&lt;/li>
&lt;li>对于大型集群，这种逐一操作的管理模式极其低效且容易出错，可能需要编写复杂的脚本或者使用批量管理工具，进一步增加实施难度。&lt;/li>
&lt;/ol>
&lt;h3 id="测试验证与恢复预案">测试验证与恢复预案&lt;/h3>
&lt;p>若操作过程中遇到问题，需要有完备的回滚策略和恢复预案，准备应对可能发生的各类异常状况，以防业务长时间受到影响。&lt;/p></description></item><item><title>Kubernetes 生产环境集群安装实践</title><link>https://openksc.github.io/zh/blogs/kubesphere-k8s-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-k8s-cluster/</guid><description>&lt;blockquote>
&lt;p>作者：老 Z，运维架构师，云原生爱好者，目前专注于云原生运维&lt;/p>&lt;/blockquote>
&lt;h2 id="前提说明">前提说明&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>本系列文档适用于中小规模 (&amp;lt;=50) 的 K8s 生产环境，大型环境没有经验，有待验证&lt;/p>
&lt;/li>
&lt;li>
&lt;p>所有节点采用云上虚拟机的方式部署&lt;/p>
&lt;/li>
&lt;li>
&lt;p>本系列文档没考虑 K8s 安全配置，安全要求高的环境不适用，后续会补充完善&lt;/p>
&lt;/li>
&lt;li>
&lt;p>本系列文档属于实践之路上的积累，会不断根据线上遇到的问题进行优化改进&lt;/p>
&lt;/li>
&lt;li>
&lt;p>本系列文档基于 KubeSphere 部署的 Kubernetes，后续的很多功能实现都依托于 KubeSphere&lt;/p>
&lt;/li>
&lt;li>
&lt;p>本系列文档涉及的 Ansible 代码可以在 &lt;a href="%22https://gitee.com/zdevops/cloudnative%22">https://gitee.com/zdevops/cloudnative&lt;/a> 获取&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="kubesphere-简介">&lt;a href="https://kubesphere.io/zh/" title="KubeSphere 简介" target="_blank" rel="noopener noreferrer">KubeSphere 简介&lt;/a>&lt;/h2>
&lt;h3 id="全栈的-k8s-容器云-paas-解决方案">全栈的 K8s 容器云 PaaS 解决方案&lt;/h3>
&lt;p>KubeSphere 是在 K8s 之上构建的以应用为中心的多租户容器平台，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。KubeSphere 提供了运维友好的向导式操作界面，帮助企业快速构建一个强大和功能丰富的容器云平台。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>完全开源&lt;/p>
&lt;p>通过 CNCF 一致性认证的 K8s 平台，100% 开源，由社区驱动与开发&lt;/p>
&lt;/li>
&lt;li>
&lt;p>简易安装&lt;/p>
&lt;p>支持部署在任何基础设施环境，提供在线与离线安装，支持一键升级与扩容集群&lt;/p>
&lt;/li>
&lt;li>
&lt;p>功能丰富&lt;/p>
&lt;p>在一个平台统一纳管 DevOps、云原生可观测性、服务网格、应用生命周期、多租户、多集群、存储与网络&lt;/p>
&lt;/li>
&lt;li>
&lt;p>模块化 &amp;amp; 可插拔&lt;/p>
&lt;p>平台中的所有功能都是可插拔与松耦合，您可以根据业务场景可选安装所需功能组件&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="选型理由-从运维的角度考虑">选型理由 (从运维的角度考虑)&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>安装简单，使用简单&lt;/strong>&lt;/li>
&lt;li>具备构建一站式企业级的 DevOps 架构与可视化运维能力 (省去自己用开源工具手工搭建积木)&lt;/li>
&lt;li>提供从平台到应用维度的日志、监控、事件、审计、告警与通知，实现集中式与多租户隔离的可观测性&lt;/li>
&lt;li>简化应用的持续集成、测试、审核、发布、升级与弹性扩缩容&lt;/li>
&lt;li>为云原生应用提供基于微服务的灰度发布、流量管理、网络拓扑与追踪&lt;/li>
&lt;li>提供易用的界面命令终端与图形化操作面板，满足不同使用习惯的运维人员&lt;/li>
&lt;li>可轻松解耦，避免厂商绑定&lt;/li>
&lt;/ul>
&lt;h2 id="部署架构图">部署架构图&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/k8s-on-kubesphere.svg" alt="">&lt;/p></description></item><item><title>KubeSphere + Argo CD，实现真正的 GitOps！</title><link>https://openksc.github.io/zh/blogs/kubesphere-argocd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-argocd/</guid><description>&lt;blockquote>
&lt;p>来自社区用户 willqy 的分享。&lt;/p>&lt;/blockquote>
&lt;h2 id="argo-cd-简介">Argo CD 简介&lt;/h2>
&lt;p>Argo CD 是用于 Kubernetes 的声明性 GitOps 持续交付工具，应用程序定义，配置和环境应为声明性的，并应受版本控制，应用程序部署和生命周期管理应该是自动化、可审核且易于理解。&lt;/p>
&lt;p>Argo CD 遵循 GitOps 模式，该模式使用 Git 仓库作为定义所需应用程序状态的真实来源。&lt;/p>
&lt;p>Argo CD 可在指定的目标环境中自动部署所需的应用程序状态，应用程序部署可以在 Git 提交时跟踪对分支，标签的更新，或固定到清单的特定版本。&lt;/p>
&lt;p>官网：&lt;a href="https://argoproj.github.io/" target="_blank" rel="noopener noreferrer">https://argoproj.github.io/&lt;/a>&lt;/p>
&lt;p>Argo CD 架构图：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/2020112717301714.png" alt="">&lt;/p>
&lt;p>Argo CD 被实现为 Kubernetes 控制器，该控制器持续监视正在运行的应用程序，并将当前的活动状态与所需的目标状态（在 Git 存储库中指定）进行比较。当已部署应用程序的运行状态偏离目标状态时将被 Argo CD 视为 OutOfSync。&lt;/p>
&lt;p>Argo CD 报告并可视化差异，同时提供了自动或手动将实时状态同步回所需目标状态的功能。在 Git 存储库中对所需目标状态所做的任何修改都可以自动应用并同步到指定的目标环境中。&lt;/p>
&lt;p>Argo CD 支持的 Kubernetes 配置清单包括 helm charts、kustomize 或纯 YAML/json 文件等。&lt;/p>
&lt;p>&lt;strong>本篇文章涉及内容：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>使用 KubeSphere DevOps 实现 CI 部分, CD 部分由 Argo CD 完成；&lt;/li>
&lt;li>Argo CD 持续监测 Git 仓库某个目录下 yaml 文件变动，自动将 yaml 文件部署到 K8s 集群；&lt;/li>
&lt;li>Argo CD 持续监测 Harbor 镜像仓库某个镜像 tag 变动，自动将最新镜像部署到 K8s 集群。&lt;/li>
&lt;/ul>
&lt;p>基本原理图：&lt;/p></description></item><item><title>KubeSphere 3.3.0 离线安装教程</title><link>https://openksc.github.io/zh/blogs/kubesphere-v3.3.0-offline-installation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-v3.3.0-offline-installation/</guid><description>&lt;blockquote>
&lt;p>作者：运维架构师，云原生爱好者，目前专注于云原生运维，云原生领域技术栈涉及 Kubernetes、KubeSphere、DevOps、OpenStack、Ansible 等。&lt;/p>&lt;/blockquote>
&lt;p>KubeKey 是一个用于部署 K8s 集群的开源轻量级工具。&lt;/p>
&lt;p>它提供了一种灵活、快速、便捷的方式来仅安装 Kubernetes/K3s，或同时安装 K8s/K3s 和 KubeSphere，以及其他云原生插件。除此之外，它也是扩展和升级集群的有效工具。&lt;/p>
&lt;p>KubeKey v2.1.0 版本新增了清单 (manifest) 和制品 (artifact) 的概念，为用户离线部署 K8s 集群提供了一种解决方案。&lt;/p>
&lt;p>manifest 是一个描述当前 K8s 集群信息和定义 artifact 制品中需要包含哪些内容的文本文件。&lt;/p>
&lt;p>在过去，用户需要准备部署工具，镜像 tar 包和其他相关的二进制文件，每位用户需要部署的 K8s 版本和需要部署的镜像都是不同的。现在使用 KubeKey，用户只需使用清单 manifest 文件来定义将要离线部署的集群环境需要的内容，再通过该 manifest 来导出制品 artifact 文件即可完成准备工作。离线部署时只需要 KubeKey 和 artifact 就可快速、简单的在环境中部署镜像仓库和 K8s 集群。&lt;/p>
&lt;p>KubeKey 生成 manifest 文件有两种方式。&lt;/p>
&lt;ul>
&lt;li>利用现有运行中的集群作为源生成 manifest 文件，也是官方推荐的一种方式，具体参考 KubeSphere &lt;a href="https://kubesphere.com.cn/docs/v3.3/installing-on-linux/introduction/air-gapped-installation/" title="官网的离线部署文档" target="_blank" rel="noopener noreferrer">官网的离线部署文档&lt;/a>。&lt;/li>
&lt;li>根据 &lt;a href="https://github.com/kubesphere/kubekey/blob/master/docs/manifest-example.md" title="模板文件" target="_blank" rel="noopener noreferrer">模板文件&lt;/a> 手动编写 manifest 文件。&lt;/li>
&lt;/ul>
&lt;p>第一种方式的好处是可以构建 1:1 的运行环境，但是需要提前部署一个集群，不够灵活度，并不是所有人都具备这种条件的。&lt;/p>
&lt;p>因此，本文参考官方的离线文档，采用手写 manifest 文件的方式，实现离线环境的安装部署。&lt;/p>
&lt;h3 id="本文知识点">本文知识点&lt;/h3>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>了解清单 (manifest) 和制品 (artifact) 的概念&lt;/li>
&lt;li>掌握 manifest 清单的编写方法&lt;/li>
&lt;li>根据 manifest 清单制作 artifact&lt;/li>
&lt;li>离线部署 KubeSphere 和 Kubernetes&lt;/li>
&lt;/ul>
&lt;h3 id="演示服务器配置">演示服务器配置&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">zdeops-master&lt;/td>
 &lt;td style="text-align: center">192.168.9.9&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Ansible 运维控制节点&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">es-node-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">ElasticSearch&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">es-node-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">ElasticSearch&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">es-node-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">ElasticSearch&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">harbor&lt;/td>
 &lt;td style="text-align: center">192.168.9.89&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Harbor&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">22&lt;/td>
 &lt;td style="text-align: center">84&lt;/td>
 &lt;td style="text-align: center">320&lt;/td>
 &lt;td style="text-align: center">2200&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h3 id="演示环境涉及软件版本信息">演示环境涉及软件版本信息&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>操作系统：&lt;strong>CentOS-7.9-x86_64&lt;/strong>&lt;/p></description></item><item><title>KubeSphere DevOps 基于 Jenkins + Argo 实现单集群的持续交付实践</title><link>https://openksc.github.io/zh/blogs/jenkins+argo-for-single-cluster-cd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/jenkins+argo-for-single-cluster-cd/</guid><description>&lt;blockquote>
&lt;p>作者：周靖峰，青云科技容器顾问，云原生爱好者，目前专注于 DevOps，云原生领域技术涉及 Kubernetes、KubeSphere、Argo。&lt;/p>&lt;/blockquote>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>KubeSphere v3.3.0 引入了 Argo CD，可以直接通过内置的方式实现 GitOps，不需要额外安装，并且在 UI 上也适配了基本功能。&lt;/p>
&lt;p>今天就来介绍下如何通过 KubeSphere 3.4.0 内置的 Argo CD 实现持续交付。&lt;/p>
&lt;h2 id="文章涉及内容">文章涉及内容&lt;/h2>
&lt;ul>
&lt;li>通过 KubeSphere DevOps Jenkins 实现 CI 能力&lt;/li>
&lt;li>通过 KubeSphere DevOps Argo CD 实现 CD 能力&lt;/li>
&lt;li>Jenkins + Argo CD 的持续交付&lt;/li>
&lt;/ul>
&lt;h2 id="jenkins-配置">Jenkins 配置&lt;/h2>
&lt;p>准备一个 git 仓库信息，这里提供的示例 demo 为：https://github.com/Feeeenng/devops-maven-sample。&lt;/p>
&lt;p>项目目录信息：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/image-20240319-1.png" alt="">&lt;/p>
&lt;p>项目目录信息包含 Jenkinsfile-argo，Dockerfile 等构建信息。&lt;/p>
&lt;p>这里主要介绍一下 Jenkinsfile-argo 文件的 pipeline 重要步骤。&lt;/p>
&lt;h3 id="agent-部分">agent 部分&lt;/h3>
&lt;p>由于内置的 agent 镜像版本没有加入 &lt;code>kustomize&lt;/code> 部署工具，并且我们在整个 CI/CD 的构建流水线当中，使用了此工具，所以引入了一个自己的定制版本。&lt;/p>
&lt;pre tabindex="0">&lt;code>agent {
 kubernetes {
 inheritFrom &amp;#39;maven&amp;#39;
 containerTemplate {
 name &amp;#39;maven&amp;#39;
 image &amp;#39;feeeng/builder-maven:v3.4.0&amp;#39;
 }
 }

 }
&lt;/code>&lt;/pre>&lt;h3 id="ci-更新部分">CI 更新部分&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span> stage&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#39;update mainfast&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> when&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> branch &lt;span style="color:#e6db74">&amp;#39;master&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> steps &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> // input&lt;span style="color:#f92672">(&lt;/span>id: &lt;span style="color:#e6db74">&amp;#39;deploy-to-dev&amp;#39;&lt;/span>, message: &lt;span style="color:#e6db74">&amp;#39;deploy to dev?&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> script &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> container &lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#39;maven&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> withCredentials&lt;span style="color:#f92672">([&lt;/span>usernamePassword&lt;span style="color:#f92672">(&lt;/span>credentialsId: &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$GITHUB_CREDENTIAL_ID&lt;span style="color:#e6db74">&amp;#34;&lt;/span>, passwordVariable: &lt;span style="color:#e6db74">&amp;#39;GIT_PASSWORD&amp;#39;&lt;/span>, usernameVariable: &lt;span style="color:#e6db74">&amp;#39;GIT_USERNAME&amp;#39;&lt;/span>&lt;span style="color:#f92672">)])&lt;/span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> def encodedPassword &lt;span style="color:#f92672">=&lt;/span> URLEncoder.encode&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$GIT_PASSWORD&lt;span style="color:#e6db74">&amp;#34;&lt;/span>,&lt;span style="color:#e6db74">&amp;#39;UTF-8&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> def encodedUsername &lt;span style="color:#f92672">=&lt;/span> URLEncoder.encode&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$GIT_USERNAME&lt;span style="color:#e6db74">&amp;#34;&lt;/span>,&lt;span style="color:#e6db74">&amp;#39;UTF-8&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sh &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> git config --global user.email &amp;#34;&lt;/span>kubesphere-cd@yunify.com&lt;span style="color:#e6db74">&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> git config --global user.name &amp;#34;&lt;/span>kubesphere-cd&lt;span style="color:#e6db74">&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> git config --local credential.helper &amp;#34;&lt;/span>!f&lt;span style="color:#f92672">()&lt;/span> &lt;span style="color:#f92672">{&lt;/span> echo username&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">\\&lt;/span>$GIT_USERNAME; echo password&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">\\&lt;/span>$GIT_PASSWORD; &lt;span style="color:#f92672">}&lt;/span>; f&lt;span style="color:#e6db74">&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> cd deploy/dev/
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> kustomize edit set image &lt;/span>$REGISTRY&lt;span style="color:#e6db74">/&lt;/span>$DOCKERHUB_NAMESPACE&lt;span style="color:#e6db74">/&lt;/span>$APP_NAME&lt;span style="color:#e6db74">:latest=&lt;/span>$REGISTRY&lt;span style="color:#e6db74">/&lt;/span>$DOCKERHUB_NAMESPACE&lt;span style="color:#e6db74">/&lt;/span>$APP_NAME&lt;span style="color:#e6db74">:SNAPSHOT-&lt;/span>$BRANCH_NAME&lt;span style="color:#e6db74">-&lt;/span>$BUILD_NUMBER&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> git add .
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> git commit -m &amp;#34;&lt;/span>images update &lt;span style="color:#66d9ef">for&lt;/span> dev $BUILD_NUMBER&lt;span style="color:#e6db74">&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> git push origin HEAD:master
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这里引入了 git 的配置信息，通过 step3 当中更新了镜像 tag 以后，通过 &lt;code>kustomize&lt;/code> 工具进行镜像文件 tag 的替换，并且自动更新 git 信息。从而实现后面让 Argo CD 监听 git 变化做准备。&lt;/p></description></item><item><title>KubeSphere DevOps 流水线入门指南</title><link>https://openksc.github.io/zh/blogs/kubesphere-devops-pipeline-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-devops-pipeline-guide/</guid><description>&lt;blockquote>
&lt;p>作者：赵海亮，浙江大学计算机专业四年级在读博士生，研究方向为云计算、边缘计算、分布式系统等。&lt;/p>&lt;/blockquote>
&lt;p>虽然 KubeSphere 能够将我们从 yaml 文件的编写中解放出来，但是项目上云仍然十分繁琐。 此外，一旦项目源代码发生更替（如发布新功能或去除 bug 等），所有组件都需要重新经历 “源码打包 --&amp;gt; 制作镜像 --&amp;gt; 启动容器” 这个流程。 这意味着，项目运维人员不得不从事大量重复性劳动。为了提高项目发布的效率，工业界引入了 DevOps 的概念。&lt;/p>
&lt;p>本文首先将介绍 DevOps 是什么，随后尝试利用 KubeSphere 集成的功能来实现 DevOps。&lt;/p>
&lt;h2 id="什么是-devops">什么是 DevOps&lt;/h2>
&lt;p>目前绝大多数互联网公司将开发和系统管理划分成不同的部门。 开发部门的驱动力通常是 “频繁交付新特性”，而运维部门则更关注 IT 服务的可靠性和 IT 成本投入的效率。 两者目标的不匹配，因而存在鸿沟，从而减慢了 IT 交付业务价值的速度。 为了解决这个问题，DevOps（Development 和 Operations 的组合词）被提出。 DevOps 的目的是在企业内部搭建一个自动化 “软件交付” 和“架构变更”的流程，来使得构建、测试、发布软件能够更加地快捷、频繁和可靠。&lt;/p>
&lt;p>实现 DevOps 通常需要多个软件和工具的密切配合。 如图 1 所示，DevOps 将软件的交付流程依次划分为 Plan、Code、Build、Test、Release、Deploy、Operate 以及 Monitor 这些阶段。 当需求变更时，将会从 Monitor 重新平滑过渡至 Plan 阶段。每个阶段都有一系列的软件和工具可供选择。 对于任意项目，我们只需要基于这些软件和工具 &lt;strong>搭建一条自动化流水线&lt;/strong> ，再设置类似于 “一旦代码变更就自动执行” 这样的钩子函数，整个项目即可自动实现“持续集成 / 持续交付（CI/CD）”，这将大大减少重复劳动。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/ecb1db00-df40-47b8-a071-82e2af035892.png" alt="">&lt;/p>
&lt;p>KubeSphere DevOps 基于 Kubernetes Jenkins Agent 实现。 和传统的 Jenkins Controller-Agent 架构不同的是，在 KubeSphere 中，Jenkins Agent 可以动态扩缩容，从而降低 CI/CD 对集群资源的盲目占用。 KubeSphere 的 DevOps 用户指南参见 &lt;a href="https://kubesphere.io/zh/docs/devops-user-guide/" target="_blank" rel="noopener noreferrer">https://kubesphere.io/zh/docs/devops-user-guide/&lt;/a>。 本文将依照该指南将一个开源项目上云。&lt;/p></description></item><item><title>KubeSphere Namespace 数据删除事故分析与解决全记录</title><link>https://openksc.github.io/zh/blogs/kubesphere-namespace-problem-solving-records/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-namespace-problem-solving-records/</guid><description>&lt;blockquote>
&lt;p>作者：宇轩辞白，运维研发工程师，目前专注于云原生、Kubernetes、容器、Linux、运维自动化等领域。&lt;/p>&lt;/blockquote>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>2023 年 7 月 23 日在项目上线前夕，K8s 生产环境出现故障，经过紧急修复之后，K8s 环境恢复正常；另外我们环境引入了 KubeSphere 云原生平台技术，为了方便研发人员对于 K8s 权限的细粒度管理，我方手动将 K8s Namespace（生产环境业务命名空间）加入到 KubeSphere 中的 Workspace（企业空间），就在此时，发生了让人后背一凉、极度可怕的事故，就是生产命名空间（Namespace）被自动删除了，熟悉 K8s 的人都知道，这意味着该命名空间下的所有数据，都被清空了。&lt;/p>
&lt;h2 id="问题简述">问题简述&lt;/h2>
&lt;h3 id="事故的来龙去脉">事故的来龙去脉&lt;/h3>
&lt;p>我们项目环境有两套 K8s 集群（即生产/测试），两套 K8s 环境准备完毕之后，分别在两套 K8s 引入 KubeSphere 云原生平台，计划通过 KubeSphere 启用多集群模式去管理两套 K8s：生产 K8s 集群将设置为 Host 主集群，测试环境 K8s 设置为 Member 集群。在此期间一切准备就绪，就等次日正式对外上线。&lt;/p>
&lt;p>在 2023 年 7 月 22 号晚上七点十分，忽然收到研发人员反馈：测试环境 KubeSphere 平台无法正常使用，数据库都无法打开。&lt;/p>
&lt;p>随后我展开排查，发现整个 KubeSphere 平台都瘫痪了。经过确认，是因第三方客户技术人员做资源克隆，间接性影响了生产环境。&lt;/p>
&lt;p>排查未果，情急之下我直接卸载了 KubeSphere 进行重装，重装之后暂时恢复了正常。随后我将两套 K8s 集群重新加入到 KubeSphere 平台托管，再将 K8s 的 Namespace 加入到 KubeSphere 所创建好的 WorkSpace 进行管理。&lt;/p></description></item><item><title>KubeSphere v3.3.1 权限控制详解</title><link>https://openksc.github.io/zh/blogs/kubesphere-3.3.1-access-control-and-account-management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-3.3.1-access-control-and-account-management/</guid><description>&lt;blockquote>
&lt;p>作者：周文浩，青云科技研发工程师，KubeSphere Maintainer。热爱云原生，热爱开源，目前负责KubeSphere 权限控制的开发与维护。&lt;/p>&lt;/blockquote>
&lt;p>KubeSphere 3.3.1 已经发布一个多月了。 3.3.1 版本对于 KubeSphere 来说只是一个小的 Patch 版本，但是权限控制模块改动较大。这篇文章将从开发者的视角为你分享权限控制模块的改动内容。&lt;/p>
&lt;p>这次的改动的主要目的是提升 KubeSphere 权限控制的安全性和易用性。使用过 KubeSphere 的小伙伴应该对 KubeSphere 的租户体系有一些印象，这对于用户来说是非常重要的一部分。&lt;/p>
&lt;h2 id="权限控制及租户解析">权限控制及租户解析&lt;/h2>
&lt;p>在介绍这次的改动前，我将先简单为你介绍 KubeSphere 的权限控制和租户体系，这 对于理解本次 3.3.1 版本对于权限控制的改动有非常大的帮助。或者你可以参考&lt;a href="https://kubesphere.io/zh/docs/v3.3/quick-start/create-workspace-and-project/" target="_blank" rel="noopener noreferrer">创建企业空间、项目、用户和平台角色&lt;/a>自己动手实验一下。&lt;/p>
&lt;p>我们借鉴了 Kubernetes 的 RBAC 权限控制机制，使用&lt;strong>角色&lt;/strong>给租户授予对 KubeSphere 的操作权限，而角色是由&lt;strong>授权项&lt;/strong>组成的一个权限实体。角色分为内置角色和自定义角色，在通常的使用场景下，我们希望内置角色就能够覆盖用户的使用需求。如果你有特殊的权限要求则可以自定义一个角色。自定义角色使得你可以用 KubeSphere 提供的授权项随意组合，创建一个特定的角色。&lt;/p>
&lt;p>角色由分为不同的层级，从租户体系来看，我们将整个 KS 分为四个层级，即：&lt;/p>
&lt;ul>
&lt;li>平台 (Platform)&lt;/li>
&lt;li>企业空间（Workspace）&lt;/li>
&lt;li>命名空间 (Namespace) - 集群 (Cluster)&lt;/li>
&lt;/ul>
&lt;p>与之对应的角色则是：&lt;/p>
&lt;ul>
&lt;li>平台角色 (Platform Role)&lt;/li>
&lt;li>企业空间角色 (Workspace Role)&lt;/li>
&lt;li>命名空间角色 (Namespace Role)&lt;/li>
&lt;li>集群角色 (Cluster Role)。&lt;/li>
&lt;/ul>
&lt;p>下面这张图可以帮助你理解不同层级和角色之间的关系。&lt;/p>
&lt;p>从资源层级来看，命名空间属于企业空间，企业空间属于平台。在这里我们只将集群作为一个部署命名空间的资源池。&lt;/p>
&lt;p>租户可以被邀请进多个企业空间，并授予企业空间角色。在企业空间中，又可以被邀请到多个命名空间，并授权命名空间角色。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202212091127682.png" alt="">&lt;/p>
&lt;h2 id="角色">角色&lt;/h2>
&lt;h3 id="删除角色">删除角色&lt;/h3>
&lt;ul>
&lt;li>平台级: Users Manager (用户管理员)，Workspaces Manager (企业空间管理 员)&lt;/li>
&lt;/ul>
&lt;p>基于安全性考虑，我们删除了两个内置平台角色：Users Manager (用户管理员) 和 Workspaces Manager (企业空间管理员)。&lt;/p></description></item><item><title>KubeSphere v4 安装指南</title><link>https://openksc.github.io/zh/blogs/kubesphere-v4-install-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-v4-install-guide/</guid><description>&lt;p>日前，KubeSphere v4 发布，相较于之前的版本，新版本在架构上有了颠覆性的变化。为了让社区的各位小伙伴能够丝滑的从旧版本过渡到新版本，我们特别推出本篇安装指南文章，以供参考。&lt;/p>
&lt;p>关于 KubeSphere v4 的介绍，请阅读本文：&lt;a href="https://kubesphere.io/zh/news/kubesphere-v4-ga-announcement/" target="_blank" rel="noopener noreferrer">KubeSphere v4 开源并发布全新可插拔架构 LuBan&lt;/a>。&lt;/p>
&lt;blockquote>
&lt;p>需要注意的是，目前不支持从 KubeSphere 3.4.1 版本直接升级到 v4 版本，需要先卸载原来的版本，再安装 v4 版本。&lt;/p>&lt;/blockquote>
&lt;h2 id="卸载-kubesphere-341">卸载 KubeSphere 3.4.1&lt;/h2>
&lt;blockquote>
&lt;p>注意：&lt;/p>
&lt;ul>
&lt;li>本文仅适用于测试环境，请不要直接在生产环境操作。&lt;/li>
&lt;li>如果需要在生产环境操作，请先在测试环境验证通过后再进行。&lt;/li>
&lt;li>卸载为高风险操作，执行该操作前，请明确您知道自己将要做什么。&lt;/li>
&lt;li>该操作会导致 KubeSphere 平台自身无法使用，但不会影响 KubeSphere 之外即 K8s 集群中运行的工作负载。&lt;/li>
&lt;li>该操作会删除 KubeSphere 所有的组件及相关数据，您可以在此之前对数据进行备份。&lt;/li>
&lt;li>您可以自主选择数据迁移工具，或等待社区的数据迁移方案，社区的迁移方案计划通过脚本帮助您备份平台账户、权限及相关的数据，在新版本安装好后，可将备份数据进行导入。&lt;/li>
&lt;li>如果您期望全面的数据迁移和升级，我们建议您可以考虑 &lt;a href="https://m.qingcloud.com/page/23555798970015596/4c97b2026cb84249be20d94e71b647cf?cl_track=aec50" target="_blank" rel="noopener noreferrer">KubeSphere 企业版&lt;/a>。&lt;/li>
&lt;li>&lt;strong>社区郑重提醒您，务必谨慎操作。&lt;/strong>&lt;/li>
&lt;/ul>&lt;/blockquote>
&lt;h3 id="解绑集群">解绑集群&lt;/h3>
&lt;p>如果开启了多集群，请务必在卸载前将集群进行解绑。卸载 Host 集群前，请确保已经没有 Member 集群被当前集群纳管，且角色和账户等信息也会被删除。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/ks-v4-install-guide-1.png" alt="">&lt;/p>
&lt;h3 id="卸载-kubesphere-341-1">卸载 KubeSphere 3.4.1&lt;/h3>
&lt;p>针对待卸载集群执行该脚本。&lt;/p>
&lt;p>&lt;strong>注意：&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>执行该脚本前请确保当前集群已从 Host 集群中解绑。&lt;/li>
&lt;li>请确认即将操作的集群是将要被卸载的集群。&lt;/li>
&lt;li>执行该脚本后会将集群中的 Prometheus 以及 Elasticsearch 删除，存量监控和日志数据不会被保留。&lt;/li>
&lt;li>执行该脚本后，集群中项目网关仍可继续使用，但纳管至 KubeSphere 4.1.2 后需将网关切换到新版本网关，切换过程存需删除老版本网关，切换为新版本网关。&lt;/li>
&lt;/ol>
&lt;h4 id="卸载-devops-组件">卸载 DevOps 组件&lt;/h4>
&lt;pre tabindex="0">&lt;code>helm del -n argocd devops
helm del -n kubesphere-devops-system devops
&lt;/code>&lt;/pre>&lt;h4 id="卸载-servicemesh-组件">卸载 ServiceMesh 组件&lt;/h4>
&lt;pre tabindex="0">&lt;code>kubectl -n istio-system delete jaeger jaeger
kubectl -n istio-system delete kiali kiali

helm del -n istio-system kiali-operator
helm del -n istio-system jaeger-operator

# 下载 istioctl
wget https://github.com/istio/istio/releases/download/1.15.6/istioctl-1.15.6-linux-amd64.tar.gz
tar -zxvf istioctl-1.15.6-linux-amd64.tar.gz
./istioctl uninstall --purge
&lt;/code>&lt;/pre>&lt;h4 id="卸载-ks-core监控及日志相关组件">卸载 ks-core、监控及日志相关组件&lt;/h4>
&lt;pre tabindex="0">&lt;code>#！/bin/bash
############################################################################################################
# 该脚本用于卸载集群中的 KubeSphere v3.4.1
#
# 注意： 如果为多集群环境，执行该脚本前请确保当前集群已从 host 集群中解绑
############################################################################################################

set -x

# 清除集群所有 namespace 中的 workspace 标签
kubectl get ns -l kubesphere.io/workspace -o name | xargs -I {} bash -c &amp;#34;kubectl label {} kubesphere.io/workspace- &amp;amp;&amp;amp; kubectl patch {} -p &amp;#39;{\&amp;#34;metadata\&amp;#34;:{\&amp;#34;ownerReferences\&amp;#34;:[]}}&amp;#39; --type=merge&amp;#34;

# # 清除集群所有 namespace 中的 kubefed 标签
kubectl get ns -l kubefed.io/managed -o name | xargs -I {} bash -c &amp;#34;kubectl label {} kubefed.io/managed- &amp;amp;&amp;amp; kubectl patch {} -p &amp;#39;{\&amp;#34;metadata\&amp;#34;:{\&amp;#34;ownerReferences\&amp;#34;:[]}}&amp;#39; --type=merge&amp;#34;

# 清除集群中的 workspace 以及 workspacetemplate 资源
kubectl get workspacetemplate -A -o name | xargs -I {} kubectl patch {} -p &amp;#39;{&amp;#34;metadata&amp;#34;:{&amp;#34;ownerReferences&amp;#34;:[]}}&amp;#39; --type=merge
kubectl get workspace -A -o name | xargs -I {} kubectl patch {} -p &amp;#39;{&amp;#34;metadata&amp;#34;:{&amp;#34;ownerReferences&amp;#34;:[]}}&amp;#39; --type=merge

kubectl get workspacetemplate -A -o name | xargs -I {} kubectl delete {}
kubectl get workspace -A -o name | xargs -I {} kubectl delete {}

# 删除 clusterroles
delete_cluster_roles() {
 for role in `kubectl get clusterrole -l iam.kubesphere.io/role-template -o jsonpath=&amp;#34;{.items[*].metadata.name}&amp;#34;`
 do
 kubectl delete clusterrole $role 2&amp;gt;/dev/null
 done
}

delete_cluster_roles

# 删除 clusterrolebindings
delete_cluster_role_bindings() {
 for rolebinding in `kubectl get clusterrolebindings -l iam.kubesphere.io/role-template -o jsonpath=&amp;#34;{.items[*].metadata.name}&amp;#34;`
 do
 kubectl delete clusterrolebindings $rolebinding 2&amp;gt;/dev/null
 done
}
delete_cluster_role_bindings

# 删除 validatingwebhookconfigurations
for webhook in ks-events-admission-validate users.iam.kubesphere.io network.kubesphere.io validating-webhook-configuration resourcesquotas.quota.kubesphere.io
do
 kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io $webhook 2&amp;gt;/dev/null
done

# 删除 mutatingwebhookconfigurations
for webhook in ks-events-admission-mutate logsidecar-injector-admission-mutate mutating-webhook-configuration
do
 kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io $webhook 2&amp;gt;/dev/null
done

# 删除 users
for user in `kubectl get users -o jsonpath=&amp;#34;{.items[*].metadata.name}&amp;#34;`
do
 kubectl patch user $user -p &amp;#39;{&amp;#34;metadata&amp;#34;:{&amp;#34;finalizers&amp;#34;:null}}&amp;#39; --type=merge
done
kubectl delete users --all 2&amp;gt;/dev/null

# 删除 iam 资源
for resource_type in `echo globalrolebinding loginrecord rolebase workspacerole globalrole workspacerolebinding`; do
 for resource_name in `kubectl get ${resource_type}.iam.kubesphere.io -o jsonpath=&amp;#34;{.items[*].metadata.name}&amp;#34;`; do
 kubectl patch ${resource_type}.iam.kubesphere.io ${resource_name} -p &amp;#39;{&amp;#34;metadata&amp;#34;:{&amp;#34;finalizers&amp;#34;:null}}&amp;#39; --type=merge
 done
 kubectl delete ${resource_type}.iam.kubesphere.io --all 2&amp;gt;/dev/null
done

# 卸载 ks-core
helm del -n kubesphere-system ks-core
helm del -n kubesphere-system ks-redis &amp;amp;&amp;gt; /dev/null || true
kubectl delete pvc -n kubesphere-system -l app=redis-ha --ignore-not-found || true
kubectl delete deploy -n kubesphere-system -l app.kubernetes.io/managed-by!=Helm --field-selector metadata.name=redis --ignore-not-found || true
kubectl delete svc -n kubesphere-system -l app.kubernetes.io/managed-by!=Helm --field-selector metadata.name=redis --ignore-not-found || true
kubectl delete secret -n kubesphere-system -l app.kubernetes.io/managed-by!=Helm --field-selector metadata.name=redis-secret --ignore-not-found || true
kubectl delete cm -n kubesphere-system -l app.kubernetes.io/managed-by!=Helm --field-selector metadata.name=redis-configmap --ignore-not-found || true
kubectl delete pvc -n kubesphere-system -l app.kubernetes.io/managed-by!=Helm --field-selector metadata.name=redis-pvc --ignore-not-found || true
kubectl delete deploy -n kubesphere-system --all --ignore-not-found
kubectl delete svc -n kubesphere-system --all --ignore-not-found
kubectl delete cm -n kubesphere-system --all --ignore-not-found
kubectl delete secret -n kubesphere-system --all --ignore-not-found
kubectl delete sa -n kubesphere-system --all --ignore-not-found

# 删除 Gateway 资源
for gateway in `kubectl -n kubesphere-controls-system get gateways.gateway.kubesphere.io -o jsonpath=&amp;#34;{.items[*].metadata.name}&amp;#34;`
do
 kubectl -n kubesphere-controls-system patch gateways.gateway.kubesphere.io $gateway -p &amp;#39;{&amp;#34;metadata&amp;#34;:{&amp;#34;finalizers&amp;#34;:null}}&amp;#39; --type=merge
done
kubectl -n kubesphere-controls-system delete gateways.gateway.kubesphere.io --all 2&amp;gt;/dev/null

# 删除crd
kubectl delete crd globalrolebindings.iam.kubesphere.io
kubectl delete crd globalroles.iam.kubesphere.io
kubectl delete crd users.iam.kubesphere.io
kubectl delete crd workspacerolebindings.iam.kubesphere.io
kubectl delete crd workspaceroles.iam.kubesphere.io 
kubectl delete crd workspaces.tenant.kubesphere.io
kubectl delete crd workspacetemplates.tenant.kubesphere.io
kubectl delete crd gateways.gateway.kubesphere.io

## 卸载 监控组件
# 删除 Prometheus/ALertmanager/ThanosRuler
kubectl -n kubesphere-monitoring-system delete Prometheus k8s --ignore-not-found
kubectl -n kubesphere-monitoring-system delete secret additional-scrape-configs --ignore-not-found
kubectl -n kubesphere-monitoring-system delete serviceaccount prometheus-k8s --ignore-not-found
kubectl -n kubesphere-monitoring-system delete service prometheus-k8s --ignore-not-found
kubectl -n kubesphere-monitoring-system delete role prometheus-k8s-config --ignore-not-found
kubectl -n kubesphere-monitoring-system delete rolebinging prometheus-k8s-config --ignore-not-found

kubectl -n kubesphere-monitoring-system delete Alertmanager main --ignore-not-found
kubectl -n kubesphere-monitoring-system delete secret alertmanager-main --ignore-not-found
kubectl -n kubesphere-monitoring-system delete service alertmanager-main --ignore-not-found

kubectl -n kubesphere-monitoring-system delete ThanosRuler kubesphere --ignore-not-found

# 删除 ServiceMonitor/PrometheusRules
kubectl -n kubesphere-monitoring-system delete ServiceMonitor alertmanager coredns etcd ks-apiserver kube-apiserver kube-controller-manager kube-proxy kube-scheduler kube-state-metrics kubelet node-exporter prometheus prometheus-operator s2i-operator thanosruler --ignore-not-found
kubectl -n kubesphere-monitoring-system delete PrometheusRule kubesphere-rules prometheus-k8s-coredns-rules prometheus-k8s-etcd-rules prometheus-k8s-rules --ignore-not-found

# 删除 prometheus-operator
kubectl -n kubesphere-monitoring-system delete deployment prometheus-operator --ignore-not-found
kubectl -n kubesphere-monitoring-system delete service prometheus-operator --ignore-not-found
kubectl -n kubesphere-monitoring-system delete serviceaccount prometheus-operator --ignore-not-found

# 删除 kube-state-metrics/node-exporter
kubectl -n kubesphere-monitoring-system delete deployment kube-state-metrics --ignore-not-found
kubectl -n kubesphere-monitoring-system delete service kube-state-metrics --ignore-not-found
kubectl -n kubesphere-monitoring-system delete serviceaccount kube-state-metrics --ignore-not-found

kubectl -n kubesphere-monitoring-system delete daemonset node-exporter --ignore-not-found
kubectl -n kubesphere-monitoring-system delete service node-exporter --ignore-not-found
kubectl -n kubesphere-monitoring-system delete serviceaccount node-exporter --ignore-not-found

# 删除 Clusterrole/ClusterRoleBinding
kubectl delete clusterrole kubesphere-prometheus-k8s kubesphere-kube-state-metrics kubesphere-node-exporter kubesphere-prometheus-operator
kubectl delete clusterrolebinding kubesphere-prometheus-k8s kubesphere-kube-state-metrics kubesphere-node-exporter kubesphere-prometheus-operator

# 删除 notification-manager
helm delete notification-manager -n kubesphere-monitoring-system

# 清理 kubesphere-monitoring-system
kubectl delete deploy -n kubesphere-monitoring-system --all --ignore-not-found

# 删除监控 crd
kubectl delete crd alertmanagerconfigs.monitoring.coreos.com
kubectl delete crd alertmanagers.monitoring.coreos.com
kubectl delete crd podmonitors.monitoring.coreos.com
kubectl delete crd probes.monitoring.coreos.com
kubectl delete crd prometheusagents.monitoring.coreos.com
kubectl delete crd prometheuses.monitoring.coreos.com
kubectl delete crd prometheusrules.monitoring.coreos.com
kubectl delete crd scrapeconfigs.monitoring.coreos.com
kubectl delete crd servicemonitors.monitoring.coreos.com
kubectl delete crd thanosrulers.monitoring.coreos.com
kubectl delete crd clusterdashboards.monitoring.kubesphere.io
kubectl delete crd dashboards.monitoring.kubesphere.io

# 删除 metrics-server
kubectl delete apiservice v1beta1.metrics.k8s.io
kubectl -n kube-system delete deploy metrics-server
kubectl -n kube-system delete service metrics-server
kubectl delete ClusterRoleBinding system:metrics-server
kubectl delete ClusterRoleBinding metrics-server:system:auth-delegator
kubectl -n kube-system delete RoleBinding metrics-server-auth-reader
kubectl delete ClusterRole system:metrics-server
kubectl delete ClusterRole system:aggregated-metrics-reader
kubectl -n kube-system delete ServiceAccount ServiceAccount

## 卸载 日志组件
# 删除 fluent-bit
kubectl -n kubesphere-logging-system delete fluentbitconfigs fluent-bit-config --ignore-not-found
kubectl -n kubesphere-logging-system patch fluentbit fluent-bit -p &amp;#39;{&amp;#34;metadata&amp;#34;:{&amp;#34;finalizers&amp;#34;:null}}&amp;#39; --type=merge
kubectl -n kubesphere-logging-system delete fluentbit fluent-bit --ignore-not-found

# 删除 ks-logging
helm del -n kubesphere-logging-system logsidecar-injector &amp;amp;&amp;gt; /dev/null || true

# 删除 ks-events
helm del -n kubesphere-logging-system ks-events &amp;amp;&amp;gt; /dev/null || true

# 删除 kube-auditing
helm del -n kubesphere-logging-system kube-auditing &amp;amp;&amp;gt; /dev/null || true

# 删除 es 
helm del -n kubesphere-logging-system elasticsearch-logging &amp;amp;&amp;gt; /dev/null || true
helm del -n kubesphere-logging-system elasticsearch-logging-curator &amp;amp;&amp;gt; /dev/null || true

# 删除 opensearch
helm del -n kubesphere-logging-system opensearch-master &amp;amp;&amp;gt; /dev/null || true
helm del -n kubesphere-logging-system opensearch-data &amp;amp;&amp;gt; /dev/null || true
helm del -n kubesphere-logging-system opensearch-logging-curator &amp;amp;&amp;gt; /dev/null || true

# 清理 kubesphere-logging-system
kubectl delete deploy -n kubesphere-logging-system --all --ignore-not-found
&lt;/code>&lt;/pre>&lt;h4 id="检查-namespace-标签">检查 Namespace 标签&lt;/h4>
&lt;p>确认所有 Namespace 不包含 &lt;code>kubesphere.io/workspace&lt;/code> 标签。&lt;/p></description></item><item><title>KubeSphere v4 扩展组件使用指南</title><link>https://openksc.github.io/zh/blogs/kubesphere-v4-extension-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-v4-extension-guide/</guid><description>&lt;p>日前，KubeSphere v4 发布，相较于之前的版本，新版本在架构上有了较大的变化。其中，有一个新的概念——扩展组件。&lt;/p>
&lt;p>本文我们将针对扩展组件做一个详细的说明，让大家对扩展组件能够了解、理解和丝滑使用。&lt;/p>
&lt;p>关于 KubeSphere v4 的介绍，请阅读本文：&lt;a href="https://www.kubesphere.io/zh/news/kubesphere-v4-ga-announcement/" target="_blank" rel="noopener noreferrer">KubeSphere v4 开源并发布全新可插拔架构 LuBan&lt;/a>。&lt;/p>
&lt;p>如何安装 KubeSphere v4，请参考本文：&lt;a href="https://www.kubesphere.io/zh/blogs/kubesphere-v4-install-guide/" target="_blank" rel="noopener noreferrer">KubeSphere v4 安装指南&lt;/a>。&lt;/p>
&lt;h2 id="扩展市场介绍">扩展市场介绍&lt;/h2>
&lt;p>KubeSphere 扩展组件用于扩展 KubeSphere 的平台能力，用户可在系统运行时动态地安装、卸载、启用、禁用扩展组件。&lt;/p>
&lt;p>监控、告警、通知、项目网关和集群网关、卷快照、网络隔离等功能，将由扩展组件来提供。&lt;/p>
&lt;p>&lt;strong>扩展组件的方式，解决了 KubeSphere 之前版本诸如“发版周期长”、“代码耦合”、“系统资源占用过多”等问题。用户可以根据自己的需求来安装和启用扩展组件，真正做到按需使用，实现轻量化。另外，用户还可以根据自己的需求进行定制和扩展，以满足不同的设计和功能要求。&lt;/strong>&lt;/p>
&lt;p>目前，我们已经开源了 21 个扩展组件，您可以根据对应 v3.4 中的功能决定是否安装，分别是：&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>扩展组件名称&lt;/th>
 &lt;th>对应 v3.4 功能点&lt;/th>
 &lt;th>新增功能点&lt;/th>
 &lt;th>挂载位置&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>KubeSphere 网络&lt;/td>
 &lt;td>IP池、网络隔离&lt;/td>
 &lt;td>无&lt;/td>
 &lt;td>项目、企业空间、集群管理页面左侧导航栏&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>KubeSphere 应用商店管理&lt;/td>
 &lt;td>应用上架审核，chart 包上传&lt;/td>
 &lt;td>全局应用实例管理&lt;/td>
 &lt;td>九宫格，企业空间导航&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>KubeSphere 存储&lt;/td>
 &lt;td>存储类授权规则，PVC 自动扩容，快照&lt;/td>
 &lt;td>无&lt;/td>
 &lt;td>集群和项目的左侧导航栏&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>KubeSphere 服务网格&lt;/td>
 &lt;td>灰度发布&lt;br>自制应用&lt;/td>
 &lt;td>无&lt;/td>
 &lt;td>项目管理页面左侧导航栏&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>KubeSphere 多集群代理连接&lt;/td>
 &lt;td>使用代理连接模式纳管集群&lt;/td>
 &lt;td>无&lt;/td>
 &lt;td>添加集群模式选择下拉框&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>KubeSphere 网关&lt;/td>
 &lt;td>项目、企业空间、集群网关&lt;/td>
 &lt;td>创建 Ingress 时支持配置 Ingress class&lt;/td>
 &lt;td>项目、企业空间、集群管理页面左侧导航栏&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>DevOps&lt;/td>
 &lt;td>流水线、持续部署、代码仓库、S2I/B2I&lt;/td>
 &lt;td>无&lt;/td>
 &lt;td>企业空间左侧导航栏&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>KubeEdge&lt;/td>
 &lt;td>边缘计算&lt;/td>
 &lt;td>无&lt;/td>
 &lt;td>集群左侧导航栏&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>联邦集群应用管理&lt;/td>
 &lt;td>联邦项目以及联邦应用&lt;/td>
 &lt;td>无&lt;/td>
 &lt;td>企业空间左侧导航栏&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>OpenSearch 分布式检索与分析引擎&lt;/td>
 &lt;td>OpenSearch&lt;/td>
 &lt;td>无&lt;/td>
 &lt;td>无挂载点，直接暴露服务进行访问&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>Grafana for WhizardTelemetry&lt;/td>
 &lt;td>-&lt;/td>
 &lt;td>新增扩展，增强 WhizardTelemetry 可观测平台的可视化能力&lt;/td>
 &lt;td>无挂载点，直接暴露服务进行访问&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>Grafana Loki for WhizardTelemetry&lt;/td>
 &lt;td>-&lt;/td>
 &lt;td>部署 Grafana Loki&lt;/td>
 &lt;td>无&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>WhizardTelemetry 数据流水线&lt;/td>
 &lt;td>提供日志、事件、审计等数据收集能力，用以替代 FluentBit&lt;/td>
 &lt;td>无&lt;/td>
 &lt;td>无&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>WhizardTelemetry 平台服务&lt;/td>
 &lt;td>提供监控、日志、审计、事件、通知查询接口&lt;/td>
 &lt;td>无&lt;/td>
 &lt;td>无&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>WhizardTelemetry 告警&lt;/td>
 &lt;td>KubeSphere 告警&lt;/td>
 &lt;td>无&lt;/td>
 &lt;td>集群和项目的左侧导航栏&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>WhizardTelemetry 事件&lt;/td>
 &lt;td>KubeSphere 事件&lt;/td>
 &lt;td>支持使用 Loki 作为后端存储&lt;/td>
 &lt;td>⼯具箱&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>WhizardTelemetry 日志&lt;/td>
 &lt;td>KubeSphere 日志&lt;/td>
 &lt;td>支持使用 Loki 作为后端存储&lt;/td>
 &lt;td>⼯具箱、集群设置&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>WhizardTelemetry 监控&lt;/td>
 &lt;td>KubeSphere 监控&lt;/td>
 &lt;td>增强集群概览与项目概览页面监控&lt;/td>
 &lt;td>集群和项目的左侧导航栏将显示监控告警，可查看集群状态等，集群、企业空间、项目下等诸多页面也将显示相关监控数据&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>WhizardTelemetry 通知&lt;/td>
 &lt;td>KubeSphere 通知&lt;/td>
 &lt;td>支持使用 Loki 作为通知历史的后端存储。通知只需要在 host 集群部署了&lt;/td>
 &lt;td>平台设置左侧导航栏&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>Metrics Server&lt;/td>
 &lt;td>可视化创建和管理 HPA&lt;/td>
 &lt;td>无&lt;/td>
 &lt;td>工作负载（Deployment/SatetfulSet） 详情页支持可视化创建和管理 HPA&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>Gatekeeper&lt;/td>
 &lt;td>安全准入策略管理&lt;/td>
 &lt;td>UI 支持&lt;br>版本更新&lt;/td>
 &lt;td>集群管理页面左侧导航栏&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>扩展组件仓库：https://github.com/kubesphere-extensions/ks-extensions/&lt;/p></description></item><item><title>KubeSphere v4 应用商店配置指南</title><link>https://openksc.github.io/zh/blogs/kubesphere-v4-appstore-configuration-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-v4-appstore-configuration-guide/</guid><description>&lt;p>在 KubeSphere v4 版本中，为保持平台的简洁性，系统默认移除了内置应用商店中的应用。用户可以按照下列步骤进行手动配置和添加。&lt;/p>
&lt;blockquote>
&lt;p>注意：应用商店和扩展市场有所不同，扩展市场的使用方法将在后续文档中详细介绍。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Helm Repo 源&lt;/strong>：安装时需要从源下载 Chart 包，需要保证源是可用状态，平台会定时从源同步最新的应用信息&lt;/li>
&lt;li>&lt;strong>应用商店&lt;/strong>：应用被存储在平台中，默认不会自动更新，可在审核后全局可见&lt;/li>
&lt;li>&lt;strong>商店导入工具&lt;/strong>：用于将 Helm Repo 源中的应用转为应用商店应用&lt;/li>
&lt;/ul>&lt;/blockquote>
&lt;h2 id="企业空间中的-helm-repo-源配置">企业空间中的 Helm Repo 源配置&lt;/h2>
&lt;h3 id="添加源">添加源&lt;/h3>
&lt;p>在企业空间中添加的源仅对当前空间可见。&lt;/p>
&lt;ul>
&lt;li>进入企业空间，选择左侧边栏中的应用仓库。&lt;/li>
&lt;li>添加一个 Helm Repo 源。&lt;/li>
&lt;/ul>
&lt;p>KubeSphere v3.x 默认提供的 Helm Repo 源为： &lt;a href="https://charts.kubesphere.io/stable" target="_blank" rel="noopener noreferrer">https://charts.kubesphere.io/stable&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/20241101-1.png" alt="">&lt;/p>
&lt;h3 id="使用源">使用源&lt;/h3>
&lt;ul>
&lt;li>进入项目，选择应用负载 &amp;gt; 应用。&lt;/li>
&lt;li>点击创建 &amp;gt; 从应用模板，选择对应的应用源。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/20241101-2.png" alt="">&lt;/p>
&lt;h2 id="仅在企业空间的应用管理">仅在企业空间的应用管理&lt;/h2>
&lt;h3 id="安装应用商店扩展">安装应用商店扩展&lt;/h3>
&lt;p>在 KubeSphere v4 中需要安装应用商店扩展，才能在企业空间上传应用 Chart 包，上传的 Chart 应用默认只在当前企业空间可见，如需全局可见，请参考后续的全局应用配置。&lt;/p>
&lt;ul>
&lt;li>进入企业空间，选择左侧边栏中的应用模板。&lt;/li>
&lt;li>选择上传 Helm Chart以添加应用。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/20241101-3.png" alt="">&lt;/p>
&lt;h3 id="使用上传应用">使用上传应用&lt;/h3>
&lt;ul>
&lt;li>进入项目，选择应用负载 &amp;gt; 应用。&lt;/li>
&lt;li>点击创建 &amp;gt; 从应用模板，选择当前企业空间的应用源。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/20241101-4.png" alt="">&lt;/p>
&lt;h2 id="全局应用商店的应用">全局应用商店的应用&lt;/h2>
&lt;h3 id="提交审核">提交审核&lt;/h3>
&lt;p>在企业空间上传 Chart 后，可以将应用提交审核，通过审核后，该应用将在全局应用商店中可见，所有用户均可访问。&lt;/p></description></item><item><title>KubeSphere 边缘节点 IP 冲突的分析和解决思路分享</title><link>https://openksc.github.io/zh/blogs/kubesphere-edgenode-ip/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-edgenode-ip/</guid><description>&lt;p>在上一篇&lt;a href="https://kubesphere.com.cn/blogs/edge-node-monitoring/" target="_blank" rel="noopener noreferrer">监控问题排查的文章&lt;/a>中，笔者分析了 KubeSphere 3.1.0 集成 KubeEdge 中的边缘监控原理和问题排查思路，在介绍 EdgeWatcher 组件时提到了“边缘节点的内网 IP 需要集群内唯一”这样的限制条件。本文就来深入分析一下这个问题，并尝试给各位边缘开发者提供一些解决的建议和思路。&lt;/p>
&lt;h2 id="正常场景">正常场景&lt;/h2>
&lt;p>在边缘节点加入云端集群时，需要指定 “Node Name” 和 “Internal IP”，顾名思义，就是边缘节点的节点名称和内网 IP 地址。这里的内网 IP 地址就是本文的主题，该地址需要在集群内唯一。&lt;/p>
&lt;p>KubeSphere 在 EdgeWatcher 中提供了用户指定的内网 IP是否被占用的验证功能。验证失败(IP 已被占用)的情况下，则不会为该边缘节点提供加入集群的命令行输出。下面两张图展示了验证成功和失败的场景。&lt;/p>
&lt;p>&lt;strong>验证成功：&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/1623996910-446834-image.png" alt="">&lt;/p>
&lt;p>&lt;strong>验证失败：&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/1623996958-232941-image.png" alt="">&lt;/p>
&lt;p>可以说，KubeSphere 在这一点上已经做的非常用心了，给用户提供了 UI 的 “Validate” 按钮和后台 API，不管是直接使用还是基于 KubeSphere 的二次开发都会非常便捷。&lt;/p>
&lt;h2 id="非法场景">非法场景&lt;/h2>
&lt;p>在上一节中展示了内网 IP 被占用的结果就是不能加入集群，因为该 IP 已经被注册在了 EdgeWatcher 中，不能再被其他边缘节点使用。&lt;/p>
&lt;p>那么如果一个 IP 还没有被注册到 EdgeWatcher 中，也就是边缘节点没有被真正接入集群时，还是可以跳过这一步验证，将相同内网 IP 的两个边缘节点加入同一个集群中，制造这个非法的使用场景。&lt;/p>
&lt;p>这个非法场景带来的问题就是：相同 IP 的“较早加入集群”的边缘节点在 logs exec 和 metrics 的功能上都会失效。即下图的运维功能都是没有数据的。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/1623997023-119243-image.png" alt="">&lt;/p>
&lt;p>之前，笔者也在 KubeSphere 的开发者社区提过这个&lt;a href="https://ask.kubesphere.io/forum/d/4388-kubesphere-31" target="_blank" rel="noopener noreferrer">问题&lt;/a>，同时也和负责边缘模块的社区开发者有过交流，确认了在 KubeSphere 的产品设计上，内网 IP 需要管理员或者用户自行按需进行规划，保证不重复。&lt;/p></description></item><item><title>KubeSphere 部署 Kafka 集群实战指南</title><link>https://openksc.github.io/zh/blogs/deploy-kafka-on-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deploy-kafka-on-kubesphere/</guid><description>&lt;p>本文档将详细阐述如何利用 Helm 这一强大的工具，快速而高效地在 K8s 集群上安装并配置一个 Kafka 集群。&lt;/p>
&lt;p>&lt;strong>实战服务器配置(架构 1:1 复刻小规模生产环境，配置略有不同)&lt;/strong>&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ksp-registry&lt;/td>
 &lt;td style="text-align: center">192.168.9.90&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Harbor 镜像仓库&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.94&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker/CI&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">400+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/Longhorn/Ceph/NFS&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.98&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">300+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/Longhorn/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.99&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">300+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/Longhorn/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gpu-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.101&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker(GPU NVIDIA Tesla M40 24G)&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gpu-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.102&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker(GPU NVIDIA Tesla P100 16G)&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gateway-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.103&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;td style="text-align: center">自建应用服务代理网关/VIP：192.168.9.100&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gateway-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.104&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;td style="text-align: center">自建应用服务代理网关/VIP：192.168.9.100&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-mid&lt;/td>
 &lt;td style="text-align: center">192.168.9.105&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">部署在 k8s 集群之外的服务节点（Gitlab 等）&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">15&lt;/td>
 &lt;td style="text-align: center">68&lt;/td>
 &lt;td style="text-align: center">152&lt;/td>
 &lt;td style="text-align: center">600&lt;/td>
 &lt;td style="text-align: center">2100+&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>实战环境涉及软件版本信息&lt;/strong>&lt;/p></description></item><item><title>KubeSphere 部署 Litmus 至 Kubernetes 开启混沌实验</title><link>https://openksc.github.io/zh/blogs/litmus-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/litmus-kubesphere/</guid><description>&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/20210609224203.png" alt="">&lt;/p>
&lt;p>对于云服务而言，如果系统出现异常，将会带来很大的损失。为了最大程度地降低损失，我们只能不断探寻系统何时会出现异常，甚至缩小到某些特定参数变化是否会造成系统异常。然而随着云原生的发展，不断推进着微服务的进一步解耦，海量的数据与用户规模也带来了基础设施的大规模分布式演进，系统中的故障变得越来越难以预料。&lt;strong>我们需要在系统中不断进行实验，主动找出系统的缺陷，这种方法被称作混沌工程&lt;/strong>。毕竟实践是检验真理的唯一标准，所以混沌工程可以帮助我们更加透彻地掌握系统的运行规律，提高系统的弹性能力。&lt;/p>
&lt;p>Litmus 是一种开源的云原生混沌工程工具集，专注于 Kubernetes 集群进行模拟故障测试，以帮助开发者和 SRE 发现集群及程序中的缺陷，从而提高系统的健壮性。&lt;/p>
&lt;h2 id="litmus-架构">Litmus 架构&lt;/h2>
&lt;p>Litmus 的架构如图所示：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/20210601004600.png" alt="">&lt;/p>
&lt;p>Litmus 的组件可以划分为两部分：&lt;/p>
&lt;ol>
&lt;li>Portal&lt;/li>
&lt;li>Agents&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Portal&lt;/strong> 是一组 Litmus 组件，作为跨云管理混沌实验的控制平面 (WebUI) ，用于协调和观察 Agent 上的混沌实验工作流。&lt;/p>
&lt;p>&lt;strong>Agent&lt;/strong> 也是一组 Litmus 组件，包括运行在 K8s 集群上的混沌实验工作流。&lt;/p>
&lt;p>使用 &lt;strong>Portal&lt;/strong>，用户可以在 &lt;strong>Agent&lt;/strong> 上创建和调度新的混沌实验工作流，并从 &lt;strong>Portal&lt;/strong> 上观察结果。用户还可以将更多的集群连接到 Portal，并将 Portal 作为跨云混沌工程管理的单个门户。&lt;/p>
&lt;h3 id="portal-组件">Portal 组件&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Litmus WebUI&lt;/strong>&lt;/p>
&lt;p>Litmus WebUI 提供了 Web 用户界面，用户可以在这里轻松构建和观察混沌实验工作流。Litmus WebUI 也充当了跨云混沌实验控制平面。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Litmus Server&lt;/strong>&lt;/p>
&lt;p>Litmus Server 作为中间件，用于处理来自用户界面的 API 请求，并将配置和处理结果详情信息存储到数据库中。它还充当各个请求之间的通信接口，并将工作流程调度到 &lt;strong>Agent&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Litmus DB&lt;/strong>&lt;/p>
&lt;p>Litmus DB 作为混沌实验工作流及其测试结果详情的存储系统。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="agent-组件">Agent 组件&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Chaos Operator&lt;/strong>&lt;/p>
&lt;p>Chaos Operator 监视 &lt;code>ChaosEngine&lt;/code> 并执行 &lt;code>CR&lt;/code> 中提到的混沌实验。Chaos Operator 是命名空间范围的，默认情况下运行在 &lt;code>litmus&lt;/code> 命名空间中。实验完成后，Chaos Operator 会调用 &lt;code>chaos-exporter&lt;/code> 将混沌实验的指标导出到 Prometheus 数据库中。&lt;/p></description></item><item><title>KubeSphere 部署 SkyWalking 至 Kubernetes 开启无侵入 APM</title><link>https://openksc.github.io/zh/blogs/skywalking-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/skywalking-kubesphere/</guid><description>&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200311155321.png" alt="">&lt;/p>
&lt;p>Kubernetes 天然适合分布式的微服务应用。然而，当开发者们将应用从传统的架构迁移到 Kubernetes 以后，会发现分布式的应用依旧存在各种各样的问题，例如大量微服务间的调用关系复杂、系统耗时或瓶颈难以排查、服务异常定位困难等一系列应用性能管理问题，而 APM 正是实时监控并管理微服务应用性能的利器。&lt;/p>
&lt;h2 id="为什么需要-apm">为什么需要 APM&lt;/h2>
&lt;p>APM 无疑是在大规模的微服务开发与运维场景下是必不可少的一环，APM 需要主要从这三个角度去解决三大场景问题：&lt;/p>
&lt;ul>
&lt;li>测试角度：性能测试调优监控总览，包括容器总体资源状况（如 CPU、内存、IO）与链路总体状况&lt;/li>
&lt;li>研发角度：链路服务的细节颗粒追踪，数据分析与数据安全&lt;/li>
&lt;li>运维角度：跟踪请求的处理过程，来对系统在前后端处理、服务端调用的性能消耗进行跟踪，实时感知并追踪访问体验差的业务&lt;/li>
&lt;/ul>
&lt;h2 id="为什么选择-apache-skywalking">为什么选择 Apache SkyWalking&lt;/h2>
&lt;p>社区拥有很丰富的 APM 解决方案，比如著名的 Pinpoint、Zipkin、SkyWalking、CAT 等。在经过一番调研后，KubeSphere 选择将 &lt;strong>Apache SkyWalking&lt;/strong> 作为面向 Kubernetes 的 APM 开源解决方案，将 Apache SkyWalking 集成到了 KubeSphere，作为应用模板在 &lt;strong>KubeSphere 容器平台&lt;/strong> 提供给用户一键部署至 Kubernetes 的能力，进一步增强在微服务应用维度的可观测性。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200311122646.png" alt="">&lt;/p>
&lt;p>Apache SkyWalking 在 2019 年 4 月 17 正式成为 &lt;strong>Apache 顶级项目&lt;/strong>，提供分布式追踪、服务网格遥测分析、度量聚合和可视化一体化解决方案。Apache SkyWalking 专为微服务、云原生和基于容器的架构而设计。这是 KubeSphere 选择 Apache SkyWalking 的主要原因。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200311150734.png" alt="">&lt;/p>
&lt;p>并且，Apache SkyWalking 本身还具有很多优势，包括多语言自动探针，比如 Java、.NET Core 和 Node.JS，能够实现无侵入式的探针接入 APM 检测，轻量高效，多种后端存储支持，提供链路拓扑与 Tracing 等优秀的可视化方案，模块化，提供 UI、存储、集群管理多种机制可选，并且支持告警。同时，Apache SkyWalking 还很容易与 SpringCloud 应用进行集成。&lt;/p></description></item><item><title>KubeSphere 部署 TiDB 云原生分布式数据库</title><link>https://openksc.github.io/zh/blogs/tidb-on-kbesphere-using-qke/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/tidb-on-kbesphere-using-qke/</guid><description>&lt;p>&lt;img src="https://ap3.qingstor.com/kubesphere-website/docs/20201028212049.png" alt="KubeSphere 部署 TiDB 云原生数据库">&lt;/p>
&lt;h2 id="tidb-简介">TiDB 简介&lt;/h2>
&lt;p>&lt;a href="https://pingcap.com/" target="_blank" rel="noopener noreferrer">TiDB&lt;/a> 是 PingCAP 公司自主设计、研发的开源分布式关系型数据库，具备水平扩容或者缩容、金融级高可用、实时 HTAP、云原生的分布式数据库、兼容 MySQL 5.7 协议和 MySQL 生态等重要特性。TiDB 适合高可用、强一致要求较高、数据规模较大等各种应用场景。&lt;/p>
&lt;p>&lt;img src="https://img-blog.csdnimg.cn/20201009174139735.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25ldHdvcmtlbg==,size_16,color_FFFFFF,t_70#pic_center" alt="TiDB 架构">&lt;/p>
&lt;h2 id="kubesphere-简介">KubeSphere 简介&lt;/h2>
&lt;p>&lt;a href="https://kubesphere.io" target="_blank" rel="noopener noreferrer">KubeSphere&lt;/a> 是在 Kubernetes 之上构建的以应用为中心的多租户容器平台，完全开源，支持多云与多集群管理，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。KubeSphere 提供了运维友好的向导式操作界面，帮助企业快速构建一个强大和功能丰富的容器云平台。&lt;/p>
&lt;p>&lt;img src="https://img-blog.csdnimg.cn/20201009114300360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25ldHdvcmtlbg==,size_16,color_FFFFFF,t_70#pic_center" alt="KubeSphere 架构">&lt;/p>
&lt;h2 id="部署环境准备">部署环境准备&lt;/h2>
&lt;p>KubeSphere 是由青云 QingCloud 开源的容器平台，&lt;strong>支持在任何基础设施上安装部署&lt;/strong>。在青云公有云上支持一键部署 KubeSphere（QKE）。&lt;/p>
&lt;p>下面以在青云云平台快速启用 KubeSphere 容器平台为例部署 TiDB 分布式数据库，至少需要准备 3 个可调度的 node 节点。你也可以在任何 Kubernetes 集群或 Linux 系统上安装 KubeSphere，可以参考 &lt;a href="https://kubesphere.io/docs" target="_blank" rel="noopener noreferrer">KubeSphere 官方文档&lt;/a>。&lt;/p>
&lt;ol>
&lt;li>登录青云控制台：&lt;a href="https://console.qingcloud.com/" target="_blank" rel="noopener noreferrer">https://console.qingcloud.com/&lt;/a>，点击左侧容器平台，选择 KubeSphere，点击创建并选择合适的集群规格：&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://img-blog.csdnimg.cn/20201021141612520.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25ldHdvcmtlbg==,size_16,color_FFFFFF,t_70#pic_center" alt="青云控制台">&lt;/p>
&lt;ol start="2">
&lt;li>创建完成后登录到 KubeSphere 平台界面：&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://img-blog.csdnimg.cn/20201021141829490.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25ldHdvcmtlbg==,size_16,color_FFFFFF,t_70#pic_center" alt="KubeSphere 平台界面">&lt;/p>
&lt;ol start="3">
&lt;li>点击下方的 Web Kubectl 集群客户端命令行工具，连接到 Kubectl 命令行界面。执行以下命令安装 TiDB Operator CRD：&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/pingcap/TiDB-Operator/v1.1.6/manifests/crd.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="4">
&lt;li>执行后的返回结果如下：&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://img-blog.csdnimg.cn/20201021135918671.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25ldHdvcmtlbg==,size_16,color_FFFFFF,t_70#pic_center" alt="Kubectl 命令行界面">&lt;/p></description></item><item><title>KubeSphere 部署 Zookeeper 实战教程</title><link>https://openksc.github.io/zh/blogs/zookeeper-on-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/zookeeper-on-kubesphere/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;h3 id="知识点">知识点&lt;/h3>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>如何利用 &lt;strong>AI 助手&lt;/strong>辅助运维工作&lt;/li>
&lt;li>单节点 Zookeeper 安装部署&lt;/li>
&lt;li>集群模式 Zookeeper 安装部署&lt;/li>
&lt;li>开源应用选型思想&lt;/li>
&lt;/ul>
&lt;h3 id="实战服务器配置架构-11-复刻小规模生产环境配置略有不同">实战服务器配置(架构 1:1 复刻小规模生产环境，配置略有不同)&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-worker-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker/CI&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">storage-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.81&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS/Ceph/Longhorn/NFS/&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">storage-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.82&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS/Ceph/Longhorn&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">storage-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.83&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS/Ceph/Longhorn&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">registry&lt;/td>
 &lt;td style="text-align: center">192.168.9.80&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Sonatype Nexus 3&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">10&lt;/td>
 &lt;td style="text-align: center">32&lt;/td>
 &lt;td style="text-align: center">88&lt;/td>
 &lt;td style="text-align: center">500&lt;/td>
 &lt;td style="text-align: center">1100+&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h2 id="实战环境涉及软件版本信息">实战环境涉及软件版本信息&lt;/h2>
&lt;ul>
&lt;li>操作系统：&lt;strong>openEuler 22.03 LTS SP2 x86_64&lt;/strong>&lt;/li>
&lt;li>KubeSphere：&lt;strong>3.3.2&lt;/strong>&lt;/li>
&lt;li>Kubernetes：&lt;strong>v1.24.12&lt;/strong>&lt;/li>
&lt;li>Containerd：&lt;strong>1.6.4&lt;/strong>&lt;/li>
&lt;li>GlusterFS：&lt;strong>10.0-8&lt;/strong>&lt;/li>
&lt;li>KubeKey: &lt;strong>v3.0.8&lt;/strong>&lt;/li>
&lt;li>Zookeeper：&lt;strong>3.8.2&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>今天我们的实战内容采用&lt;strong>场景模拟&lt;/strong>的形式，模拟真实运维工作中，&lt;strong>必然会&lt;/strong>遇到的一个场景。&lt;/p></description></item><item><title>KubeSphere 部署向量数据库 Milvus 实战指南</title><link>https://openksc.github.io/zh/blogs/deploy-milvus-on-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deploy-milvus-on-kubesphere/</guid><description>&lt;p>Milvus 是一个为通用人工智能（GenAI）应用而构建的开源向量数据库。它以卓越的性能和灵活性，提供了一个强大的平台，用于存储、搜索和管理大规模的向量数据。Milvus 能够执行高速搜索，并以最小的性能损失扩展到数百亿向量。其分布式架构确保了系统的高可用性和水平可扩展性，满足不断增长的数据需求。同时，Milvus 提供了丰富的 API 和集成选项，使其成为机器学习、计算机视觉和自然语言处理等 AI 应用的理想选择。&lt;/p>
&lt;p>随着 AI 大模型的兴起，Milvus 成为了众多 AI 应用的首选向量数据库。本文将引导您探索，如何在 KubeSphere 管理的 Kubernetes 集群上，高效地部署和管理 Milvus 集群，让您的应用能够充分利用 Milvus 的强大功能。&lt;/p>
&lt;p>&lt;strong>实战服务器配置(架构 1:1 复刻小规模生产环境，配置略有不同)&lt;/strong>&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ksp-registry&lt;/td>
 &lt;td style="text-align: center">192.168.9.90&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Harbor 镜像仓库&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.94&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker/CI&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">400+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/Longhorn/Ceph/NFS&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.98&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">300+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/Longhorn/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.99&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">300+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/Longhorn/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gpu-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.101&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker(GPU NVIDIA Tesla M40 24G)&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gpu-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.102&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker(GPU NVIDIA Tesla P100 16G)&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gateway-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.103&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;td style="text-align: center">自建应用服务代理网关/VIP：192.168.9.100&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gateway-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.104&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;td style="text-align: center">自建应用服务代理网关/VIP：192.168.9.100&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-mid&lt;/td>
 &lt;td style="text-align: center">192.168.9.105&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">部署在 k8s 集群之外的服务节点（Gitlab 等）&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">15&lt;/td>
 &lt;td style="text-align: center">68&lt;/td>
 &lt;td style="text-align: center">152&lt;/td>
 &lt;td style="text-align: center">600&lt;/td>
 &lt;td style="text-align: center">2100+&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>实战环境涉及软件版本信息&lt;/strong>&lt;/p></description></item><item><title>KubeSphere 的异地多活方案探索</title><link>https://openksc.github.io/zh/blogs/kubesphere-multihosts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-multihosts/</guid><description>&lt;p>遇到这样一个场景，在同一套环境中需要存在多个 Host 控制面集群，因此想探索下 KubeSphere 的异地多活混合容器云管理方案。&lt;/p>
&lt;h2 id="集群角色介绍">集群角色介绍&lt;/h2>
&lt;p>一个兼容原生的 K8s 集群，可通过 &lt;code>ks-installer&lt;/code> 来初始化完成安装，成为一个 QKE 集群。QKE 集群分为多种角色，默认是 None 角色（standalone 模式），开启多集群功能时，可以设置为 Host 或者 Member 角色。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kcp-multicluster.png" alt="">&lt;/p>
&lt;ul>
&lt;li>None 角色，是最小化安装的默认模式，会安装必要的 ks-apiserver, ks-controller-manager, ks-console 和其他组件
&lt;ul>
&lt;li>ks-apiserver, kcp 的 API 网关，包含审计、认证、权限校验等功能&lt;/li>
&lt;li>ks-controller, 各类自定义 CRD 的控制器和平台管理逻辑的实现&lt;/li>
&lt;li>ks-console, 前端界面 UI&lt;/li>
&lt;li>ks-installer, 初始化安装和变更 QKE 集群的工具，由 shell-operator 触发 ansible-playbooks 来工作。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Member 角色，承载工作负载的业务集群，和 None 模式的组件安装情况一致&lt;/li>
&lt;li>Host 角色，整个混合云管理平台的控制面，会在 None 的基础上，再额外安装 tower，kubefed-controller-manager， kubefed-admission-webhook 等组件
&lt;ul>
&lt;li>tower，代理业务集群通信的 server 端，常用于不能直连 Member 集群 api-server 的情况&lt;/li>
&lt;li>kubefed-controller-manager，社区的 &lt;a href="https://github.com/kubernetes-sigs/kubefed" target="_blank" rel="noopener noreferrer">Kubefed&lt;/a> 联邦资源的控制器&lt;/li>
&lt;li>kubefed-admission-webhook， 社区的 Kubefed 联邦资源的动态准入校验器&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="多集群管理原理">多集群管理原理&lt;/h2>
&lt;p>上段提到 QKE 有 3 种角色，可通过修改 &lt;code>cc&lt;/code> 配置文件的 &lt;code>clusterRole&lt;/code> 来使能, ks-installer 监听到配置变化的事件，会初始化对应集群角色的功能。&lt;/p></description></item><item><title>KubeSphere 对 Apache Log4j 2 远程代码执行最新漏洞的修复方案</title><link>https://openksc.github.io/zh/blogs/apache-log4j2-vulnerability-solution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/apache-log4j2-vulnerability-solution/</guid><description>&lt;p>Apache Log4j 2 是一款开源的日志记录工具，被广泛应用于各类框架中。近期，Apache Log4j 2 被爆出存在漏洞，漏洞现已公开，本文为 KubeSphere 用户提供建议的修复方案。&lt;/p>
&lt;p>此次漏洞是由于 Log4j 2 提供的 lookup 功能造成的，该功能允许开发者通过一些协议去读取相应环境中的配置。但在实现的过程中，并未对输入进行严格的判断，从而造成漏洞的发生。由于大量的软件都使用了 Log4j 2 插件，所以大量的 Java 类产品均被波及，包括但不限于 Apache Solr、srping-boot-strater-log4j2、Apache Struts2、ElasticSearch、Dubbo、Redis、Logstash、Kafka...更多组件可以参考 &lt;a href="https://mvnrepository.com/artifact/org.apache.logging.log4j/log4j-core/usages?p=1" target="_blank" rel="noopener noreferrer">Log4j 2 相关文档&lt;/a>。&lt;/p>
&lt;p>受影响的 Log4j 版本为 Apache Log4j 2.x &amp;lt; 2.15.0-rc2。目前官方发布了 Apache 2.15.0-rc2 版本对该漏洞进行了修复，但是该版本并非正式发行版，故存在不稳定的因素，如要升级建议对相关数据进行备份。&lt;/p>
&lt;blockquote>
&lt;p>受影响的 KubeSphere 版本为 v3.2.1 及其以下；受影响的 ElasticSearch 版本为 v6.8.22 以下及 v7.16.2 以下。&lt;/p>&lt;/blockquote>
&lt;p>同时，也提供了三种方法对漏洞进行补救，为&lt;/p>
&lt;ul>
&lt;li>将系统环境变量 &lt;code>FORMAT_MESSAGES_PATTERN_DISABLE_LOOKUPS&lt;/code> 设置为 &lt;code>true&lt;/code>&lt;/li>
&lt;li>修改配置 &lt;code>log4j2.formatMsgNoLookups=True&lt;/code>&lt;/li>
&lt;li>修改 JVM 参数 &lt;code>-Dlog4j2.formatMsgNoLookups=true&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>以下三种解决方法，您可以任选其中一种进行参考。&lt;/p>
&lt;h2 id="方法一修改系统环境变量">方法一：修改系统环境变量&lt;/h2>
&lt;p>由于 KubeSphere 默认使用了 ElasticSearch 收集日志，所以也应该在 KubeSphere 修改相应的配置来对漏洞进行修复。以下说明如何在 KubeSphere 中进行相应的操作对 ElasticSearch 进行修复。&lt;/p></description></item><item><title>KubeSphere 多租户与认证鉴权实践：使用 GitLab 账号登陆 KubeSphere</title><link>https://openksc.github.io/zh/blogs/gitlab-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/gitlab-kubesphere/</guid><description>&lt;h2 id="介绍">介绍&lt;/h2>
&lt;p>KubeSphere 多租户是实际生产使用中非常需要的一个功能，该功能满足不同用户登陆 KubeSphere 平台的需求。比如开发，运维，测试都需要登陆 KubeSphere 平台，并且需要为不同身份的用户配置不同的权限。当公司内需要访问 KubeSphere 的用户比较多时，管理员再去手动为用户创建账号就不太灵活了。KubeSphere 包含一个内置的 OAuth 服务和帐户系统，用户通过获取 OAuth 访问令牌以对 API 进行身份验证，我们可以通过接入 LDAP 或者 OIDC 来提供身份认证信息。&lt;/p>
&lt;h3 id="多租户方案">多租户方案&lt;/h3>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/image-20210802131844011.png" alt="">&lt;/p>
&lt;h3 id="认证鉴权链路">认证鉴权链路&lt;/h3>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/image-20210802133353633.png" alt="">&lt;/p>
&lt;h2 id="使用">使用&lt;/h2>
&lt;p>假设集群内已经最小化安装 KubeSphere。我们这里使用 OIDC 身份提供者进行认证，通过 Dex 接入到 GitLab 中，使用 GitLab 中的用户完成认证。&lt;/p>
&lt;h3 id="安装-dex">安装 Dex&lt;/h3>
&lt;p>Dex 是一种身份认证服务，它使用 OpenID Connect 来驱动其他应用程序的身份验证。Dex 通过 “connectors” 充当其他身份提供商的门户。 Dex 可以将身份验证推到 LDAP 服务器、SAML 提供商或已建立的身份提供商（如 GitHub、Gitlab、Google 和 Active Directory等）。 客户端编写身份验证逻辑以与 Dex 交互认证，然后 Dex 通过 connector 转发到后端用户认证方进行认证，并返回给客户端 Oauth2 Token。与其相似的身份认证服务还有 Keycloak，auth0 等。&lt;/p>
&lt;p>首先需要在 gitlab 上创建应用，在范围里勾选这几个 &lt;code>read_user&lt;/code> &lt;code>profile&lt;/code> &lt;code>email&lt;/code> &lt;code>openid&lt;/code>，创建后需要记住页面上的应用程序 id 和密码，后面会用得到。&lt;/p></description></item><item><title>KubeSphere 核心架构浅析</title><link>https://openksc.github.io/zh/blogs/kubesphere-core-architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-core-architecture/</guid><description>&lt;p>KubeSphere 是在 Kubernetes 之上构建的面向云原生应用的容器混合云管理系统。支持多云与多集群管理，提供全栈的自动化运维能力，帮助企业用户简化 DevOps 工作流，提供了运维友好的向导式操作界面，帮助企业快速构建一个强大和功能丰富的容器云平台。&lt;/p>
&lt;p>KubeSphere 为用户提供构建企业级 Kubernetes 环境所需的多项功能，例如多云与多集群管理、Kubernetes 资源管理、DevOps、应用生命周期管理、微服务治理（服务网格）、日志查询与收集、服务与网络、多租户管理、监控告警、事件与审计查询、存储管理、访问权限控制、GPU 支持、网络策略、镜像仓库管理以及安全管理等。&lt;/p>
&lt;p>得益于 Kubernetes 优秀的架构与设计，KubeSphere 取长补短采用了更为轻量的架构模式，灵活的整合资源，进一步丰富了 K8s 生态。&lt;/p>
&lt;h2 id="kubesphere-核心架构">KubeSphere 核心架构&lt;/h2>
&lt;p>KubeSphere 的核心架构如下图所示：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/1629366117-239933-image.png" alt="">&lt;/p>
&lt;p>核心组件主要有三个：&lt;/p>
&lt;ul>
&lt;li>ks-console 前端服务组件&lt;/li>
&lt;li>ks-apiserver 后端服务组件&lt;/li>
&lt;li>ks-controller-manager 资源状态维护组件&lt;/li>
&lt;/ul>
&lt;p>KubeSphere 的后端设计中沿用了 K8s 声明式 API 的风格，所有可操作的资源都尽可能的抽象成为 &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" target="_blank" rel="noopener noreferrer">CustomResource&lt;/a>。与命令式 API 相比，声明性API的使用更加简洁，并且提供了更好的抽象性, 告诉程序最终的期望状态（做什么），而不关心怎么做。&lt;/p>
&lt;p>典型地，在声明式 API 中：&lt;/p>
&lt;ol>
&lt;li>你的 API 包含相对而言为数不多的、尺寸较小的对象（资源）。&lt;/li>
&lt;li>对象定义了应用或者基础设施的配置信息。&lt;/li>
&lt;li>对象更新操作频率较低。&lt;/li>
&lt;li>通常需要人来读取或写入对象。&lt;/li>
&lt;li>对象的主要操作是 CRUD 风格的（创建、读取、更新和删除）。&lt;/li>
&lt;li>不需要跨对象的事务支持：API 对象代表的是期望状态而非确切实际状态。&lt;/li>
&lt;/ol>
&lt;p>命令式 API（Imperative API）与声明式有所不同。 以下迹象表明你的 API 可能不是声明式的：&lt;/p>
&lt;ol>
&lt;li>客户端发出“做这个操作”的指令，之后在该操作结束时获得同步响应。&lt;/li>
&lt;li>客户端发出“做这个操作”的指令，并获得一个操作 ID，之后需要判断请求是否成功完成。&lt;/li>
&lt;li>你会将你的 API 类比为RPC。&lt;/li>
&lt;li>直接存储大量数据。&lt;/li>
&lt;li>在对象上执行的常规操作并非 CRUD 风格。&lt;/li>
&lt;li>API 不太容易用对象来建模。&lt;/li>
&lt;/ol>
&lt;p>借助 kube-apiserver、etcd 实现数据同步和数据持久化，通过 ks-controller-manager 维护这些资源的状态，以达到最终状态的一致性。如果你熟悉 K8s，可以很好的理解声明式 API 带来的好处，这也是 KubeSphere 最为核心的部分。&lt;/p></description></item><item><title>KubeSphere 后端源码深度解析</title><link>https://openksc.github.io/zh/blogs/kubesphere-backend-sourcecode/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-backend-sourcecode/</guid><description>&lt;p>这篇文章我们将学习在 vscode 上的 ssh remote 插件基础上，尝试 debug 和学习 KubeSphere 后端模块架构。&lt;/p>
&lt;h2 id="前提">前提&lt;/h2>
&lt;ul>
&lt;li>安装好 vscode 以及 ssh remote container 插件；&lt;/li>
&lt;li>在远程主机上安装好 kubenertes 容器 &amp;quot; 操作系统 &amp;quot; 和 KubeSphere &amp;gt;= v3.1.0 云“控制面板”；&lt;/li>
&lt;li>安装 go &amp;gt;=1.16;&lt;/li>
&lt;li>在 KubeSphere 上安装了需要 debug 的 KubeSphere 组件，如 devops、kubeedge 或者 whatever, 如果是默认激活的组件，像 monitoring，不需要去激活。&lt;/li>
&lt;/ul>
&lt;h2 id="配置-launch-文件">配置 launch 文件&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">cat&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">.vscode/launch.json&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// 使用 IntelliSense 了解相关属性。 
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// 悬停以查看现有属性的描述。
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#f92672">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;0.2.0&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;configurations&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ks-apiserver&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;go&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;request&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;launch&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;mode&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;program&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;${workspaceFolder}/cmd/ks-apiserver/apiserver.go&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="ks-apiserver-调试依赖文件">ks-apiserver 调试依赖文件&lt;/h2>
&lt;p>在相对路径 cmd/ks-apiserver/ 下配置 kubesphere.yaml。&lt;/p></description></item><item><title>KubeSphere 集群配置 NFS 存储解决方案</title><link>https://openksc.github.io/zh/blogs/kubesphere-nfs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-nfs/</guid><description>&lt;blockquote>
&lt;p>作者：申红磊，QingCloud 容器解决方案架构师，开源项目爱好者，KubeSphere Member&lt;/p>&lt;/blockquote>
&lt;p>在正式阅读本文之前，先友情提醒一下：不建议您在生产环境中使用 NFS 存储（特别是 Kubernetes 1.20 或以上版本），原因如下：&lt;/p>
&lt;ul>
&lt;li>selfLink was empty 在 K8s 集群 v1.20 之前都存在，在 v1.20 之后被删除问题。&lt;/li>
&lt;li>还有可能引起 failed to obtain lock 和 input/output error 等问题，从而导致 Pod CrashLoopBackOff。此外，部分应用不兼容 NFS，例如 Prometheus 等。&lt;/li>
&lt;/ul>
&lt;h3 id="安装-nfs-server">安装 NFS Server&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#安装 NFS 服务器端&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo apt-get update &lt;span style="color:#75715e">#执行以下命令确保使用最新软件包&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo apt-get install nfs-kernel-server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#安装 NFS 客户端&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo apt-get install nfs-common
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># yum&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ yum install -y nfs-utils
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="创建共享目录">创建共享目录&lt;/h3>
&lt;p>先查看配置文件 /etc/exports：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ cat /etc/exports
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># /etc/exports: the access control list for filesystems which may be exported&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#		to NFS clients. See exports(5).&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Example for NFSv2 and NFSv3:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># /srv/homes hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Example for NFSv4:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># /srv/nfs4 gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># /srv/nfs4/homes gss/krb5i(rw,sync,no_subtree_check)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>创建共享目标并赋权：&lt;/p></description></item><item><title>KubeSphere 接入外部 Elasticsearch 最佳实践</title><link>https://openksc.github.io/zh/blogs/kubesphere-elasticsearch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-elasticsearch/</guid><description>&lt;blockquote>
&lt;p>作者：张坚，科大讯飞开发工程师，云原生爱好者。&lt;/p>&lt;/blockquote>
&lt;p>大家好，我是张坚。今天来聊聊如何在 KubeSphere 中集成外置的 ES 组件。&lt;/p>
&lt;p>KubeSphere 在安装完成时候可以启用日志组件，这样会安装 ES 组件并可以收集所有部署组件的日志，也可以收集审计日志，然后可以很方便的在 KubeSphere 平台上进行日志查询。&lt;/p>
&lt;p>但是在实际使用过程中发现使用 KubeSphere 自身的 ES 会很重，而且官方也建议我们将日志接入到外部的 ES 中减轻 Kubernetes 的压力。&lt;/p>
&lt;p>以下为操作实战。&lt;/p>
&lt;h2 id="前置步骤">前置步骤&lt;/h2>
&lt;p>&lt;strong>ES 集群需支持 http 协议&lt;/strong>&lt;/p>
&lt;p>1️⃣ 搭建好外部 ES 集群，使用 http 协议（非本文重点）。&lt;/p>
&lt;blockquote>
&lt;p>测试环境 IP: 172.30.10.226,172.30.10.191,172.30.10.184&lt;br>
port: 9200&lt;br>
username: elastic&lt;br>
password: changeme&lt;br>&lt;/p>&lt;/blockquote>
&lt;p>2️⃣ 对 ES 做负载均衡。&lt;/p>
&lt;p>有三种常见的做法：&lt;/p>
&lt;ol>
&lt;li>使用 nginx 做负载均衡；&lt;/li>
&lt;li>单协调节点；&lt;/li>
&lt;li>通过自定义 service 和 endpoints 负载均衡。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>&lt;strong>本文档基于第三种负载均衡方案（通过 endpoints 负载）做对接。&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;h2 id="备份-ks-installer">备份 ks-installer&lt;/h2>
&lt;p>管理员账号登录 KubeSphere，在平台管理 - 集群管理 - CRD 中搜索 clusterconfiguration，在&lt;strong>自定义资源&lt;/strong>中，点击 &lt;code>ks-installer&lt;/code> 选择&lt;strong>编辑 YAML&lt;/strong> ，复制备份。&lt;/p></description></item><item><title>KubeSphere 开源 KubeEye：Kubernetes 集群自动巡检工具</title><link>https://openksc.github.io/zh/blogs/kubeeye-automatic-cluster-diagnostic-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubeeye-automatic-cluster-diagnostic-tool/</guid><description>&lt;h2 id="为什么开源-kubeeye">为什么开源 KubeEye&lt;/h2>
&lt;p>Kubernetes 作为容器编排的事实标准，虽然架构优雅功能也非常强大，但是 Kubernetes 在日常运行过程中总会有一些疑难杂症和隐性的问题让集群管理员和 Yaml 工程师们非常头疼。&lt;/p>
&lt;ul>
&lt;li>基础设施守护进程问题：ntp 服务中断；&lt;/li>
&lt;li>硬件问题：如 CPU，内存或磁盘异常；&lt;/li>
&lt;li>内核问题：内核死锁，文件系统损坏；&lt;/li>
&lt;li>容器运行时问题：运行时守护进程无响应；&lt;/li>
&lt;li>···&lt;/li>
&lt;/ul>
&lt;p>这样的问题还有很多，并且这些隐性的异常问题对集群的控制面来说是不可见的，因此 Kubernetes 将继续将 Pod 调度到异常的节点，进而造成集群和运行的应用带来非常大的安全与稳定性的风险。&lt;/p>
&lt;h2 id="什么是-kubeeye">什么是 KubeEye&lt;/h2>
&lt;p>KubeEye 是一款&lt;strong>开源的集群自动巡检工具&lt;/strong>，旨在发现 Kubernetes 上的各种问题，比如应用配置错误、集群组件不健康和节点问题。KubeEye 使用 Go 语言基于 &lt;a href="https://github.com/FairwindsOps/polaris" target="_blank" rel="noopener noreferrer">Polaris&lt;/a> 和 &lt;a href="https://github.com/kubernetes/node-problem-detector" target="_blank" rel="noopener noreferrer">Node-Problem-Detector&lt;/a> 开发，内置了一系列异常检测规则。除了预定义的规则，它还支持自定义规则。&lt;/p>
&lt;h2 id="kubeeye-能做什么">KubeEye 能做什么&lt;/h2>
&lt;ul>
&lt;li>发现与检测 Kubernetes 集群控制平面的问题，包括 &lt;strong>kube-apiserver/kube-controller-manager/etcd&lt;/strong> 等；&lt;/li>
&lt;li>帮助你检测 Kubernetes 的各种节点问题，包括内存/CPU/磁盘压力，意外的内核错误日志等；&lt;/li>
&lt;li>根据行业最佳实践验证你的工作负载 yaml 规范，帮助你使你的集群稳定。&lt;/li>
&lt;/ul>
&lt;h2 id="架构图">架构图&lt;/h2>
&lt;p>KubeEye 通过调用 Kubernetes API，通过常规匹配日志中的关键错误信息和容器语法的规则匹配来获取集群诊断数据，详见架构。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubeeye-architecture.png" alt="">&lt;/p>
&lt;h2 id="内置检查项">内置检查项&lt;/h2>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>是/否&lt;/th>
 &lt;th>检查项&lt;/th>
 &lt;th>描述&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>ETCDHealthStatus&lt;/td>
 &lt;td>如果 etcd 启动并正常运行&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>ControllerManagerHealthStatus&lt;/td>
 &lt;td>如果 kubernetes kube-controller-manager 正常启动并运行&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>SchedulerHealthStatus&lt;/td>
 &lt;td>如果 kubernetes kube-schedule 正常启动并运行&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>NodeMemory&lt;/td>
 &lt;td>如果节点内存使用量超过阈值&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>DockerHealthStatus&lt;/td>
 &lt;td>如果 docker 正常运行&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>NodeDisk&lt;/td>
 &lt;td>如果节点磁盘使用量超过阈值&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>KubeletHealthStatus&lt;/td>
 &lt;td>如果 kubelet 激活状态且正常运行&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>NodeCPU&lt;/td>
 &lt;td>如果节点 CPU 使用量超过阈值&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>NodeCorruptOverlay2&lt;/td>
 &lt;td>Overlay2 不可用&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>NodeKernelNULLPointer&lt;/td>
 &lt;td>node 显示 NotReady&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>NodeDeadlock&lt;/td>
 &lt;td>死锁是指两个或两个以上的进程在争夺资源时互相等待的现象。&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>NodeOOM&lt;/td>
 &lt;td>监控那些消耗过多内存的进程，尤其是那些消耗大量内存非常快的进程，内核会杀掉它们，防止它们耗尽内存&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>NodeExt4Error&lt;/td>
 &lt;td>Ext4 挂载失败&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>NodeTaskHung&lt;/td>
 &lt;td>检查D状态下是否有超过 120s 的进程&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>NodeUnregisterNetDevice&lt;/td>
 &lt;td>检查对应网络&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>NodeCorruptDockerImage&lt;/td>
 &lt;td>检查 docker 镜像&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>NodeAUFSUmountHung&lt;/td>
 &lt;td>检查存储&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>NodeDockerHung&lt;/td>
 &lt;td>Docker hang住, 检查 docker 的日志&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodSetLivenessProbe&lt;/td>
 &lt;td>如果为pod中的每一个容器设置了 livenessProbe&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodSetTagNotSpecified&lt;/td>
 &lt;td>镜像地址没有声明标签或标签是最新&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodSetRunAsPrivileged&lt;/td>
 &lt;td>以特权模式运行 Pod 意味着 Pod 可以访问主机的资源和内核功能&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodSetImagePullBackOff&lt;/td>
 &lt;td>Pod 无法正确拉出镜像，因此可以在相应节点上手动拉出镜像&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodSetImageRegistry&lt;/td>
 &lt;td>检查镜像形式是否在相应仓库&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodSetCpuLimitsMissing&lt;/td>
 &lt;td>未声明 CPU 资源限制&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodNoSuchFileOrDirectory&lt;/td>
 &lt;td>进入容器查看相应文件是否存在&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodIOError&lt;/td>
 &lt;td>这通常是由于文件 IO 性能瓶颈&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodNoSuchDeviceOrAddress&lt;/td>
 &lt;td>检查对应网络&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodInvalidArgument&lt;/td>
 &lt;td>检查对应存储&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodDeviceOrResourceBusy&lt;/td>
 &lt;td>检查对应的目录和 PID&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodFileExists&lt;/td>
 &lt;td>检查现有文件&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodTooManyOpenFiles&lt;/td>
 &lt;td>程序打开的文件/套接字连接数超过系统设置值&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodNoSpaceLeftOnDevice&lt;/td>
 &lt;td>检查磁盘和索引节点的使用情况&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>NodeApiServerExpiredPeriod&lt;/td>
 &lt;td>将检查 ApiServer 证书的到期日期少于30天&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodSetCpuRequestsMissing&lt;/td>
 &lt;td>未声明 CPU 资源请求值&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodSetHostIPCSet&lt;/td>
 &lt;td>设置主机 IP&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodSetHostNetworkSet&lt;/td>
 &lt;td>设置主机网络&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodHostPIDSet&lt;/td>
 &lt;td>设置主机 PID&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodMemoryRequestsMiss&lt;/td>
 &lt;td>没有声明内存资源请求值&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodSetHostPort&lt;/td>
 &lt;td>设置主机端口&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodSetMemoryLimitsMissing&lt;/td>
 &lt;td>没有声明内存资源限制值&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodNotReadOnlyRootFiles&lt;/td>
 &lt;td>文件系统未设置为只读&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodSetPullPolicyNotAlways&lt;/td>
 &lt;td>镜像拉策略并非总是如此&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodSetRunAsRootAllowed&lt;/td>
 &lt;td>以 root 用户执行&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodDangerousCapabilities&lt;/td>
 &lt;td>您在 ALL / SYS_ADMIN / NET_ADMIN 等功能中有危险的选择&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>PodlivenessProbeMissing&lt;/td>
 &lt;td>未声明 ReadinessProbe&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>√&lt;/td>
 &lt;td>privilegeEscalationAllowed&lt;/td>
 &lt;td>允许特权升级&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;/td>
 &lt;td>NodeNotReadyAndUseOfClosedNetworkConnection&lt;/td>
 &lt;td>http 2-max-streams-per-connection&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;/td>
 &lt;td>NodeNotReady&lt;/td>
 &lt;td>无法启动 ContainerManager 无法设置属性 TasksAccounting 或未知属性&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h2 id="怎么使用">怎么使用&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>机器上安装 KubeEye&lt;/p></description></item><item><title>KubeSphere 使用 HTTPS 协议集成 Harbor 镜像仓库指南</title><link>https://openksc.github.io/zh/blogs/kubesphere-harbor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-harbor/</guid><description>&lt;blockquote>
&lt;p>作者：申红磊，青云科技容器解决方案架构师，开源项目爱好者，KubeSphere Member。&lt;/p>&lt;/blockquote>
&lt;p>上面两篇文章讲了如何&lt;a href="https://shenhonglei.blog.csdn.net/article/details/124183802" target="_blank" rel="noopener noreferrer">部署 HTTPS Harbor&lt;/a> 和&lt;a href="https://blog.csdn.net/shenhonglei1234/article/details/124837763" target="_blank" rel="noopener noreferrer">对接 HTTP 的 Harbor 镜像仓库&lt;/a>；接下来详细介绍一下，如何添加基于 HTTPS 的 Harbor 镜像仓库对接使用说明。&lt;/p>
&lt;p>因为 KubeSphere 无法直接解析 Harbor 域名，需要在 CoreDNS 添加解析记录，否则会报 no such host。&lt;/p>
&lt;h2 id="nodelocal-dnscache">NodeLocal DNSCache&lt;/h2>
&lt;p>NodeLocal DNSCache 通过在集群上运行一个 DNSCache Daemonset 来提高 clusterDNS 性能和可靠性。相比于纯 CoreDNS 方案，nodelocaldns + CoreDNS 方案能够大幅降低 DNS 查询 timeout 的频次，提升服务稳定性。&lt;/p>
&lt;p>nodelocaldns 通过添加 iptables 规则能够接收节点上所有发往 xxx.xxx.xx.xx 的 DNS 查询请求，把针对集群内部域名查询请求路由到 CoreDNS；把集群外部域名请求直接通过 Host 网络发往集群外部 DNS 服务器。&lt;/p>
&lt;h2 id="将-nodelocaldns-解析都转发给-coredns">将 nodelocaldns 解析都转发给 coredns&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#forward ./etc/resolv.conf指向coredns service ip&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#search coredns service ip&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get svc coredns -n kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT&lt;span style="color:#f92672">(&lt;/span>S&lt;span style="color:#f92672">)&lt;/span> AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>coredns ClusterIP 10.233.0.3 &amp;lt;none&amp;gt; 53/UDP,53/TCP,9153/TCP 28d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#修改nodelocaldns的配置configmap的值&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl edit cm nodelocaldns -n kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">## 将 forward ./etc/resolv.conf 调整为：forward . 10.233.0.3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Corefile: |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster.local:53 &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> errors
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cache &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> success &lt;span style="color:#ae81ff">9984&lt;/span> &lt;span style="color:#ae81ff">30&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> denial &lt;span style="color:#ae81ff">9984&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reload
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> loop
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> bind 169.254.25.10
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> forward . 10.233.0.3 &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> force_tcp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> prometheus :9253
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> health 169.254.25.10:9254
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">## 保存、重启、或者手动重启，效果一样&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202211091927610.png" alt="">&lt;/p></description></item><item><title>KubeSphere 网关的设计与实现</title><link>https://openksc.github.io/zh/blogs/kubesphere-gateway/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-gateway/</guid><description>&lt;h2 id="kubesphere-中为什么需要网关">KubeSphere 中为什么需要网关？&lt;/h2>
&lt;p>如果需要将 K8s 集群内的服务暴露到外部访问有那些方式呢？可以通过将 Service 设置成 NodePort 方式暴露出去或者通过 Ingress 方式。另外使用 Ingress 方式可以实现将请求分发到一个或多个 Service，可以同一个 IP 地址下暴露多个服务等优势。&lt;/p>
&lt;p>但是对于 Ingress 方式而言，在 K8s 中只是内置了 Ingress CRD（可以创建 Ingress 资源），没有内置 Ingress Controller，必须部署了 Ingress Controller 才能为 Ingress 资源提供外部访问集群内部服务的能力。而 KubeSphere 中的网关就是 Ingress Controller 。&lt;/p>
&lt;h2 id="网关的设计">网关的设计&lt;/h2>
&lt;p>KubeSphere v3.2 对网关进行了重构，在保留了原有网关功能的基础上增加了以下几点新功能：&lt;/p>
&lt;ol>
&lt;li>启用集群和项目级别的网关：可以根据业务上的需求灵活选择不同粒度的网关。&lt;/li>
&lt;li>增减网关副本数：灵活调整副本数达到更高的可用性。&lt;/li>
&lt;li>灵活配置 Ingress Controller 配置选项。&lt;/li>
&lt;li>可指定网关应用负载安装的位置：可选择将网关应用负载安装的位置指定某固定命名空间或分别让其位于各自项目命名空间下。结合 KubeSphere 中的权限管理，若让资源位于各个项目命名空间下，拥有该项目权限的用户也能查看到网关资源。&lt;/li>
&lt;li>网关日志：集中查询网关日志，将分布在各个副本的网关日志集中起来查询。&lt;/li>
&lt;li>网关监控指标：监控网关中的一些指标，包括请求总量/成功率/延迟 等指标。&lt;/li>
&lt;/ol>
&lt;h2 id="网关的实现">网关的实现&lt;/h2>
&lt;p>目前 K8s 支持和维护 &lt;a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller#readme" target="_blank" rel="noopener noreferrer">AWS&lt;/a>、 &lt;a href="https://git.k8s.io/ingress-gce/README.md" target="_blank" rel="noopener noreferrer">GCE&lt;/a> 和 &lt;a href="https://git.k8s.io/ingress-nginx/README.md#readme" target="_blank" rel="noopener noreferrer">Nginx&lt;/a> Ingress 控制器，KubeSphere 使用 &lt;a href="https://github.com/kubernetes/ingress-nginx" target="_blank" rel="noopener noreferrer">Ingress Nginx Controller&lt;/a> 作为默认的网关实现，没有做任何代码修改。&lt;/p>
&lt;h3 id="各个功能点的实现思路">各个功能点的实现思路&lt;/h3>
&lt;ul>
&lt;li>集群和项目级别的网关：这个通过传入参数覆盖默认的 Helm Chart Values 来实现并在代码逻辑里控制，如果启用了集群网关就不能启用项目网关了；若启用了项目网关又启用了集群网关，那么通过两个网关入口都可以访问，只是这样会有两个 Ingress Controller 同时 Watch 相同的 Ingress 对象。&lt;/li>
&lt;li>增减网关副本数&amp;amp;配置 Ingress Controller 配置选项：这个通过传入参数覆盖默认的 Helm Chart Values 来实现，实现过程用到的 Helm Operator 将在后面重点介绍。&lt;/li>
&lt;li>可指定网关应用负载安装的位置：可选择将网关应用负载安装的位置指定某固定命名空间或分别让其位于各自项目命名空间下。这个在代码逻辑中控制，并做成了配置项，默认将所有资源安装在 kubesphere-controls-system 下。&lt;/li>
&lt;li>网关日志：使用到了 KubeSphere 中日志组件，日志组件会采集日志数据然后存储在 Elasticsearch 中，网关在查询日志过程就根据参数在 Elasticsearch 中查询日志。&lt;/li>
&lt;li>网关监控指标：使用到了 KubeSphere 中监控组件，KubeSphere 内部配置了 Prometheus 相关的参数采集 Ingress 相关指标，查询监控信息过程就根据监控组件中的 API 查询相关数据。&lt;/li>
&lt;/ul>
&lt;p>下面重点介绍设计实现过程抽象出的 CRD 和如何巧妙地用 Helm Operator 集成。&lt;/p></description></item><item><title>KubeSphere 与 Jenkins 的集成解析</title><link>https://openksc.github.io/zh/blogs/kubesphere-jenkins/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-jenkins/</guid><description>&lt;blockquote>
&lt;p>作者：gfengwong&lt;/p>&lt;/blockquote>
&lt;h2 id="kubesphere-的-devops-模块介绍">KubeSphere 的 DevOps 模块介绍&lt;/h2>
&lt;ul>
&lt;li>KubeSphere 使用可插拔的 DevOps 模块实现 DevOps 功能；&lt;/li>
&lt;li>DevOps 驱动 Jenkins 实现具体的操作，例如流水线等。&lt;/li>
&lt;/ul>
&lt;p>DevOps 与 KubeSphere 的关系如下图, 详细的&lt;a href="https://kubesphere.io/docs/v3.3/introduction/architecture/" target="_blank" rel="noopener noreferrer">组件介绍&lt;/a>。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/20190810073322.png" alt="">&lt;/p>
&lt;h2 id="集成的亮点">集成的亮点&lt;/h2>
&lt;p>DevOps 与 Jenkins 集成紧密且优雅，从构建、部署到使用维护纯云原生方式实现：&lt;/p>
&lt;ul>
&lt;li>一键部署；&lt;/li>
&lt;li>一个参数启用 DevOps 功能；&lt;/li>
&lt;li>一个 K8s 集群内即可完成从 Jenkins、流水线的全生命周期。&lt;/li>
&lt;/ul>
&lt;h2 id="具体集成说明">具体集成说明&lt;/h2>
&lt;p>用户使用 KubeSphere 平台的 DevOps 功能时，调用 devops-api 发送请求，DevOps 收到请求后，部分请求直接调用 Jenkins 进行操作，部分请求通过更新 devops-controller 监听的资源，通过 devops-controller 来操作 Jenkins。&lt;/p>
&lt;p>运行流水线阶段，Jenkins 配置了 K8s 动态 slave：&lt;/p>
&lt;ul>
&lt;li>Jenkins pod 信息(镜像、卷等)发送给 K8s；&lt;/li>
&lt;li>K8s 启动 Jenkins slave pod 并通过远程协议与 Jenkins master 建立连接；&lt;/li>
&lt;li>运行流水线；&lt;/li>
&lt;li>运行完毕之后根据设置删除/保留创建的 pod。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubesphere-devops-3-0.png" alt="">&lt;/p></description></item><item><title>KubeSphere 在互联网电商行业的应用实践</title><link>https://openksc.github.io/zh/blogs/kubesphere-best-practices-in-the-internet-ecommerce-industry/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-best-practices-in-the-internet-ecommerce-industry/</guid><description>&lt;h2 id="背景">背景&lt;/h2>
&lt;p>在云原生的时代背景下，Kubernetes 已经成为了主流选择。然而，Kubernetes 的原生操作复杂性和学习曲线较高，往往让很多团队在使用和管理上遇到挑战。因此，市面上出现了许多对 Kubernetes 进行封装和优化的工具，其中 KubeSphere 是一个集成了多种开源工具、提供全方位解决方案的企业级容器管理平台。&lt;/p>
&lt;p>我们团队早期在持续集成和持续交付（CI/CD）、微服务治理、多租户管理和 DevOps 自动化方面就遇到了不少挑战，如 Jenkins 的权限管理短板、开发调试交互困难、多集群管理分散增加运维成本等。为了提升开发效率和运维质量，我们决定评估和引入一个适合的 Kubernetes 平台工具。经过调研和比较，我们最终选择了 KubeSphere。&lt;/p>
&lt;h2 id="选型说明">选型说明&lt;/h2>
&lt;p>我们选择了 KubeSphere 3.3.2 版本，在选型过程中，我们主要考量了以下几个因素：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>易用性&lt;/strong>：平台是否提供直观的用户界面和简单易懂的操作流程，能够降低学习成本。&lt;/li>
&lt;li>&lt;strong>功能完备性&lt;/strong>：平台是否集成了 CI/CD、监控、日志管理、微服务治理等功能，能够满足我们现有和未来的需求。&lt;/li>
&lt;li>&lt;strong>扩展性&lt;/strong>：平台是否支持插件机制，能够方便地集成第三方工具和服务。&lt;/li>
&lt;li>&lt;strong>社区活跃度&lt;/strong>：平台的社区是否活跃，是否有及时的技术支持和丰富的文档资源。&lt;/li>
&lt;/ul>
&lt;p>在对比了几款主流的 Kubernetes 平台工具后，我们发现 KubeSphere 在上述几个方面表现得都非常出色。尤其是在易用性和功能完备性方面，KubeSphere 提供了用户友好的界面和全方位的功能集成，能够显著降低我们的运维难度和学习成本。&lt;/p>
&lt;h2 id="实践过程">实践过程&lt;/h2>
&lt;h3 id="基础设施与部署架构">基础设施与部署架构&lt;/h3>
&lt;p>我们使用的是公有云环境，划分了 3 个 VPC，分别为非生产网络、生产网络，以及运维网络，其中非生产和生产网络隔离，而运维网络与其他网络打通，我们在运维网络中部署了运维中控平台用于管理这 3 个 VPC 的资源，KubeSphere 则借助其联邦模式，以运维网络的集群为主，其余网络的集群为成员，实现多集群管理，为了方便运维中控平台的管理，服务器推行了标准化，保障规格和配置的一致性，统一采用 32 核 128G 的配置，操作系统为 Centos7.9。&lt;/p>
&lt;p>KubeSphere 集群节点界面截图：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/ks-20240625-1.png" alt="">&lt;/p>
&lt;p>部署架构示意图：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/ks-20240625-2.png" alt="">&lt;/p>
&lt;h3 id="相关业务与-kubesphere-的契合点">相关业务与 KubeSphere 的契合点&lt;/h3>
&lt;p>在选择和使用 KubeSphere 的过程中，我们发现它与我们现有的业务需求有着高度的契合，具体表现在以下几个方面：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>持续集成与持续交付（CI/CD）&lt;/strong>：我们的开发流程高度依赖于 CI/CD 流水线，以确保代码的快速构建、测试和部署。我们采用的是 Jenkins pipeline 的技术方案，恰好 KubeSphere 也集成了 Jenkins，支持 pipeline 的流水线发布，使我们能够快速迁移和集成现有的 CI/CD 流程。&lt;/p></description></item><item><title>KubeSphere 在互联网医疗行业的应用实践</title><link>https://openksc.github.io/zh/blogs/kubesphere-practice-of-internet-healthcare-industry/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-practice-of-internet-healthcare-industry/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>2020 年我国互联网医疗企业迎来了“爆发元年”，互联网医疗企业的迅速发展的同时，也暴露出更多的不足。互联网医疗作为医疗行业发展的趋势，对于解决中国医疗资源分配不平衡和人们日益增长的医疗健康需求之间的矛盾具有诸多意义。但对于能否切实解决居民就诊的问题，以及企业能否实现持续发展等是国家以及企业十分关注的问题。而我司在这条道路上沉淀多年，一直致力于互联网医疗服务，拥有自己完善医疗产品平台以及技术体系。&lt;/p>
&lt;h2 id="项目简介">项目简介&lt;/h2>
&lt;h3 id="建设目标">建设目标&lt;/h3>
&lt;p>第三方客户业务环境均是 IDC 自建机房环境，提供虚拟化服务器资源，计划引入 Kubernetes 技术，满足互联网医疗需求。&lt;/p>
&lt;h3 id="技术现状">技术现状&lt;/h3>
&lt;p>据悉，第三方客户已有的架构体系已不满足日益增长的业务量，缺少一个完整且灵活的技术架构体系。&lt;/p>
&lt;h3 id="平台架构图">平台架构图&lt;/h3>
&lt;h4 id="线上平面逻辑架构图参考">线上平面逻辑架构图参考&lt;/h4>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/20230914001.png" alt="">&lt;/p>
&lt;p>上图便是我们项目生产企业架构图，从逻辑上分为四大版块。&lt;/p>
&lt;h4 id="devops-cicd-平台">DevOps CI/CD 平台&lt;/h4>
&lt;p>关于 CI/CD 自动化开源工具相信大家都了解不少，就我个人而言，我所熟知的就有 Jenkins、GitLab、Spug 以及我接下来将会为大家介绍的 KubeSphere。它同样也能完成企业级 CI/CD 持续交付事宜。&lt;/p>
&lt;h4 id="kubernetes-集群">Kubernetes 集群&lt;/h4>
&lt;p>因业务需要，这里将测试、生产两套环境独立开，避免相互影响。如上图所示是三个 Matsre 节点，五个 Node 节点，这里 Master 节点标注污点使其 Pod 不可调度，避免主节点负载过高等情况发生。另外测试环境集群规模相对较小，Master 节点数量相同，但 Node 节点仅只有两个作为使用，因仅作测试，没有问题。&lt;/p>
&lt;h4 id="底层存储环境">底层存储环境&lt;/h4>
&lt;p>底层存储环境我们并未采用容器化的方式进行部署，而是以传统的方式部署。这样做也是为了高效，而且在互联网业务中，存储服务都有一定的性能要求来应对高并发场景。因此将其部署在裸机服务器上是最佳的选择。MySQL、Redis、NFS 均做了高可用，避免了单点问题，NFS 在这里是作为 KubeSphere StorageClass 存储类。关于 StorageClass 存储类选型还有很多，比如 Ceph、OpenEBS 等等，它们都是 KubeSphere 能接入的开源底层存储类解决方案。尤其是 Ceph，得到了很多互联网大厂的青睐，此时你们可能会问我，为什么选择 NFS 而不选择 Ceph，我只能说，在工具类选型中，只有最合适的，没有最好的，适合你的业务类型你就选择什么，而不是人云亦云，哪个工具热度高而去选择哪个工具。&lt;/p>
&lt;h4 id="分布式监控平台">分布式监控平台&lt;/h4>
&lt;p>一个完整的互联网应用平台自然是少不了监控告警了。在过去几年，我们所熟知的 Nagios、Zabbix、Cacti 这几款都是老牌监控了，现如今都渐渐退出历史的舞台。如今 Prometheus 脱颖而出，深受各大互联网企业青睐，结合 Grafana，不得不说是真的香。在该架构体系中，我也是毫不犹豫的选择了它。&lt;/p>
&lt;h2 id="背景介绍">背景介绍&lt;/h2>
&lt;p>客户现有平台环境缺少完整的技术架构体系，业务版本更新迭代困难，无论是业务还是技术平台都出现较为严重的瓶颈问题，不足以支撑现有的业务体系。为了避免导致用户流失，需要重新制定完整的架构体系。而如今，互联网技术不断更新迭代，随着 Kubernetes 日益盛行，KubeSphere 也应运而生。一个技术的兴起必定会能带动整个技术生态圈的发展，我相信，KubeSphere 的出现，能带给我们远不止你想象的价值和便捷。&lt;/p></description></item><item><title>KubeSphere 助力提升研发效能的应用实践分享</title><link>https://openksc.github.io/zh/blogs/best-practices-kubesphere-assist-in-improving-the-efficiency-of-rd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/best-practices-kubesphere-assist-in-improving-the-efficiency-of-rd/</guid><description>&lt;blockquote>
&lt;p>作者：卢运强，主要从事 Java、Python 和 Golang 相关的开发工作。热爱学习和使用新技术；有着十分强烈的代码洁癖；喜欢重构代码，善于分析和解决问题。&lt;a href="https://lucumt.info/post/devops/share-kubepshere-using-experience-for-current-company/" target="_blank" rel="noopener noreferrer">原文链接&lt;/a>。&lt;/p>&lt;/blockquote>
&lt;p>我司从 2022 年 6 月开始使用 KubeSphere，到目前为止快一年时间，简要记录下此过程中的经验积累，供大家参考。&lt;/p>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>公司当前有接近 3000 人的规模，主要业务为汽车配套相关的软硬件开发，其中专门从事软件开发约有 800 人，这其中 Java 开发的约占 70%，余下的为 C/C++ 嵌入式和 C# 桌面程序的开发。&lt;/p>
&lt;p>在 Java 开发部分，约 80% 的都是 Java EE 开发，由于公司的业务主要是给外部客户提供软硬件产品和咨询服务，在早期公司和部门更关注的是如何将产品销售给更多的客户、获得更多的订单和尽快回款，对软件开发流程这块没有过多的重视，故早期在软件开发部分不是特别规范化。软件开发基于项目主要采用&lt;a href="https://zh.wikipedia.org/zh-hans/%E6%95%8F%E6%8D%B7%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91" target="_blank" rel="noopener noreferrer">敏捷开发&lt;/a>或&lt;a href="https://zh.wikipedia.org/wiki/%E7%80%91%E5%B8%83%E6%A8%A1%E5%9E%8B" target="_blank" rel="noopener noreferrer">瀑布模型&lt;/a>，而对于软件部署和运维依旧采用的纯手工方式。&lt;/p>
&lt;p>随着公司规模的扩大与软件产品线的增多，上述方式逐渐暴露出一些问题：&lt;/p>
&lt;ul>
&lt;li>存在大量重复性工作，在软件快速迭代时，需要频繁的手工编译部署，耗费时间，且此过程缺乏日志记录，后续无法追踪审计；&lt;/li>
&lt;li>缺乏审核功能，对于测试环境和生产环境的操作需要审批流程，之前通过邮件和企业微信无法串联；&lt;/li>
&lt;li>缺乏准入功能，随着团队规模扩大，人员素质参差不齐，需要对软件开发流程、代码风格都需要强制固化；&lt;/li>
&lt;li>缺乏监控功能，后续不同团队、项目采用的监控方案不统一，不利于知识的积累；&lt;/li>
&lt;li>不同客户的定制化功能太多（logo，字体，IP 地址，业务逻辑等），采用手工打包的方式效率低，容易遗漏出错。&lt;/li>
&lt;/ul>
&lt;p>在竞争日益激烈的市场环境下，&lt;strong>公司需要把有限的人力资源优先用于业务迭代开发&lt;/strong>，解决上述问题变得愈发迫切。&lt;/p>
&lt;h2 id="选型说明">选型说明&lt;/h2>
&lt;p>基于前述原因，部门准备选用网络上开源的系统来尽可能的解决上述痛点，在技术选型时有如下考量点：&lt;/p>
&lt;ul>
&lt;li>采用尽量少的系统，最好一套系统能解决前述所有问题，避免多个系统维护和整合的成本；&lt;/li>
&lt;li>采用开源版本，避免公司内部手工开发，节约人力；&lt;/li>
&lt;li>安装过程简洁，不需要复杂的操作，能支持离线安装；&lt;/li>
&lt;li>文档丰富、社区活跃、使用人员较多，遇到问题能较容易的找到答案；&lt;/li>
&lt;li>支持容器化部署，公司和部门的业务中自动驾驶和云仿真相关的越来越多，此部分对算力和资源提出了更高的要求。&lt;/li>
&lt;/ul>
&lt;p>我们最开始采用的是 &lt;a href="https://www.jenkins.io/" target="_blank" rel="noopener noreferrer">Jenkins&lt;/a>，通过 Jenkins 基本上能解决我们 90% 的问题，但依旧有如下问题影使用体验:&lt;/p>
&lt;ul>
&lt;li>对于云原生支持不太好，不利于部门后续云仿真相关的业务使用；&lt;/li>
&lt;li>UI 界面简陋，交互方式不友好(项目构建日志输出等)；&lt;/li>
&lt;li>对于项目，资源的权限分配与隔离过于简陋，不满足多项目多部门使用时细粒度的区分要求。&lt;/li>
&lt;/ul>
&lt;p>在网络上查找后发现类似的工具有很多，经过初步对比筛选后倾向于 KubeSphere、Zadig 这 2 款产品，它们的基本功能都类似，进一步对比如下：&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>&lt;/th>
 &lt;th style="text-align: center">KubeSphere&lt;/th>
 &lt;th style="text-align: center">Zadig&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>&lt;strong>云原生支持&lt;/strong>&lt;/td>
 &lt;td style="text-align: center">&lt;strong>高&lt;/strong>&lt;/td>
 &lt;td style="text-align: center">一般&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>UI 美观度&lt;/strong>&lt;/td>
 &lt;td style="text-align: center">&lt;strong>高&lt;/strong>&lt;/td>
 &lt;td style="text-align: center">一般&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>GitHub Star&lt;/strong>&lt;/td>
 &lt;td style="text-align: center">&lt;strong>12.4k&lt;/strong>&lt;/td>
 &lt;td style="text-align: center">2k&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>社区活跃度&lt;/strong>&lt;/td>
 &lt;td style="text-align: center">&lt;strong>高&lt;/strong>&lt;/td>
 &lt;td style="text-align: center">一般&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>经过对比，KubeSphere 较为符合我们的需求，尤其是 KubeSphere 的 UI 界面十分美观，故最终选定 KubeSphere 作为部门内部的持续集成与容器化管理系统！&lt;/p></description></item><item><title>MySQL on K8s：开源开放的 MySQL 高可用容器编排方案</title><link>https://openksc.github.io/zh/blogs/mysql-on-k8s/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/mysql-on-k8s/</guid><description>&lt;blockquote>
&lt;p>本文是上海站 Meetup 讲师高日耀根据其分享内容整理的文章。&lt;a href="https://kubesphere.com.cn/live/mysql-shanghai/" target="_blank" rel="noopener noreferrer">点击查看视频回放&lt;/a>&lt;/p>&lt;/blockquote>
&lt;h2 id="mysql-运维有哪些挑战">MySQL 运维有哪些挑战？&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/radondb-1.jpg" alt="">&lt;/p>
&lt;p>传统的物理部署方式，即把数据库部署在物理机上。对运维人员而言，会遇到图 1 中四个维度的挑战。&lt;/p>
&lt;h3 id="1成本">1、成本&lt;/h3>
&lt;p>自建 MySQL 数据库集群，需要的硬件设备包括：&lt;strong>服务器、网络交换机、内存、CPU、硬盘等&lt;/strong> 硬件设备。&lt;/p>
&lt;p>硬件设备成本包括：&lt;strong>选型、购买、维护、升级、损坏、数据丢失等&lt;/strong>。&lt;/p>
&lt;h3 id="2传统部署">2、传统部署&lt;/h3>
&lt;p>每新增一套集群，都要进行操作系统安装、环境配置、MySQL 数据库安装、调试、性能优化、调参，后续还有系统升级，数据库升级…&lt;/p>
&lt;h3 id="3运维">3、运维&lt;/h3>
&lt;p>针对不同对场景，比如一源一副本，一源两副本，多源多副本（MGR）等，需要编写对应的运维脚本。&lt;/p>
&lt;p>在集群规模不大，MySQL 实例不多的情况下，运维人员尚且能应付，但是当集群不断增加，MySQL 实例达到成千上万个的时候，会极大的增加运维人员的负担，效率也会变得低下。&lt;/p>
&lt;p>规模越大，出现误操作的概率会越高，严重的甚至要 &lt;strong>“从删库跑路”&lt;/strong>。误删除关键数据，对于一般企业而言，应对能力较弱，危害很可能是致命的。&lt;/p>
&lt;h3 id="4资源弹性">4、资源弹性&lt;/h3>
&lt;p>传统物理机部署不具备秒级弹性的能力，来针对 MySQL 在高峰或低谷时资源的自动弹性伸缩。例如，在业务高峰扩展 CPU、内存等资源，在低峰期收回闲置资源。&lt;/p>
&lt;p>如果设计一次电商秒杀的场景持续时间是 30 分钟，那么我们可以在 30 分钟内，将网络、CPU、内存、磁盘等资源提升到最大。在秒杀结束之后，释放这些资源，极大节省成本。&lt;/p>
&lt;h2 id="主流解决方案">主流解决方案&lt;/h2>
&lt;p>针对如上挑战，主流的解决方案有两种：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>物理机 + 管理平台&lt;/p>
&lt;p>通过数据库管理平台，对数据库的统一管理，减轻数据库运维成本，提高数据库整体的可用性。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>上云&lt;/p>
&lt;p>将数据库部署到一个虚拟计算环境中，也就是常说的数据库上云。市面上各大云厂商都提供了 RDS 服务。&lt;/p>
&lt;p>数据库上云可以实现按需付费、按需扩展、高可用性以及存储整合等优势。大大降低运维人员大规模部署和运维数据库的难度。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>那么，还有没有其它方案来解决这些挑战呢？接下来为大家介绍数据库容器化。&lt;/p>
&lt;h2 id="为什么要做数据库容器化">为什么要做数据库容器化？&lt;/h2>
&lt;p>据 CNCF 云原生产业联盟发布的&lt;a href="https://www.cncf.io/blog/2021/04/28/cncf-cloud-native-survey-china-2020/" target="_blank" rel="noopener noreferrer">《中国云原生用户调查报告2020年》&lt;/a>显示，60% 以上的中国企业已在生产环境中应用容器技术，其中 43% 的企业已将容器技术用于核心生产业务。&lt;/p>
&lt;h3 id="容器技术">容器技术&lt;/h3>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/radondb-2.jpg" alt="">&lt;/p>
&lt;h3 id="docker-横空出世">Docker 横空出世&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>相当轻量镜像标准化制作&lt;/p>
&lt;p>使安装部署和交付非常高效。解决了 PaaS 服务打包复杂，环境不一致等问题。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>轻量级虚拟化(rootfs/cgroup/namespace)&lt;/p>
&lt;p>有助于资源共享，降级性能损耗&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="kubernetes-容器编排的事实标准">Kubernetes 容器编排的事实标准&lt;/h3>
&lt;p>依托着 Google Borg 项目的理论优势，继承了 Google 的大规模生产环境的经验。K8s 提供了一套基于容器构建分布式系统的基础依赖。K8s 也成为容器编排的事实标准。&lt;/p></description></item><item><title>NebulaGraph 的云产品交付实践</title><link>https://openksc.github.io/zh/blogs/nebulagraph-cloud-service/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/nebulagraph-cloud-service/</guid><description>&lt;h2 id="nebulagraph-介绍">NebulaGraph 介绍&lt;/h2>
&lt;p>NebulaGraph 是由杭州悦数科技有限公司自主研发的一款开源分布式图数据库产品，擅长处理千亿节点万亿条边的超大数据集，同时保持毫秒级查询延时。得益于其 shared-nothing 以及存储与计算分离的架构设计，NebulaGraph 具备在线水平扩缩容能力；原生分布式架构，使用 Raft 协议保证数据一致性，确保集群高可用；同时兼容 openCypher，能够无缝对接 Neo4j 用户，降低学习及迁移成本。&lt;/p>
&lt;p>NebulaGraph 经过几年的发展，目前已经形成由云服务、可视化工具、图计算、大数据生态支持、工程相关的 Chaos 以及性能压测等产品构成的生态，接下来会围绕云服务展开，分享落地过程中的实践经验。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202209141554143.png" alt="">&lt;/p>
&lt;h2 id="交付模式">交付模式&lt;/h2>
&lt;p>NebulaGraph 在云上的交付模式分为自管模式、半托管模式与全托管模式三种。&lt;/p>
&lt;h3 id="自管模式">自管模式&lt;/h3>
&lt;p>自管模式基于各家云厂商的的资源堆栈编排产品交付，例如 AWS Cloudformation、Azure ResourceManager、Aliyun Resource Orchestration Service、GCP DeploymentManager 等等。自管模式的特点是所有资源部署在客户的租户内，用户自己运维管理，软件服务商负责将产品上架到 Marketplace，按照最佳实践给客户提供服务配置组装和一键部署的能力，相比于传统模式下以天计的交付周期，现在几分钟内就可以在云上部署一个图数据库。&lt;/p>
&lt;h3 id="半托管模式">半托管模式&lt;/h3>
&lt;p>半托管模式是在自管模式的基础上为客户提供了代运维的能力，阿里云计算巢通过将应用发布为服务的方式，为服务商提供了一个智能简捷的服务发布和管理平台，覆盖了服务的整个生命周期，包括服务的交付、部署、运维等。当客户的集群出现问题时，服务商运维人员的所有操作均被记录，资源操作通过 ActionTrail 记录日志，实例操作保留录屏，还原运维过程，做到运维安全合规可追溯，避免服务纠纷。&lt;/p>
&lt;p>NebulaGraph 采用存储与计算分离的架构。存储计算分离有诸多优势，最直接的优势就是，计算层和存储层可以根据各自的情况弹性扩容、缩容。存储计算分离还带来了另一个优势：使水平扩展成为可能，通过计算巢提供的弹性伸缩能力，保障自身扩缩容需要。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202209141554802.png" alt="">&lt;/p>
&lt;h3 id="全托管模式">全托管模式&lt;/h3>
&lt;p>全托管模式交付由服务商托管的图数据库产品，客户按需订阅付费，只需选择产品规格与节点，NebulaGraph 全栈产品便可在几分钟内交付。客户无需关注底层资源的监控运维，数据库集群的稳定性保障工作，这些都将由服务商解决。&lt;/p>
&lt;p>NebulaGraph DBaaS 依托于 Kubernetes 构建，Kubernetes 的架构设计带来以下优势：通过声明式 API 将整体运维复杂性下沉，交给 IaaS 层实现和持续优化；抽象出 Loadbalance Service、Ingress、NodePool、PVC 等对象，帮助应用层可以更好通过业务语义使用基础设施，无需关注底层实现差异；通过 CRD（Custom Resource Definition）/ Operator 等方法提供领域相关的扩展实现，最大化 Kubernetes 的应用价值。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202209141554811.png" alt="">&lt;/p>
&lt;h2 id="落地实践">落地实践&lt;/h2>
&lt;p>落地实践主要讲述全托管模式产品的架构演进，云原生技术与业务平台的融合。&lt;/p>
&lt;h3 id="iac">IaC&lt;/h3>
&lt;p>下图是 Azure 业务侧基础设施的架构，初始配置时对接到管理平台需要耗时几个小时，这在有大量用户申请订阅实例的情况下是完全不能接受的。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202209141554654.png" alt="">&lt;/p>
&lt;p>因此，我们想到了将基础设施模板化，先定义出 dev、test、prod 三种运行环境，再将资源划分为 VPC &amp;amp; Peering、Private DNS Zone、Kubernetes、Database、Container Registry、Bastion 等几个类别，使用 terraform 完成自动化配置。但是，仅完成这一步是远远不够的，为了满足客户侧 Kubernetes 集群及时弹性要求，我们定义了 Cluster CRD，将 Cluster 的所有操作放入 Operator 里执行，terraform 的可执行文件与模板代码打包到容器镜像后由 Job 驱动运行，Operator 向 Job 注入云厂商、地域、子网等环境变量，业务集群的状态保存到 Cluster Status 里。到此，配置基础设施实现了手动向自动化的演进。&lt;/p></description></item><item><title>NetApp 存储在 KubeSphere 上的实践</title><link>https://openksc.github.io/zh/blogs/netapp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/netapp/</guid><description>&lt;p>&lt;a href="https://www.netapp.com/cn/index.aspx" target="_blank" rel="noopener noreferrer">NetApp&lt;/a> 是向目前的数据密集型企业提供统一存储解决方案的居世界最前列的公司，其 Data ONTAP是全球首屈一指的存储操作系统。NetApp 的存储解决方案涵盖了专业化的硬件、软件和服务，为开放网络环境提供了无缝的存储管理。
&lt;strong>Ontap&lt;/strong>数据管理软件支持高速闪存、低成本旋转介质和基于云的对象存储等存储配置，为通过块或文件访问协议读写数据的应用程序提供统一存储。
&lt;strong>Trident&lt;/strong>是一个由NetApp维护的完全支持的开源项目。以帮助您满足容器化应用程序的复杂&lt;strong>持久性&lt;/strong>需求。
&lt;a href="https://github.com/kubesphere" target="_blank" rel="noopener noreferrer">KubeSphere&lt;/a> 是一款开源项目，在目前主流容器调度平台 Kubernetes 之上构建的企业级分布式多租户&lt;strong>容器管理平台&lt;/strong>，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大降低开发、测试、运维的日常工作的复杂度。&lt;/p>
&lt;h2 id="整体方案">整体方案&lt;/h2>
&lt;p>在 VMware Workstation 环境下安装 ONTAP; ONTAP 系统上创建 SVM(Storage Virtual Machine) 且对接 nfs 协议；在已有 k8s 环境下部署 Trident,Trident 将使用 ONTAP 系统上提供的信息（svm、managementLIF 和 dataLIF）作为后端来提供卷；在已创建的 k8s 和StorageClass 卷下部署 KubeSphere。&lt;/p>
&lt;h2 id="版本信息">版本信息&lt;/h2>
&lt;ul>
&lt;li>Ontap: 9.5&lt;/li>
&lt;li>Trident: v19.07&lt;/li>
&lt;li>k8s: 1.15&lt;/li>
&lt;li>kubesphere: 2.0.2&lt;/li>
&lt;/ul>
&lt;h2 id="步骤">步骤&lt;/h2>
&lt;p>主要描述ontap搭建及配置、Trident搭建和配置和kubesphere搭建及配置等方面。&lt;/p>
&lt;h2 id="ontap-搭建及配置">OnTap 搭建及配置&lt;/h2>
&lt;p>在 VMware Workstation 上 &lt;code>Simulate_ONTAP_9.5P6_Installation_and_Setup_Guide&lt;/code> 运行，Ontap 启动之后，按下面操作配置，其中以 &lt;code>cluster base license&lt;/code>、&lt;code>feature licenses for the non-ESX build&lt;/code> 配置证书、e0c、ip address：&lt;code>192.168.*.20、netmask:255.255.255.0&lt;/code>、&lt;strong>集群名: cluster1、密码&lt;/strong>等信息。&lt;/p>
&lt;p>&lt;code>https://IP address&lt;/code>,&lt;/p></description></item><item><title>OpenFunction 应用系列之一: 以 Serverless 的方式实现 Kubernetes 日志告警</title><link>https://openksc.github.io/zh/blogs/serverless-way-for-kubernetes-log-alert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/serverless-way-for-kubernetes-log-alert/</guid><description>&lt;h2 id="概述">概述&lt;/h2>
&lt;p>当我们将容器的日志收集到消息服务器之后，我们该如何处理这些日志？部署一个专用的日志处理工作负载可能会耗费多余的成本，而当日志体量骤增、骤降时亦难以评估日志处理工作负载的待机数量。本文提供了一种基于 Serverless 的日志处理思路，可以在降低该任务链路成本的同时提高其灵活性。&lt;/p>
&lt;p>我们的大体设计是使用 Kafka 服务器作为日志的接收器，之后以输入 Kafka 服务器的日志作为事件，驱动 Serverless 工作负载对日志进行处理。据此的大致步骤为：&lt;/p>
&lt;ol>
&lt;li>搭建 Kafka 服务器作为 Kubernetes 集群的日志接收器&lt;/li>
&lt;li>部署 OpenFunction 为日志处理工作负载提供 Serverless 能力&lt;/li>
&lt;li>编写日志处理函数，抓取特定的日志生成告警消息&lt;/li>
&lt;li>配置 &lt;a href="https://github.com/kubesphere/notification-manager/" target="_blank" rel="noopener noreferrer">Notification Manager&lt;/a> 将告警发送至 Slack&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202108261124546.png" alt="">&lt;/p>
&lt;p>在这个场景中，我们会利用到 &lt;a href="https://github.com/OpenFunction/OpenFunction" target="_blank" rel="noopener noreferrer">OpenFunction&lt;/a> 带来的 Serverless 能力。&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://github.com/OpenFunction/OpenFunction" target="_blank" rel="noopener noreferrer">OpenFunction&lt;/a> 是 KubeSphere 社区开源的一个 FaaS（Serverless）项目，旨在让用户专注于他们的业务逻辑，而不必关心底层运行环境和基础设施。该项目当前具备以下关键能力：&lt;/p>
&lt;ul>
&lt;li>支持通过 dockerfile 或 buildpacks 方式构建 OCI 镜像&lt;/li>
&lt;li>支持使用 Knative Serving 或 OpenFunctionAsync ( KEDA + Dapr ) 作为 runtime 运行 Serverless 工作负载&lt;/li>
&lt;li>自带事件驱动框架&lt;/li>
&lt;/ul>&lt;/blockquote>
&lt;h2 id="使用-kafka-作为日志接收器">使用 Kafka 作为日志接收器&lt;/h2>
&lt;p>首先，我们为 KubeSphere 平台开启 &lt;strong>logging&lt;/strong> 组件（可以参考 &lt;a href="https://kubesphere.io/zh/docs/pluggable-components/" target="_blank" rel="noopener noreferrer">启用可插拔组件&lt;/a> 获取更多信息）。然后我们使用 &lt;a href="https://github.com/strimzi/strimzi-kafka-operator" target="_blank" rel="noopener noreferrer">strimzi-kafka-operator&lt;/a> 搭建一个最小化的 Kafka 服务器。&lt;/p></description></item><item><title>OpenFunction：从 0 到 1，打造新一代开源函数计算平台</title><link>https://openksc.github.io/zh/blogs/faas-openfunction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/faas-openfunction/</guid><description>&lt;p>&lt;strong>无服务器计算&lt;/strong>，即通常所说的 Serverless，已经成为当前云原生领域炙手可热的名词，是继 IaaS，PaaS 之后云计算发展的下一波浪潮。Serverless 强调的是一种架构思想和服务模型，让开发者无需关心基础设施（服务器等），而是专注到应用程序业务逻辑上。加州大学伯克利分校在论文 A Berkeley View on Serverless Computing 中给出了两个关于 Serverless 的核心观点：&lt;/p>
&lt;ul>
&lt;li>有服务的计算并不会消失，但随着 Serverless 的成熟，有服务计算的重要性会逐渐降低。&lt;/li>
&lt;li>Serverless 最终会成为云时代的计算范式，它能够在很大程度上替代有服务的计算模式，并给 Client-Server 时代划上句号。&lt;/li>
&lt;/ul>
&lt;p>那么什么是 Serverless 呢？&lt;/p>
&lt;h2 id="serverless-介绍">Serverless 介绍&lt;/h2>
&lt;p>关于什么是 Serverless，加州大学伯克利分校在之前提到的论文中也给出了明确定义：&lt;code>Serverless computing = FaaS + BaaS&lt;/code>。云服务按抽象程度从底层到上层传统的分类是硬件、云平台基本组件、PaaS、应用，但 PaaS 层的理想状态是具备 Serverless 的能力，因此这里我们将 PaaS 层替换成了 Serverless，即下图中的黄色部分。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202112011620107.png" alt="">&lt;/p>
&lt;p>Serverless 包含两个组成部分 &lt;strong>BaaS&lt;/strong> 和 &lt;strong>FaaS&lt;/strong>，其中对象存储、关系型数据库以及 MQ 等云上基础支撑服务属于 BaaS（后端即服务），这些都是每个云都必备的基础服务，FaaS（函数即服务）才是 Serverless 的核心。&lt;/p>
&lt;h2 id="现有开源-serverless-平台分析">现有开源 Serverless 平台分析&lt;/h2>
&lt;p>KubeSphere 社区从 2020 年下半年开始对 Serverless 领域进行深度调研。经过一段时间的调研后，我们发现：&lt;/p>
&lt;ul>
&lt;li>现有开源 FaaS 项目绝大多数启动较早，大部分都在 Knative 出现前就已经存在了；&lt;/li>
&lt;li>Knative 是一个非常杰出的 Serverless 平台，但是 Knative Serving 仅仅能运行应用，不能运行函数，还不能称之为 FaaS 平台；&lt;/li>
&lt;li>Knative Eventing 也是非常优秀的事件管理框架，但是设计有些过于复杂，用户用起来有一定门槛；&lt;/li>
&lt;li>OpenFaaS 是比较流行的 FaaS 项目，但是技术栈有点老旧，依赖于 Prometheus 和 Alertmanager 进行 Autoscaling，在云原生领域并非最专业和敏捷的做法；&lt;/li>
&lt;li>近年来云原生 Serverless 相关领域陆续涌现出了很多优秀的开源项目如 &lt;a href="https://keda.sh/" target="_blank" rel="noopener noreferrer">KEDA&lt;/a>、 &lt;a href="https://dapr.io/" target="_blank" rel="noopener noreferrer">Dapr&lt;/a>、 &lt;a href="https://buildpacks.io/" target="_blank" rel="noopener noreferrer">Cloud Native Buildpacks（CNB）&lt;/a>、 &lt;a href="https://tekton.dev/" target="_blank" rel="noopener noreferrer">Tekton&lt;/a>、 &lt;a href="https://shipwright.io/" target="_blank" rel="noopener noreferrer">Shipwright&lt;/a> 等，为创建新一代开源 FaaS 平台打下了基础。&lt;/li>
&lt;/ul>
&lt;p>综上所述，我们调研的结论就是：&lt;strong>现有开源 Serverless 或 FaaS 平台并不能满足构建现代云原生 FaaS 平台的要求，而云原生 Serverless 领域的最新进展却为构建新一代 FaaS 平台提供了可能。&lt;/strong>&lt;/p></description></item><item><title>OpenPitrix Insight</title><link>https://openksc.github.io/zh/blogs/openpitrix-insight/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/openpitrix-insight/</guid><description>&lt;p>云计算在今天已经被绝大多数的企业所采用，具知名云服务厂商 &lt;a href="https://www.rightscale.com/blog/cloud-industry-insights/cloud-computing-trends-2018-state-cloud-survey" target="_blank" rel="noopener noreferrer">RightScale 最近的调查&lt;/a>显示，已经有越来越多的厂商采用多云管理。客户有太多的理由来选择多云管理了，其中最大的原因莫过于采用单一的供应商，会导致被锁定。因此，如何管理多云环境，并在多云的环境下进行自动化，正成为众多企业的刚需，而在这其中，应用程序的管理显得尤为的重要。进一步讲，颇具挑战的是创建一个一站式的应用管理平台，来管理不同类型的应用程序，其中包括传统的应用（或者称之为单体应用，或者传统的主从、分片、peer-to-peer 架构的企业分布式应用）、微服务应用、以及近来发展迅猛的 Serverless 应用等，OpenPitrix 就是为了解决这些问题而生的。用一句话来描述 OpenPitrix：&lt;/p>
&lt;blockquote>
&lt;p>OpenPitrix 是一款开源项目，用来在多云环境下打包、部署和管理不同类型的应用，包括传统应用、微服务应用以及 Serverless 应用等，其中云平台包括 AWS、Azure、Kubernetes、QingCloud、OpenStack、VMWare 等。&lt;/p>&lt;/blockquote>
&lt;p>微服务，即众所周知的微服务架构，这是程序设计的必然趋势，企业创建新的应用时选择的主要方式。另外，开源项目 Kubernetes 已经成为事实上的&lt;a href="https://www.cncf.io/blog/2017/06/28/survey-shows-kubernetes-leading-orchestration-platform/" target="_blank" rel="noopener noreferrer">编排平台的领导者&lt;/a>，其在自动化部署、扩展性、以及管理容器化的应用有着独特的优势。但是，仍然有大量的传统遗留应用用户想在毋须改变其架构的情况下迁入到云平台中，而且对很多用户来讲，采用微服务架构，或者是 Serverless 架构还是比较遥远的事情，所以，我们需要帮助这些用户将他们的传统应用迁入到云计算平台中，这也是 OpenPitrix 很重要的一个功能。&lt;/p>
&lt;p>在2017年3月27日，QingCloud 发布 &lt;a href="https://appcenter.qingcloud.com/" target="_blank" rel="noopener noreferrer">AppCenter&lt;/a>，一款旨在为传统企业应用开发商和云用户之间架设友好桥梁的平台，该平台最大的亮点在于其可以让开发者以极低的学习成本就可以将传统的应用程序移植到 QingCloud 中运行，并且具有云计算的所有特性，如敏捷性、伸缩性、稳定性、监控等。通常，一位开发者只需花上几个小时就可以理解整个工作流程，然后，再花一到两周的时间(这具体要取决于应用的复杂性)将应用移植到云平台中。该平台上线之后一直颇受用户的青睐和夸赞，但有一些用户提出更多的需求，希望将之部署到他们内部来管理他们的多云环境。为了满足用户的需求，QingCloud 将之扩展，即在多云的环境下管理多种类型的应用程序，并且采用开源的方法来进行项目的良性发展。&lt;/p>
&lt;p>俗语有云：&amp;quot;知易行难&amp;quot;，尽管 OpenPitrix 原始团队在云计算应用开发有着足够丰富的经验，并成功的开发出了稳定的商业化产品：AppCenter，要知道，等待在前方的依然有很多困难要克服。&lt;a href="https://github.com/openpitrix/openpitrix" target="_blank" rel="noopener noreferrer">OpenPitrix&lt;/a> 从一开始就是以开源的方式来进行，并且在2017年的8月份在 GitHub 上创建了组织和项目，一直到2018年2月24日才写下第一行功能代码，在此期间，团队的所有成员都在思考系统的每个关键点，这些讨论的细节均可在 GitHub 上&lt;a href="https://github.com/openpitrix/openpitrix/issues" target="_blank" rel="noopener noreferrer">公开访问&lt;/a>。&lt;/p>
&lt;p>以上便是 OpenPitrix 项目的来龙去脉介绍，接下来会解释一些详细的功能和设计细节。&lt;/p>
&lt;h2 id="主要的功能">主要的功能&lt;/h2>
&lt;p>OpenPitrix 所希望实现的功能包括以下内容：&lt;/p>
&lt;ul>
&lt;li>支持多个云平台，如 AWS、Azure、Kubernetes、QingCloud、OpenStack、VMWare 等等;&lt;/li>
&lt;li>云平台的支持是高度可扩展和插拔的;&lt;/li>
&lt;li>支持多种应用程序的类型：传统应用、微服务应用、Serverless 应用;&lt;/li>
&lt;li>应用程序的支持也是高度可扩展的，这也就意味着无论将来出现哪种新的应用程序类型，OpenPitrix 平台都可以通过添加相应的插件来支持它;&lt;/li>
&lt;li>应用程序的仓库是可配置的，这也就意味着由 OpenPitrix 所驱动的商店，其应用均是可以用来交易的;&lt;/li>
&lt;li>应用程序库的可见性是可配置的，包括公开、私有或仅让某特定的一组用户可访问，由 OpenPitrix 所驱动的市场，每个供应商都能够操作属于她/他自己的应用商店。&lt;/li>
&lt;/ul>
&lt;h2 id="用户场景实例">用户场景实例&lt;/h2>
&lt;p>OpenPitrix 典型的用户场景有：&lt;/p>
&lt;ul>
&lt;li>某企业是采用了多云的系统（包括混合云），要实现一站式的应用管理平台，从而实现应用的部署和管理；&lt;/li>
&lt;li>云管平台（CMP）可以将 OpenPitrix 视为其其中一个组件，以实现在多云环境下管理应用；&lt;/li>
&lt;li>可以作为 Kubernetes 的一个应用管理系统。OpenPitrix 和 Helm 有着本质上的不同，虽然 OpenPitrix 底层用了 Helm 来部署 Kubernetes 应用，但 OpenPitrix 着眼于应用的全生命周期管理，比如在企业中，通常会按照应用的状态来分类，如开发、测试、预览、生产等；甚至有些组织还会按照部门来归类，而这是 Helm 所没有的。&lt;/li>
&lt;/ul>
&lt;h2 id="架构概览">架构概览&lt;/h2>
&lt;p>OpenPitrix 设计的最根本的思想就是解耦应用和应用运行时环境（此处使用运行时环境代替云平台，下同），如下图所示。应用程序能够运行在哪个环境，除了需要匹配 provider 信息之外，还需要匹配应用所在仓库的选择器 (selector) 和运行时环境的标签 (label)，即当某个最终用户从商店里选择了某个具体的应用，然后尝试部署它时，系统会自动选择运行时环境。如果有多个运行时环境可以运行此应用的话，则系统会弹出相应的对话框来让用户自行选择，更多设计细节请参考 &lt;a href="https://github.com/openpitrix/openpitrix/tree/master/docs/design" target="_blank" rel="noopener noreferrer">OpenPitrix 设计文档&lt;/a>。&lt;/p></description></item><item><title>Prometheus 长期存储主流方案对比</title><link>https://openksc.github.io/zh/blogs/prometheus-storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/prometheus-storage/</guid><description>&lt;p>Prometheus 作为云原生时代崛起的标志性项目，已经成为可观测领域的事实标准。Prometheus 是单实例不可扩展的，那么如果用户需要采集更多的数据并且保存更长时间该选择怎样的长期存储方案呢？&lt;/p>
&lt;p>2022 年 8 月 9 日，在 CSDN 云原生系列在线峰会第 15 期“Prometheus 峰会”上，青云科技可观测与函数计算负责⼈霍秉杰分享了《Prometheus Long-Term Storage：海纳百川，有容乃大》。&lt;/p>
&lt;h2 id="prometheus-简介及其局限性">Prometheus 简介及其局限性&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/prometheus-storage-1.jpg" alt="">&lt;/p>
&lt;p>云原生时代崛起的 Prometheus 已经在可观测领域得到了广泛应用，其影响力远远超出了云原生的范畴，具有两个显著特点。&lt;/p>
&lt;h3 id="单实例不可扩展">单实例，不可扩展&lt;/h3>
&lt;p>Prometheus 的作者及社区核心开发者都秉承一个理念：Prometheus 只聚焦核心的功能，扩展性的功能留给社区解决，所以 Prometheus 自诞生至今都是单实例不可扩展的。&lt;/p>
&lt;p>这对于很多从大数据时代走过来的工程师而言有点不可思议，大数据领域的很多开源项目比如 Elasticsearch、HBase、Cassandra 等无一不是多节点多角色的设计。&lt;/p>
&lt;p>Prometheus 的核心开发者曾这样解释，Prometheus 结合 Go 语言的特性和优势，使得 Prometheus 能够以更小的代价抓取并存储更多数据，而 Elasticsearch 或 Cassandra 等 Java 实现的大数据项目处理同样的数据量会消耗更多的资源。也就是说，单实例、不可扩展的 Prometheus 已强大到可以满足大部分用户的需求。&lt;/p>
&lt;h3 id="pull-模式抓取数据">Pull 模式抓取数据&lt;/h3>
&lt;p>Prometheus 倡导用 Pull 模式获取数据，即 Prometheus 主动地去数据源拉取数据。对于不便于 Pull 的数据源，Prometheus 提供了 PushGateway 进行处理，但 PushGateway 在部分应用场景上存在限制。&lt;/p>
&lt;p>尽管单实例的 Prometheus 已经足够强大，但还是存在部分需求是其无法满足的，如跨集群聚合、更长时间的存储等。为了扩展 Prometheus，社区给出了多种方案。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/prometheus-storage-2.jpg" alt="">&lt;/p>
&lt;p>在 Prometheus 长期存储出现之前，用户若需要跨集群聚合计算数据时，社区提供 Federation 方式实现。&lt;/p></description></item><item><title>T3 出行云原生容器化平台实践</title><link>https://openksc.github.io/zh/blogs/t3-kubesphere-best-practice/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/t3-kubesphere-best-practice/</guid><description>&lt;h2 id="公司简介">公司简介&lt;/h2>
&lt;p>T3 出行是南京领行科技股份有限公司打造的智慧出行生态平台，由中国第一汽车集团有限公司、东风汽车集团有限公司、重庆长安汽车股份有限公司发起，联合腾讯、阿里巴巴等互联网企业共同投资打造。公司以“成为最值得信赖的出行服务企业”为品牌愿景，“科技引领，愉悦出行”为使命，倡导“可信，更自由”的出行理念，致力为用户提供“可信、安全、品质”出行服务，让用户感受更加自由的出行体验。&lt;/p>
&lt;h2 id="背景介绍">背景介绍&lt;/h2>
&lt;p>随着 T3 出行业务体量持续上涨，服务的稳定性需要系统化的保障。容器化改造将提供标准化的环境，基于应用运行环境实现完整的版本控制，消除开发到生产的环境差异，保证应用生命周期内环境一致性和标准化。同时容器化环境可以让服务共享计算资源，并通过混部方式来提高整体计算资源的利用率，降低企业应用的基础设施运营成本。&lt;/p>
&lt;p>容器化之前 T3 出行是传统的虚拟机模式，所有业务都部署在虚拟机上，全体产研通过堡垒机、传统的监控系统、日志平台等进行日常应用的运维。而一旦服务容器化开始，我们必然需要一个云原生的容器化管理平台，让 T3 出行全体产研从传统的虚拟机操作模式转变为云原生操作模式。同时，之前日常的应用运维模式需要使用多个平台进行协作，产研定位一个应用性能问题往往需要来回切换多个平台。所以我们希望容器化平台可以集成周边的配套，如日志查看、监控系统，让产研尽量在一个平台内完成日常运维的工作；也可以作为平台工程的一部分，让产研在开发环境可以拥有足够的权限创建、更新、删除非基线环境，而无需了解底层架构知识，通过自助化的环境能力可以让研发并行开发测试，最终让业务可以快速、高效增长。&lt;/p>
&lt;h2 id="选型说明">选型说明&lt;/h2>
&lt;p>我们的选型思路基本上是根据功能、UI 体验、社区活跃度、学习成本这 4 点来的。首先必须要满足我们对容器平台的需求（在背景介绍中已经描述），其次是社区活跃度以及生态，最后是 UI 体验，在 UI 体验中包含了用户的学习成本，我们希望以低学习成本的方式让 T3 出行的研发能够快速上手容器平台，同时也具备运维视角，如此就既满足了研发的应用视角维度，也满足了运维的集群视角维度。&lt;/p>
&lt;p>我们在选型期间对比了 Rancher、OpenShift、KubeSphere，最终选择了 KubeSphere 作为 T3 出行云原生容器平台。KubeSphere 定位是以应用为中心的容器平台，提供简单易用的操作界面，帮助用户屏蔽掉那些技术细节，一定程度上降低了学习成本。同时 Kubesphere 具备优秀的容器管理能力、多集群支持能力、多租户能力、天然集成的可观测能力等，让我们可以在一个平台上满足了日常运维所需。&lt;/p>
&lt;h2 id="实践过程">实践过程&lt;/h2>
&lt;h3 id="多集群统一管理">多集群统一管理&lt;/h3>
&lt;p>KubeSphere 多集群中角色分为主集群和成员集群，由 1 个主集群和多个成员集群组成，与我们原先的集群规划不谋而合。主集群我们作为成员集群的控制面存在，通过主集群下发不同的管理策略给到成员集群。对于成员集群而言，我们根据不同的环境、不同的租户性质也会划分到不同集群。如根据环境区分，我们会有开发集群、测试集群、预生产集群、生产集群；而根据租户性质，我们会有一些对接三方业务的集群。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/image-t3-1-new.png" alt="">&lt;/p>
&lt;h3 id="项目管理">项目管理&lt;/h3>
&lt;p>在很多传统的 DevOps 平台中，并没有与项目进行联动，服务往往只是关联了组织架构，当组织架构变动，服务的元信息就不准确了。而且，对于一个项目来说，经常会有跨部门合作的情况，而业务发布的视角却是在各自的部门下的，项目成员无法在一个视图下看到项目的所有业务，在项目的协作过程中自然就增加了许多沟通成本。而 KubeSphere 就是基于项目维度对容器服务进行管理的，在 KubeSphere 中一个项目对应的就是 Kubernetes 一个 Namespace，租户之间的视图是隔离的，日常只需要在自己的项目视图下进行协作即可。&lt;/p>
&lt;p>我们的 DevOps 平台正在逐步的往项目集成方向发展，目前我们是按照业务域进行统一管理，一个业务域代表了一个 KubeSphere 中的一个项目，业务域下会有多个相关的业务服务，无论组织架构如何变换，业务域始终不变。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/image-t3-2.png" alt="">&lt;/p>
&lt;h3 id="多租户管理">多租户管理&lt;/h3>
&lt;p>KubeSphere 中的多租户管理是基于企业空间维度来完成的，企业空间是用来管理项目、DevOps 项目、应用模板和应用仓库的一种逻辑单元。我们可以在企业空间中控制资源访问权限，也可以安全地在团队内部分享资源。企业空间可以关联多个集群中的多个项目，并对企业空间中的成员进行权限管理，引用 KubeSphere 官方配图便于大家直观的理解：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/image-t3-3.png" alt="">&lt;/p>
&lt;p>我们是以业务域为项目维度进行统一管理，那么企业空间作为 KubeSphere 最小的租户管理单元自然是被我们按照业务域进行划分。所以 T3 当下的多租户管理逻辑就是：企业空间（业务域） -&amp;gt; 集群（开发、测试等）-&amp;gt; 项目（业务域）。同时在企业空间中，我们也抽象出了部门管理维度，使用 KubeSphere 的部门管理，便于我们给不同的人员赋予不同集群（环境）操作权限。&lt;/p></description></item><item><title>本来生活的 DevOps 升级之路</title><link>https://openksc.github.io/zh/blogs/benlai-devops/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/benlai-devops/</guid><description>&lt;p>我叫杨杨，就职于本来生活网（Benlai.com），负责发布系统架构。我们公司咋说呢，简单说就是卖水果、蔬菜的😄，下面还是来一段官方介绍。&lt;/p>
&lt;h2 id="本来生活简介">本来生活简介&lt;/h2>
&lt;p>本来生活网创办于 2012 年，是一个专注于食品、水果、蔬菜的电商网站，从优质食品供应基地、供应商中精挑细选，剔除中间环节，提供冷链配送、食材食品直送到家服务。致力于通过保障食品安全、提供冷链宅配、基地直送来改善中国食品安全现状，成为中国优质食品提供者。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200327131801.png" alt="">&lt;/p>
&lt;h2 id="技术现状">技术现状&lt;/h2>
&lt;h3 id="基础设施">基础设施&lt;/h3>
&lt;blockquote>
&lt;ul>
&lt;li>部署在 IDC 机房&lt;/li>
&lt;li>拥有 100 多台物理机&lt;/li>
&lt;li>虚拟化部署&lt;/li>
&lt;/ul>&lt;/blockquote>
&lt;h3 id="存在的问题">存在的问题&lt;/h3>
&lt;blockquote>
&lt;ul>
&lt;li>物理机 95% 以上的占用率&lt;/li>
&lt;li>相当多的资源闲置&lt;/li>
&lt;li>应用扩容比较慢&lt;/li>
&lt;/ul>&lt;/blockquote>
&lt;h2 id="拥抱-devops-与-kubernetes">拥抱 DevOps 与 Kubernetes&lt;/h2>
&lt;p>公司走上容器平台的 DevOps 这条康庄大道主要目标有三：&lt;/p>
&lt;blockquote>
&lt;p>1、提高资源利用率&lt;/p>
&lt;p>2、提高发布效率&lt;/p>
&lt;p>3、降低运维的工作成本等等&lt;/p>&lt;/blockquote>
&lt;p>其实最主要的还是 &lt;strong>省钱&lt;/strong>，对就是 &lt;strong>省钱&lt;/strong>。接下来就是介绍我们本来生活的 DevOps 升级之路：&lt;/p>
&lt;h2 id="level-1工具选型">Level 1：工具选型&lt;/h2>
&lt;p>我们从初步接触 DevOps 相关知识，在此期间偶然了解到开源的 KubeSphere (kubesphere.io)。KubeSphere 是在 Kubernetes 之上构建的以应用为中心的企业级容器平台，支持敏捷开发与自动化运维、DevOps、微服务治理、灰度发布、多租户管理、监控告警、日志查询与收集、应用商店、存储管理、网络管理等多种业务场景。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200327141156.png" alt="">&lt;/p>
&lt;p>KubeSphere 内置的基于 Jenkins 的 DevOps 流水线非常适合我们，并且还打通了我们日常运维开发中需要的云原生工具生态，这个平台正是我们当初希望自己开发实现的。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200327132943.png" alt="">&lt;/p>
&lt;p>于是，我们开始学习 KubeSphere 与 Jenkins 的各种操作、语法、插件等，开始构建适合我们自己的 CI/CD 的整个流程。最终结合 KubeSphere 容器平台，初步实现了第一级的 CI/CD 流程。&lt;/p>
&lt;p>在 &lt;strong>Level 1&lt;/strong> 的流程中，我们主要实现了拉取代码、编译应用、发布镜像到本地仓库、部署到本地 Kubernetes 集群；如下图&lt;/p></description></item><item><title>超越预期：containerd 如何成为 Kubernetes 的首选容器运行时</title><link>https://openksc.github.io/zh/blogs/containerd-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/containerd-kubernetes/</guid><description>&lt;h2 id="踏上-containerd-技术之旅">踏上 containerd 技术之旅&lt;/h2>
&lt;p>容器技术已经成为现代软件开发和部署的核心工具。通过容器，开发者可以创建轻量级、便携的运行环境，从而简化应用程序的开发、测试和部署流程。在容器技术的生态系统中，容器运行时扮演着至关重要的角色。本篇文章将探讨低级和高级容器运行时的区别，并解释为什么 Kubernetes 选择 containerd 作为其默认的容器运行时。此外，我们还将介绍三种与 containerd 相关的 CLI 工具：ctr、crictl 和 nerdctl。&lt;/p>
&lt;h2 id="containerd-架构及说明">containerd 架构及说明&lt;/h2>
&lt;p>containerd 是一个高效、可靠的开源容器运行时，它被设计为从开发到生产环境的核心容器管理解决方案。containerd 的架构主要分为三个部分：生态系统（Ecosystem）、平台（Platform）和客户端（Client）。每个部分在整个系统中扮演着不同的角色，协同工作以提供全面的容器管理功能。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/3db600ce38aa46a6cbb4e09b681c95d4.png" alt="">&lt;/p>
&lt;h3 id="生态系统ecosystem">生态系统（Ecosystem）&lt;/h3>
&lt;p>containerd 的生态系统包括一系列与其集成的工具和组件，这些工具和组件扩展了 containerd 的功能并增强了其适用性。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>CRI 插件&lt;/strong>：与 Kubernetes 紧密集成，通过实现 Container Runtime Interface (CRI)，使 Kubernetes 能够管理容器。&lt;/li>
&lt;li>&lt;strong>CNI 插件&lt;/strong>：使用 Container Network Interface (CNI)插件进行网络管理，提供容器的网络连接。&lt;/li>
&lt;li>&lt;strong>CSI 插件&lt;/strong>：Container Storage Interface (CSI)插件用于存储管理，允许容器挂载和管理存储卷。&lt;/li>
&lt;li>&lt;strong>镜像管理&lt;/strong>：支持 Docker 镜像和 OCI 镜像规范，提供从镜像仓库拉取、存储和管理容器镜像的能力。&lt;/li>
&lt;li>&lt;strong>插件机制&lt;/strong>：允许通过插件扩展 containerd 的功能，满足特定的需求。&lt;/li>
&lt;/ul>
&lt;h3 id="平台platform">平台（Platform）&lt;/h3>
&lt;p>containerd 的平台层是整个系统的核心，负责管理和调度容器运行时的所有基本操作。这个层次的主要组件包括：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>守护进程&lt;/strong>：containerd 守护进程负责处理所有的容器管理请求，并维护容器的生命周期。&lt;/li>
&lt;li>&lt;strong>gRPC API&lt;/strong>：通过 gRPC API 与外部客户端通信，提供标准化的接口以执行容器操作。&lt;/li>
&lt;li>&lt;strong>任务管理&lt;/strong>：管理容器的创建、启动、停止和删除任务，确保容器按照预期运行。&lt;/li>
&lt;li>&lt;strong>快照管理&lt;/strong>：使用快照（Snapshot）机制管理容器文件系统，实现高效的存储操作。&lt;/li>
&lt;li>&lt;strong>事件监控&lt;/strong>：实时监控容器事件，提供日志记录和事件通知功能，便于运维人员进行故障排查和系统监控。&lt;/li>
&lt;/ul>
&lt;h3 id="客户端client">客户端（Client）&lt;/h3>
&lt;p>containerd 的客户端提供用户与 containerd 平台交互的方式。主要的客户端工具包括：&lt;/p></description></item><item><title>从 KubeSphere 3.1.0 边缘节点的监控问题排查，简要解析边缘监控原理</title><link>https://openksc.github.io/zh/blogs/edge-node-monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/edge-node-monitoring/</guid><description>&lt;p>KubeSphere 3.1.0 通过集成 KubeEdge，将节点和资源的管理延伸到了边缘，也是 KubeSphere 正式支持边缘计算的第一个版本。&lt;/p>
&lt;p>本文作者也第一时间搭建和试用了边缘节点相关的功能，但是在边缘节点纳管之后遇到了一些监控的小问题，在排查过程中也顺带了解了一下 KubeSphere 对于边缘节点的监控原理，发出来和大家分享，方便其他的开发者能够更快的排查问题或进行二次开发。&lt;/p>
&lt;h2 id="环境版本和构成">环境版本和构成&lt;/h2>
&lt;p>通过 KubeKey 安装，参数如下，其余组件版本默认未改动。&lt;/p>
&lt;p>Kubernetes: v1.19.8&lt;/p>
&lt;p>KubeSphere: v3.1.0&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/1621237867-89115-image.png" alt="">&lt;/p>
&lt;h2 id="问题现象">问题现象&lt;/h2>
&lt;p>通过生成的 keadm 命令行将边缘节点加入集群，并在边缘节点上部署 POD，该 POD 的监控信息不能显示。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/1621238378-230370-image.png" alt="">&lt;/p>
&lt;h2 id="监控原理">监控原理&lt;/h2>
&lt;p>定位和解决问题之前，肯定是要先搞懂工作原理。&lt;/p>
&lt;h3 id="1-kubeedge">1. KubeEdge&lt;/h3>
&lt;p>KubeEdge 的 Edgecore 组件对 Kubelet 进行了轻量化改造，Edgecore 和 Cloudcore（云端）也不在同一个 Cluster 网络中，通过 K8s 默认的方式进行 metrics 获取肯定是行不通的（logs 和 exec 原理相同）。&lt;/p>
&lt;p>当前 KubeEdge 的实现方法是 kube-apiserver 上的 iptables 转发给云端的 Cloudcore，Cloudcore 通过和 Edgecore 之间的 WebSocket 通道向边缘端进行消息和数据传递。&lt;/p>
&lt;p>KubeEdge 官方的使用手册和文档如下： &lt;a href="https://kubeedge.io/en/docs/advanced/metrics/" target="_blank" rel="noopener noreferrer">https://kubeedge.io/en/docs/advanced/metrics/&lt;/a>。&lt;/p>
&lt;p>为了便于大家理解，作者画了一张图，整体的流程请参考如下：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/1621239902-395149-image.png" alt="">&lt;/p>
&lt;h3 id="2-metrics-server">2. Metrics-server&lt;/h3>
&lt;p>原生的 K8S 中就是通过 Metrics-server 这个官方组件进行节点和 POD 的 CPU/Memory 等数据的监控。&lt;/p></description></item><item><title>对于 Serverless, DevOps, 多云及边缘可观测性的思考与实践</title><link>https://openksc.github.io/zh/blogs/serverless-devops-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/serverless-devops-kubesphere/</guid><description>&lt;p>从单体到微服务，再到 Serverless，应用在逐渐地轻量化。有人可能会有疑问，微服务都还没有顺畅的搭建起来，现在又出了 Serverless，应该怎么办？&lt;/p>
&lt;p>其实两者之间并不是一个相互替代的关系。我们可以看到，现在有单体应用在跑，也有微服务的应用在跑，那么以后 Serverless 这种应用也会有一定的用途。而微服务里也可以调用 Serverless 的函数（function）。&lt;/p>
&lt;h2 id="serverless-定义">Serverless 定义&lt;/h2>
&lt;p>Serverless 类似于之前大数据的概念，大家都知道，但不是特别确切了解其中的含义。&lt;/p>
&lt;p>CNCF 有一个 Serverless 工作组，在 Serverless 的白皮书中进行了定义。简单来讲就是，Serverless 等于 FaaS (Function-as-a-Service) 加上 BaaS (Backend-as-a-Service)。我们可以将应用程序以函数的形式打包，然后上传到 FaaS 平台。这样我们就不用关心底层服务器的运维，因为它能自动扩展，而且是按需收费。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/severless-2.png" alt="">&lt;/p>
&lt;p>Serverless 有一个很重要的特征，就是事件驱动模式，即依赖于外部事件去触发它运行。&lt;/p>
&lt;p>至于 Backend-as-a-Service，你可以理解成支撑 Serverless 运行的一些 API 的服务，比如对象存储或者是数据库服务。&lt;/p>
&lt;h2 id="serverless-处理模型">Serverless 处理模型&lt;/h2>
&lt;p>下图是 Serverless 的处理模型。上文讲到，它的模式是事件触发，所以它会有事件源产生事件，然后 FaaS 的 controller 是根据这些事件去启动 function 的 instance，而且能够实时的去扩展，能够 scale 到 0，这样动态的去给用户提供服务。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/serverless-20.png" alt="">&lt;/p>
&lt;h2 id="函数生命周期">函数生命周期&lt;/h2>
&lt;p>函数（function）是有生命周期的，也就是函数部署流水线的流程。首先提供 function 的源码和 spec，然后把源码 build 成 docker image，再把 docker image 部署到 FaaS 平台上。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/serverless-12.png" alt="">&lt;/p>
&lt;p>涉及到函数，肯定会有函数的创建、更新、发布等操作，因篇幅关系，这里就不再赘述了。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/serverless-4.png" alt="">&lt;/p>
&lt;h2 id="serverless-应用场景">Serverless 应用场景&lt;/h2>
&lt;h3 id="对-iot-设备数据进行分析">对 IoT 设备数据进行分析&lt;/h3>
&lt;p>目前边缘计算，还有 IoT 的应用都在逐渐地兴起。 比如 IoT 中的一个传感器，它会源源不断的产生一些数据（例如传感器温度超过了 60 度），我们可以调用一个云端的函数来处理这件事情（发个通知或者做一个应用）。所以 Serverless 和 IoT 这个场景可以很好地结合起来。&lt;/p></description></item><item><title>负载均衡器 OpenELB ARP 欺骗技术解析</title><link>https://openksc.github.io/zh/blogs/openelb-arp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/openelb-arp/</guid><description>&lt;blockquote>
&lt;p>作者：大飞哥，视源电子运维工程师，KubeSphere 用户委员会广州站站长，KubeSphere Ambassador。&lt;/p>&lt;/blockquote>
&lt;p>K8s 对集群外暴露服务有三种方式：NodePort，Ingress 和 Loadbalancer。NodePort 用于暴露 TCP 服务（4 层），但限于对集群节点主机端口的占用，不适合大规模使用；Ingress 用于暴露 HTTP 服务(7 层)，可对域名地址做路由分发；Loadbalancer 则专属于云服务，可动态分配公网网关。&lt;/p>
&lt;p>对于私有云集群，没有用到公有云服务，能否使用 LoadBalancer 对外暴露服务呢？&lt;/p>
&lt;p>答案当然是肯定的，OpenELB 正是为裸金属服务器提供 LoadBalancer 服务而生的！&lt;/p>
&lt;h2 id="应用安装与配置">应用安装与配置&lt;/h2>
&lt;h3 id="安装-openelb">安装 OpenELB&lt;/h3>
&lt;blockquote>
&lt;p>参考&lt;a href="https://openelb.io/docs/getting-started/installation/install-openelb-on-kubernetes/" target="_blank" rel="noopener noreferrer">官方安装文档&lt;/a>&lt;/p>&lt;/blockquote>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ kubectl apply -f https://raw.githubusercontent.com/openelb/openelb/master/deploy/openelb.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="添加-eip-池">添加 EIP 池&lt;/h3>
&lt;blockquote>
&lt;p>EIP 地址要与集群主机节点在同一网段内，且不可绑定任何网卡；&lt;/p>&lt;/blockquote>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">network.kubesphere.io/v1alpha2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Eip&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">eip-sample-pool&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">annotations&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">eip.openelb.kubesphere.io/is-default-eip&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">address&lt;/span>: &lt;span style="color:#ae81ff">192.168.0.91-192.168.0.100&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">protocol&lt;/span>: &lt;span style="color:#ae81ff">layer2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">interface&lt;/span>: &lt;span style="color:#ae81ff">eth0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">disable&lt;/span>: &lt;span style="color:#66d9ef">false&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="配置-service-为-loadbalancer">配置 Service 为 LoadBalancer&lt;/h3>
&lt;p>把 Service 类型修改为 LoadBalancer，同时 annotations 中添加如下三行：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">lb.kubesphere.io/v1alpha1&lt;/span>: &lt;span style="color:#ae81ff">openelb&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">protocol.openelb.kubesphere.io/v1alpha1&lt;/span>: &lt;span style="color:#ae81ff">layer2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">eip.openelb.kubesphere.io/v1alpha2&lt;/span>: &lt;span style="color:#ae81ff">layer2-eip&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>总体配置清单如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">layer2-svc&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">annotations&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">lb.kubesphere.io/v1alpha1&lt;/span>: &lt;span style="color:#ae81ff">openelb&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">protocol.openelb.kubesphere.io/v1alpha1&lt;/span>: &lt;span style="color:#ae81ff">layer2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">eip.openelb.kubesphere.io/v1alpha2&lt;/span>: &lt;span style="color:#ae81ff">layer2-eip&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">selector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">layer2-openelb&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">LoadBalancer&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">http&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">port&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">targetPort&lt;/span>: &lt;span style="color:#ae81ff">8080&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">externalTrafficPolicy&lt;/span>: &lt;span style="color:#ae81ff">Cluster&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="layer2-模式中的黑客技术">Layer2 模式中的黑客技术&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/ff880d68-be25-4def-8cd6-b09a032ae753.webp" alt="">&lt;/p></description></item><item><title>混合云下的 Kubernetes 多集群管理与应用部署</title><link>https://openksc.github.io/zh/blogs/kubernetes-multicluster-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubernetes-multicluster-kubesphere/</guid><description>&lt;blockquote>
&lt;p>本文是上海站 Meetup 中讲师李宇根据其分享内容梳理成的文章。本文介绍了 Kubernetes 社区多集群方向的发展历程以及已有的多集群解决方案，分享在混合云的场景下, KubeSphere 如何基于 Kubefed 统一应用的分发与部署，以达到跨 region 的多活/容灾等目的。同时探讨未来多集群领域可能迈向的去中心化的架构。
&lt;a href="https://kubesphere.com.cn/live/multicluster-shanghai/" target="_blank" rel="noopener noreferrer">视频回放&lt;/a>&lt;/p>&lt;/blockquote>
&lt;p>大家好，很高兴来到今天下午的 Meetup。我先简单做个自我介绍，我叫李宇，目前是 KubeSphere 的一名研发，主要负责多集群方向的工作，我今天带来的分享是混合云下的 Kubernetes 多集群管理与应用部署。&lt;/p>
&lt;p>KubeSphere 在开始做 v3.0 之前，曾发起了一个社区用户调研，发现呼声最高的是支持多集群管理和跨云的应用部署，因此 KubeSphere v3.0 重点支持了多集群管理。&lt;/p>
&lt;h2 id="单集群下的-kubernetes-架构">单集群下的 Kubernetes 架构&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubeadm-HA.png" alt="">&lt;/p>
&lt;p>Kubernetes 内部分为 Master 和 Worker 两个角色。Master 上面有 API Server 负责 API 请求，Controller Manager 负责启动多个 controller，持续协调声明式的 API 从 spec 到 status 的转换过程，Scheduler 则负责 Pod 的调度，Etcd 负责集群数据的存储。Worker 则作为工作节点主要负责 Pod 的启动。&lt;/p>
&lt;p>单集群下有许多场景是无法满足企业需求的，主要分为以下几点：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>物理隔离。尽管 Kubernetes 提供了 ns 级别的隔离，你可以设置每个 Namespace 各自使用的 CPU 内存，甚至可以使用 Network Policy 配置不同 Namespace 的网络连通性，企业仍然需要一个更加彻底的物理隔离环境，以此避免业务之间的互相影响。&lt;/p></description></item><item><title>基于 Argo CD 的 GitOps 实践经验</title><link>https://openksc.github.io/zh/blogs/gitops-argocd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/gitops-argocd/</guid><description>&lt;blockquote>
&lt;p>本文是 2021 年 KubeSphere Meetup 北京站讲师裴振飞分享内容整理而成。点击观看&lt;a href="https://kubesphere.com.cn/live/gitops-cic/" target="_blank" rel="noopener noreferrer">回放视频&lt;/a>&lt;/p>&lt;/blockquote>
&lt;h2 id="行业背景">行业背景&lt;/h2>
&lt;p>业务，开发，运维，由于岗位职能的不同，人数配比也不尽相同，但却有一个共通的趋势：运维人员随业务增长的幅度，大大低于开发人员的增长。对于开发团队，越来越多的公司，把发版效率作为团队的考核目标之一，而团队 DevOps 建设，运维至关重要。&lt;/p>
&lt;p>随着容器技术的发展，Kubernetes 已经是成熟可用于生产实践的生态工具。社区组件的丰富，带来了新的解决方案，硬件资源的利用率也显著提高。但基于云原生的虚拟化，其繁杂的概念和逻辑，让用户“云里雾里”。Kubernetes 好比艘庞大的航空母舰，想要驾驭它绝非人肉运维能够解决。&lt;/p>
&lt;p>选择好的 Kubernetes 控制面板，才可以事半功倍！&lt;/p>
&lt;h2 id="为什么选择-kubesphere">为什么选择 KubeSphere？&lt;/h2>
&lt;p>我们的选型思路，有如下几点：&lt;/p>
&lt;p>1.开源项目&lt;/p>
&lt;p>2.有厂商技术支持&lt;/p>
&lt;p>3.界面对开发和运维友好&lt;/p>
&lt;p>不是所有的开源项目都有厂商在做技术支持，而 KubeSphere 完全符合前两点。开源意味着开放，用起来没有“卡脖子”风险，而厂商的技术支持，可在关键时刻施以援手，扶持企业渡过难关。界面友好这个需求，看似简单没有作用，但实际上却是影响用户体验最重要的因素，界面友好才能释放后端功能。&lt;/p>
&lt;h2 id="实践-cicd-的两个阶段">实践 CI/CD 的两个阶段&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/cic-fei-1.png" alt="">&lt;/p>
&lt;p>基于 KubeSphere 我们实践过两种方案。前一个阶段是 Source To Image，后一个阶段是基于 Argo CD 的 GitOps。&lt;/p>
&lt;h2 id="s2i-核心概念介绍">S2I 核心概念介绍&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/cic-fei-2.png" alt="">&lt;/p>
&lt;p>S2I 是个构建 Docker 镜像的工具，它是 OpenShift 上主要的构建镜像的方式之一。S2I 有个核心理念：镜像就是源代码加运行环境。源代码可能千变万化，但同一语言的运行环境，基本差异不大。Docker 镜像又是分层存储，非常适合 S2I 的操作。&lt;/p>
&lt;p>基础镜像加运行环境，我们称为 S2I 模板镜像，它是一个生成镜像的镜像。如何用一个镜像生成另一个镜像，这就是 Assemble 脚本，与此对应的，还有另外的 Run 脚本，它定义了生成出来的镜像是如何运行的。&lt;/p>
&lt;p>所以，如何自定义 S2I 模板镜像，是玩转 S2I 构建的关键。&lt;/p>
&lt;h2 id="快速定制开发-s2i-模板">快速定制开发 S2I 模板&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/cic-fei-3.png" alt="">&lt;/p></description></item><item><title>基于 CoreDNS 和 K8s 构建云原生场景下的企业级 DNS</title><link>https://openksc.github.io/zh/blogs/coredns-k8s-dns/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/coredns-k8s-dns/</guid><description>&lt;p>容器作为近些年最火热的后端技术，加快了很多企业的数字化转型进程。目前的企业，不是在使用云原生技术，就是在转向云原生技术的过程中。在容器化进程中，如何保持业务的平稳迁移，如何将现有的一些服务设施一并进行容器化迁移，也是众多企业较为关注的点。&lt;/p>
&lt;p>以 DNS 为例，如何构建一个云原生的企业 DNS 系统？&lt;/p>
&lt;h2 id="coredns-简介">CoreDNS 简介&lt;/h2>
&lt;p>CoreDNS 是一个 Go 语言编写的灵活可扩展的 DNS 服务器，在 Kubernetes 中，作为一个服务发现的配置中心，在 Kubernetes 中创建的 Service 和 Pod 都会在其中自动生成相应的 DNS 记录。Kubernetes 服务发现的特性，使 CoreDNS 很适合作为企业云原生环境的 DNS 服务器，保障企业容器化和非容器化业务服务的稳定运行。&lt;/p>
&lt;p>构建企业 DNS 服务器时，一般会有以下需求：&lt;/p>
&lt;ul>
&lt;li>用户外网域名访问服务；&lt;/li>
&lt;li>混合云业务迁移、数据共享、容灾；&lt;/li>
&lt;li>开发代码 IP 写死导致架构可用性、弹性无法实现；&lt;/li>
&lt;li>统一 DNS 管理需求，含上下级平台对接；&lt;/li>
&lt;li>DNS 劫持等网络安全风险；&lt;/li>
&lt;li>存量代码固定域名访问；&lt;/li>
&lt;li>集群外域名访问；&lt;/li>
&lt;/ul>
&lt;p>相比于 Bind 开源方案或 Windows Server DNS 商业 DNS 服务器，CoreDNS 有以下优势：&lt;/p>
&lt;ul>
&lt;li>无商业许可要求，降低投资成本；&lt;/li>
&lt;li>轻量化，通过插件实现功能添加；&lt;/li>
&lt;li>支持 DNS，DNS over TLS，DNS over HTTP/2，DNS over gRPC 协议；&lt;/li>
&lt;li>提供 kubernetes 服务发现；&lt;/li>
&lt;li>支持对接 Route53/Google Cloud DNS/AzureDNS；&lt;/li>
&lt;li>支持集成 Prometheus， OpenTracing，OPA，带来更全面的运维体验；&lt;/li>
&lt;li>支持整合容器管理平台，提供统一 DNS 系统运维。&lt;/li>
&lt;/ul>
&lt;p>构建企业云原生 DNS 前，对 CoreDNS 做一个更深入的了解。&lt;/p></description></item><item><title>基于 JuiceFS 的 KubeSphere DevOps 项目数据迁移方案</title><link>https://openksc.github.io/zh/blogs/kubesphere-data-migration-using-juicefs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-data-migration-using-juicefs/</guid><description>&lt;h2 id="方案背景和目的">方案背景和目的&lt;/h2>
&lt;p>KubeSphere 自发布以来已有 2 年之久，从 2.1.0 版本至目前最新版本 3.3。开发人员的编译构建都基于环境平台的 DevOps 功能（底层是 jenkins）实现，如果 DevOps 项目较多产生的流水线记录数据也会比较多，记录的数据存储方式默认是基于 Openebs 去做的，存储介质依赖于 ECS 宿主机 local 本地磁盘的风险是比较大的考虑到宿主机硬盘的不可靠性，随时会发生宕机导致流水线记录数据丢失造成严重影响。本教程经过本地研发平台测试通过了使用开源项目 juiceFS-CSI 且后端依托 OSS 作为后端存储实现数据迁移的检验。&lt;/p>
&lt;h3 id="前提条件">前提条件：&lt;/h3>
&lt;ol>
&lt;li>已经安装好 KubeSphere 平台（本教程使用 KubeSphere 3.2.1、K8s 版本 1.21.5）。安装方式请参考官网：&lt;a href="https://kubesphere.com.cn/docs/v3.3/" target="_blank" rel="noopener noreferrer">https://kubesphere.com.cn/docs/v3.3/&lt;/a>&lt;/li>
&lt;li>已经安装好 juiceFS-CSI 插件并且挂载好 OSS 后端、确认创建 PVC 时通过 SC 自动创建 PV 并绑定。
安装方式请参考官网 : &lt;a href="https://www.juicefs.com/docs/zh/community/introduction/" target="_blank" rel="noopener noreferrer">https://www.juicefs.com/docs/zh/community/introduction/&lt;/a>
&lt;strong>（以上两者缺一不可）&lt;/strong>&lt;/li>
&lt;/ol>
&lt;h3 id="方案实施过程">方案实施过程&lt;/h3>
&lt;h4 id="1-找到-kubesphere-平台的-jenkins-使用的-pv">1. 找到 KubeSphere 平台的 Jenkins 使用的 PV&lt;/h4>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202208041549714.png" alt="">&lt;/p>
&lt;h4 id="2-确认目前-local-磁盘保存的-jenkins-路径在-node1-节点上">2. 确认目前 Local 磁盘保存的 Jenkins 路径（在 node1 节点上）&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>/var/openebs/local/pvc-2143c5a8-9593-4e2a-8eb5-2f3a0c98219a
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="3-下载-juicefs-客户端">3. 下载 JuiceFS 客户端&lt;/h4>
&lt;p>获取最新的版本号。&lt;/p></description></item><item><title>基于 KubeKey 扩容 Kubernetes v1.24 Worker 节点实战</title><link>https://openksc.github.io/zh/blogs/expanding--kubernetes-v1.24--worker-nodes-with-kubekey/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/expanding--kubernetes-v1.24--worker-nodes-with-kubekey/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;h3 id="知识点">&lt;strong>知识点&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>KubeKey 扩容 Worker 节点&lt;/li>
&lt;li>openEuler 操作系统的基本配置&lt;/li>
&lt;li>Kubernets 基本命令&lt;/li>
&lt;/ul>
&lt;h3 id="实战服务器配置架构-11-复刻小规模生产环境配置略有不同">&lt;strong>实战服务器配置(架构 1:1 复刻小规模生产环境，配置略有不同)&lt;/strong>&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-worker-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker/CI&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">storage-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.81&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS/Ceph/Longhorn/NFS/&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">storage-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.82&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS/Ceph/Longhorn&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">storage-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.83&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">100+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS/Ceph/Longhorn&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">registry&lt;/td>
 &lt;td style="text-align: center">192.168.9.80&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">50&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Sonatype Nexus 3&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">10&lt;/td>
 &lt;td style="text-align: center">20&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">500&lt;/td>
 &lt;td style="text-align: center">1100+&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h3 id="实战环境涉及软件版本信息">&lt;strong>实战环境涉及软件版本信息&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>操作系统：&lt;strong>openEuler 22.03 LTS SP2 x86_64&lt;/strong>&lt;/p></description></item><item><title>基于 Kubernetes 部署 node.js APP</title><link>https://openksc.github.io/zh/blogs/nodejs-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/nodejs-app/</guid><description>&lt;h3 id="什么是-kubernetes">什么是 Kubernetes&lt;/h3>
&lt;p>Kubernetes 是一个开源容器编排引擎，可以帮助开发者或运维人员部署和管理容器化的应用，能够轻松完成日常开发运维过程中诸如 滚动更新，横向自动扩容，服务发现，负载均衡等需求。&lt;a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/" target="_blank" rel="noopener noreferrer">了解更多&lt;/a>&lt;/p>
&lt;h3 id="安装-kubernetes">安装 Kubernetes&lt;/h3>
&lt;p>可以通过快速安装 kubernetes 集群：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubesphere/kubesphere" target="_blank" rel="noopener noreferrer">KubeSphere Installer&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/setup/learning-environment/minikube/" target="_blank" rel="noopener noreferrer">minikube&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/" target="_blank" rel="noopener noreferrer">kubeadm&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="kubernetes-术语介绍">Kubernetes 术语介绍&lt;/h3>
&lt;h4 id="pod">&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/" target="_blank" rel="noopener noreferrer">Pod&lt;/a>&lt;/h4>
&lt;p>Pod 是 Kubernetes 最小调度单位，是一个或一组容器的集合。&lt;/p>
&lt;h4 id="deployment">&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank" rel="noopener noreferrer">Deployment&lt;/a>&lt;/h4>
&lt;p>提供对 Pod 的声明式副本控制。指定 Pod 模版，Pod 副本数量, 更新策略等。&lt;/p>
&lt;h4 id="service">&lt;a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/" target="_blank" rel="noopener noreferrer">Service&lt;/a>&lt;/h4>
&lt;p>Service 定义了 Pod 的逻辑分组和一种可以访问它们的策略。借助Service，应用可以方便的实现服务发现与负载均衡。&lt;/p>
&lt;h4 id="label--selector">&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/" target="_blank" rel="noopener noreferrer">Label &amp;amp; Selector&lt;/a>&lt;/h4>
&lt;p>Kubernetes 中使用 Label 去关联各个资源。&lt;/p>
&lt;ol>
&lt;li>通过资源对象(Deployment, etc.)上定义的 Label Selector 来筛选 Pod 数量。&lt;/li>
&lt;li>通过 Service 的 Label Selector 来选择对应的 Pod， 自动建立起每个 Service 到对应 Pod 的请求转发路由表。&lt;/li>
&lt;li>通过对某些 Node 定义特定的 Label，并且在 Pod 中添加 NodeSelector 属性，可以实现 Pod 的定向调度(运行在哪些节点上)。&lt;/li>
&lt;/ol>
&lt;h3 id="nodejs-模板项目">Nodejs 模板项目&lt;/h3>
&lt;p>&lt;a href="https://github.com/gothinkster/node-express-realworld-example-app" target="_blank" rel="noopener noreferrer">node-express-realworld-example-app&lt;/a> 是一款 node.js 编写的后端示例应用。使用 Express 作为网络框架，使用 mongodb 持久化数据，使用 jwt 作用户认证等。&lt;/p></description></item><item><title>基于 Kubernetes 的 CI/CD 利器 — Prow 入门指南</title><link>https://openksc.github.io/zh/blogs/prow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/prow/</guid><description>&lt;p>&lt;strong>Prow&lt;/strong>是k8s使用的CI/CD系统(&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow" target="_blank" rel="noopener noreferrer">https://github.com/kubernetes/test-infra/tree/master/prow&lt;/a>)，用于管理k8s的issue和pr。如果你经常去k8s社区查看pr或者提交过一些Pr后，就会经常看到一个叫&lt;strong>k8s-ci-bot&lt;/strong>的机器人在各个Pr中回复，并且还能合并pr。在k8s-ci-bot中背后工作的就是Prow。Prow是为了弥补github上一些功能上的缺陷，它也是Jenkins-X的一部分，它具备这些功能：&lt;/p>
&lt;ol>
&lt;li>执行各种Job，包括测试，批处理和制品发布等，能够基于github webhook配置job执行的时间和内容。&lt;/li>
&lt;li>一个可插拔的机器人功能（&lt;strong>Tide&lt;/strong>），能够接受&lt;code>/foo&lt;/code>这种样式的指令。&lt;/li>
&lt;li>自动合并Pr&lt;/li>
&lt;li>自带一个网页，能够查看当前任务的执行情况以及Pr的状况，也包括一些帮助信息&lt;/li>
&lt;li>基于OWNER文件在同一个repo里配置模块的负责人&lt;/li>
&lt;li>能够同时处理很多repo的很多pr&lt;/li>
&lt;li>能够导出Prometheus指标&lt;/li>
&lt;/ol>
&lt;p>Prow拥有自己的CI/CD系统，但是也能与我们常见的CI/CD一起协作，所以如果你已经习惯了Jenkins或者travis，都可以使用Prow。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20190930122309.png" alt="">&lt;/p>
&lt;h2 id="安装指南">安装指南&lt;/h2>
&lt;blockquote>
&lt;p>官方repo提供了一个基于GKE快速安装指南，本文将基于青云的Iaas搭建Prow环境。不用担心，其中大部分步骤都是平台无关的，整个安装过程能够很方便的在其他平台上使用。&lt;/p>&lt;/blockquote>
&lt;h3 id="一-准备一个kubernetes集群">一、 准备一个kubernetes集群&lt;/h3>
&lt;p>有以下多种方式准备一个集群&lt;/p>
&lt;ol>
&lt;li>利用kubeadm自建集群&lt;/li>
&lt;li>在青云控制台上点击左侧的容器平台，选择其中的QKE，简单设置一些参数之后，就可以很快创建一个kubernetes集群。&lt;/li>
&lt;li>将集群的kubeconfig复制到本地，请确保在本地运行&lt;code>kubectl cluster-info&lt;/code>正确无误&lt;/li>
&lt;/ol>
&lt;h3 id="二-准备一个github机器人账号">二、 准备一个github机器人账号&lt;/h3>
&lt;blockquote>
&lt;p>如果没有机器人账号，用个人账号也可以。机器人账号便于区分哪些Prow的行为，所以正式使用时应该用机器人账号。&lt;/p>&lt;/blockquote>
&lt;ol>
&lt;li>
&lt;p>在想要用prow管理的仓库中将机器人账号设置为管理员。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在账号设置中添加一个[personal access token][1]，此token需要有以下权限：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>必须&lt;/strong>：&lt;code>public_repo&lt;/code> 和 &lt;code>repo:status&lt;/code>&lt;/li>
&lt;li>&lt;strong>可选&lt;/strong>：&lt;code>repo&lt;/code>假如需要用于一些私有repo&lt;/li>
&lt;li>&lt;strong>可选&lt;/strong>：&lt;code>admin_org:hook&lt;/code> 如果想要用于一个组织&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>将此Token保存在文件中，比如&lt;code>${HOME}/secrets/oauth&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用&lt;code>openssl rand -hex 20&lt;/code>生成一个随机字符串用于验证webhook。将此字符串保存在本地，比如&lt;code>${HOME}/secrets/h-mac&lt;/code>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;em>注意最后两步创建的token一定需要保存好，除了需要上传到k8s，后续配置也要用到，用于双向验证&lt;/em>&lt;/p>
&lt;h3 id="三-配置k8s集群">三、 配置k8s集群&lt;/h3>
&lt;blockquote>
&lt;p>这里使用的default命名空间配置prow，如果需要配置在其他命名空间，需要在相关&lt;code>kubectl&lt;/code>的命令中配置&lt;code>-n&lt;/code>参数，并且在部署的yaml中配置命名空间。
建议将&lt;a href="https://github.com/magicsong/prow-tutorial" target="_blank" rel="noopener noreferrer">本repo&lt;/a>克隆到本地，这个repo带有很多帮助配置Prow的小工具。&lt;/p>&lt;/blockquote>
&lt;ol>
&lt;li>将上一步中创建token和hmac保存在k8s集群中&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># openssl rand -hex 20 &amp;gt; ${HOME}/secrets/h-mac&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl create secret generic hmac-token --from-file&lt;span style="color:#f92672">=&lt;/span>hmac&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>HOME&lt;span style="color:#e6db74">}&lt;/span>/secrets/h-mac
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl create secret generic oauth-token --from-file&lt;span style="color:#f92672">=&lt;/span>oauth&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">${&lt;/span>HOME&lt;span style="color:#e6db74">}&lt;/span>/secrets/oauth
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>部署Prow。由于Prow官方yaml中使用了grc.io镜像，这个镜像在中国大陆无法访问，所以我们将相应的repo搬到了dockerhub上，并提供了一份替换相关镜像名称的&lt;a href="prow.yaml">yaml&lt;/a>，利用下面的命令即可部署Prow（使用的这个repo修改后的yaml）&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/magicsong/prow-tutorial/master/prow.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>使用 &lt;code>kubectl get pod&lt;/code>看到所有Pod都running表示安装已经完成。如下图：&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20190930123654.png" alt="">&lt;/p></description></item><item><title>基于 Kubernetes 的云原生 AI 平台建设</title><link>https://openksc.github.io/zh/blogs/kubernetes-ai/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubernetes-ai/</guid><description>&lt;blockquote>
&lt;p>作者：&lt;/p>
&lt;p>黄河，极视角科技技术合伙人&lt;br />
霍秉杰，KubeSphere 资深架构师，KubeSphere 可观测性及 Serverless 产品负责人，OpenFunction 发起人&lt;/p>&lt;/blockquote>
&lt;h2 id="人工智能与-kubernetes">人工智能与 Kubernetes&lt;/h2>
&lt;p>在国外众多知名网站 2021 年对 Kubernetes 的预测中，人工智能技术与 Kubernetes 的更好结合通常都名列其中。Kubernetes 以其良好的扩展和分布式特性，以及强大的调度能力成为运行 DL/ML 工作负载的理想平台。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/Kubernetes-prediction.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/Prophecis-arch.png" alt="Prophecis 架构图">&lt;/p>
&lt;p>上面是微众银行开源的机器学习平台 Prophecis 的架构图，我们可以看到绿色的部分是机器学习平台通常都会有的功能包括训练、开发、模型、数据和应用的管理等功能。而通常这些机器学习平台都是运行在 Kubernetes 之上的，如紫色的部分所示：最底层是 Kubernetes，再往上是容器管理平台 (微众银行的开发者曾在 KubeSphere 2020 Meetup 北京站上提到这里采用的是 KubeSphere)，容器管理平台在 Kubernetes 之上提供了存储、网络、服务治理、CI/CD、以及可观测性方面的能力。&lt;/p>
&lt;p>Kubernetes 很强大，但通常在 Kubernetes 上运行 AI 的工作负载还需要更多非 K8s 原生能力的支持比如：&lt;/p>
&lt;ul>
&lt;li>用户管理: 涉及多租户权限管理等&lt;/li>
&lt;li>多集群管理&lt;/li>
&lt;li>图形化 GPU 工作负载调度&lt;/li>
&lt;li>GPU 监控&lt;/li>
&lt;li>训练、推理日志管理&lt;/li>
&lt;li>Kubernetes 事件与审计&lt;/li>
&lt;li>告警与通知&lt;/li>
&lt;/ul>
&lt;p>具体来说 Kubernetes 并没有提供完善的用户管理能力，而这是一个企业级机器学习平台的刚需；同样原生的 Kubernetes 也并没有提供多集群管理的能力，而用户有众多 K8s 集群需要统一管理；运行 AI 工作负载需要用到 GPU，昂贵的 GPU 需要有更好的监控及调度才能提高 GPU 利用率并节省成本；AI 的训练需要很长时间才能完成，从几个小时到几天不等，通过容器平台提供的日志系统可以更容易地看到训练进度；容器平台事件管理可以帮助开发者更好地定位问题；容器平台审计管理可以更容易地获知谁对哪些资源做了什么操作，让用户对整个容器平台有深入的掌控。&lt;/p></description></item><item><title>基于 KubeSphere 部署 KubeBlocks 实现数据库自由</title><link>https://openksc.github.io/zh/blogs/deploy-kubeblocks-on-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deploy-kubeblocks-on-kubesphere/</guid><description>&lt;h2 id="kubesphere-是什么">KubeSphere 是什么？&lt;/h2>
&lt;p>KubeSphere 是在 Kubernetes 之上构建的面向云原生应用的分布式操作系统，完全开源，支持多云与多集群管理，提供全栈的 IT 自动化运维能力，简化企业的 DevOps 工作流。它的架构可以非常方便地使第三方应用与云原生生态组件进行即插即用 (plug-and-play) 的集成。作为全栈的多租户容器平台，KubeSphere 提供了运维友好的向导式操作界面，帮助企业快速构建一个强大和功能丰富的容器云平台。KubeSphere 为用户提供构建企业级 Kubernetes 环境所需的多项功能，例如多云与多集群管理、Kubernetes 资源管理、DevOps、应用生命周期管理、微服务治理（服务网格）、日志查询与收集、服务与网络、多租户管理、监控告警、事件与审计查询、存储管理、访问权限控制、GPU 支持、网络策略、镜像仓库管理以及安全管理等。&lt;/p>
&lt;p>&lt;img src="https://kubesphere.io/images/docs/v3.x/zh-cn/introduction/what-is-kubesphere/kubesphere-feature-overview.jpeg" alt="">&lt;/p>
&lt;h2 id="kubeblocks-是什么">KubeBlocks 是什么？&lt;/h2>
&lt;p>KubeBlocks 这个名字来源于 Kubernetes 和 LEGO 积木，这表明在 Kubernetes 上构建数据库和分析型工作负载既高效又愉快，就像玩乐高玩具一样。KubeBlocks 将顶级云服务提供商的大规模生产经验与增强的可用性和稳定性改进相结合，帮助用户轻松构建容器化、声明式的关系型、NoSQL、流计算和向量型数据库服务。&lt;/p>
&lt;p>官网：https://kubeblocks.io/。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubeblocks-on-kubesphere-20231019-1.png" alt="">&lt;/p>
&lt;h2 id="为什么需要-kubeblocks">为什么需要 KubeBlocks？&lt;/h2>
&lt;p>Kubernetes 已经成为容器编排的事实标准。它利用 ReplicaSet 提供的可扩展性和可用性以及部署提供的推出和回滚功能来管理数量不断增加的无状态工作负载。然而，管理有状态工作负载给 Kubernetes 带来了巨大的挑战。尽管 StatefulSet 提供了稳定的持久存储和唯一的网络标识符，但这些功能对于复杂的有状态工作负载来说远远不够。&lt;/p>
&lt;p>为了应对这些挑战，并解决复杂性问题，KubeBlocks 引入了新的 workload——RSM（Replicated State Machines），具有以下能力：&lt;/p>
&lt;ul>
&lt;li>基于角色的更新顺序可减少因升级版本、缩放和重新启动而导致的停机时间。&lt;/li>
&lt;li>维护数据复制的状态，并自动修复复制错误或延迟。&lt;/li>
&lt;/ul>
&lt;h2 id="它俩结合会带来什么收益">它俩结合会带来什么收益？&lt;/h2>
&lt;p>KubeSphere 提供了一个成熟的 Kubernetes 容器管理平台，而 KubeBlocks 在其上构建了数据库专业能力。这种创新融合，打通了数据库服务容器化的技术壁垒，实现了“开箱即用”。KubeSphere 让 KubeBlocks 应用享受集群级的资源调度和服务治理。KubeBlocks 使数据库服务在 KubeSphere 中具备自动化运维的专业实力。两者的协同互补，不仅简化了数据库的云化改造，也使数据库应用交付更加快速和可靠。&lt;/p>
&lt;h2 id="部署开始">部署开始&lt;/h2>
&lt;h3 id="部署先决条件">部署先决条件&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>确保已有可用的 KubeSphere 平台，如还未部署请至官网进行部署即可。官网地址：https://kubesphere.io/zh/docs/v3.4/。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>确保宿主机网络互通并可以访问互联网。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="登录-kubesphere-平台添加-kubeblocks-官方仓库">登录 KubeSphere 平台添加 KubeBlocks 官方仓库&lt;/h3>
&lt;p>仓库地址：https://apecloud.github.io/helm-charts。&lt;/p></description></item><item><title>基于 KubeSphere 的 Kubernetes 生产环境部署架构设计及成本分析</title><link>https://openksc.github.io/zh/blogs/kubesphere-based-architecture-design-and-cost-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-based-architecture-design-and-cost-analysis/</guid><description>&lt;h2 id="知识点">知识点&lt;/h2>
&lt;ul>
&lt;li>定级：&lt;strong>中级&lt;/strong>&lt;/li>
&lt;li>运维部署架构图如何画？&lt;/li>
&lt;li>架构设计方案如何写？&lt;/li>
&lt;li>部署节点如何规划？&lt;/li>
&lt;li>成本如何分析计算？&lt;/li>
&lt;/ul>
&lt;h2 id="简介">简介&lt;/h2>
&lt;h3 id="架构概要说明">架构概要说明&lt;/h3>
&lt;p>今天分享一个实际小规模生产环境部署架构设计的案例，该架构设计概要说明如下：&lt;/p>
&lt;ul>
&lt;li>本架构设计适用于中小规模（&amp;lt;=50）的 Kubernetes 生产环境，大型环境没有经验，有待验证。&lt;/li>
&lt;li>所有节点采用云上虚拟机的方式部署，出于某些原因所有组件均自建，没有使用云上产品（&lt;strong>有条件建议使用云上产品&lt;/strong>）。&lt;/li>
&lt;li>本架构设计不包含安全设备，不包含 Kubernetes 安全配置，安全要求高的环境不适用。&lt;/li>
&lt;li>本架构设计属于第一版， 也是我在 Kubernetes 生产集群架构设计实践之路上走出的第一步，难免有一些不合理的地方（&lt;strong>欢迎各位指正&lt;/strong>），后续会根据线上遇到的问题持续进行优化改进。&lt;/li>
&lt;li>本架构设计是基于 KubeSphere 部署的 Kubernetes，后续的很多功能实现都依托于 KubeSphere。&lt;/li>
&lt;li>本架构设计时使用的当时最新的软件版本，拿到目前来看也有一定的参考意义，完全可以直接套用，换一下软件版本即可（具体怎么换，请看下文）。&lt;/li>
&lt;/ul>
&lt;p>本文只介绍&lt;strong>选型分析&lt;/strong>、&lt;strong>部署架构图&lt;/strong>、&lt;strong>部署架构设计说明&lt;/strong>、&lt;strong>部署节点规划&lt;/strong>、&lt;strong>上云总成本分析&lt;/strong>等内容，具体的安装部署暂不涉及。&lt;/p>
&lt;h3 id="选择-kubesphere-的理由">选择 KubeSphere 的理由&lt;/h3>
&lt;p>KubeSphere 是在 Kubernetes 之上构建的以应用为中心的多租户容器平台，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。&lt;/p>
&lt;p>KubeSphere 提供了运维友好的向导式操作界面，帮助企业快速构建一个强大和功能丰富的容器云平台。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>完全开源&lt;/strong>，通过 CNCF 一致性认证的 Kubernetes 平台，100% 开源，由社区驱动与开发。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>安装简单，使用简单&lt;/strong>，支持部署在任何基础设施环境，提供在线与离线安装，支持一键升级与扩容集群。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>功能丰富，在一个平台统一纳管 DevOps、云原生可观测性、服务网格、应用生命周期、多租户、多集群、存储与网络。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>模块化 &amp;amp; 可插拔，平台中的所有功能都是可插拔与松耦合，您可以根据业务场景可选安装所需功能组件。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>具备构建一站式企业级的 DevOps 架构与可视化运维能力(省去自己用开源工具手工搭积木)。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>提供从平台到应用维度的日志、监控、事件、审计、告警与通知，实现集中式与多租户隔离的可观测性。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>简化应用的持续集成、测试、审核、发布、升级与弹性扩缩容。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>为云原生应用提供基于微服务的灰度发布、流量管理、网络拓扑与追踪。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>提供易用的界面命令终端与图形化操作面板，满足不同使用习惯的运维人员。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>可轻松解耦，避免厂商绑定&lt;/strong>。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>除了上面的 10 条理由以外，更主要的是同类产品中 KubeSphere 属于最能打的，其它竞品在当年（2021 年）或多或少都有一些问题，无法走进我的心里。&lt;/p>
&lt;h2 id="部署架构设计">部署架构设计&lt;/h2>
&lt;h3 id="部署架构图">部署架构图&lt;/h3>
&lt;p>&lt;img src="https://opsman-1258881081.cos.ap-beijing.myqcloud.com//k8s-on-kubesphere-v1-A4.png" alt="">&lt;/p>
&lt;h3 id="涉及软件版本">涉及软件版本&lt;/h3>
&lt;p>&lt;strong>初次设计 v1.0 版时的主要软件版本&lt;/strong>：&lt;/p></description></item><item><title>基于 KubeSphere 的 Nebula Graph 多云架构管理实践</title><link>https://openksc.github.io/zh/blogs/nebulagraph-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/nebulagraph-kubesphere/</guid><description>&lt;blockquote>
&lt;p>本文是杭州站 Meetup 讲师乔雷根据其分享内容整理而成的文章。主要介绍了 Nebula Graph 的简介及架构，NebulaCloud 的架构和流程以及使用 KubeSphere 进行多集群管理的实践。&lt;/p>&lt;/blockquote>
&lt;p>图数据库是一种使用图结构进行语义查询的数据库，它使用节点、边和属性来表示和存储数据。图数据库的应用领域非常广泛，在反应事物之间联系的计算都可以使用图数据库来解决，常用的领域如社交领域里的好友推荐、金融领域里的风控管理、零售领域里的商品实时推荐等等。&lt;/p>
&lt;h2 id="nebula-graph-简介与架构">Nebula Graph 简介与架构&lt;/h2>
&lt;p>Nebula Graph 是一个高性能、可线性扩展、开源的分布式图数据库，它采用存储、计算分离的架构，计算层和存储层可以根据各自的情况弹性扩容、缩容，这就意味着 Nebula Graph 可以最大化利用云原生技术实现弹性扩展、成本控制，能够容纳千亿个顶点和万亿条边，并提供毫秒级查询延时的图数据库解决方案。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/NebulaGraph-architecture.jpg" alt="Nebula Graph 架构图">&lt;/p>
&lt;p>上图所示为 Nebula Graph 的架构，一个 Nebula 集群包含三个核心服务，Graph Service、Meta Service 和 Storage Service。每个服务由若干个副本组成，这些副本会根据调度策略均匀地分布在部署节点上。&lt;/p>
&lt;p>Graph Service 对应的进程是 nebula-graphd，它由无状态无关联的计算节点组成，计算节点之间互不通信。Graph Service 的主要功能，是解析客户端发送 nGQL 文本，通过词法解析 Lexer 和语法解析 Parser 生成执行计划，并通过优化后将执行计划交由执行引擎，执行引擎通过 Meta Service 获取图点和边的 schema，并通过存储引擎层获取点和边的数据。&lt;/p>
&lt;p>Meta Service 对应的进程是 nebula-metad ，它基于 Raft 协议实现分布式集群，leader 由集群中所有 Meta Service 节点选出，然后对外提供服务，followers 处于待命状态并从 leader 复制更新的数据。一旦 leader 节点 down 掉，会再选举其中一个 follower 成为新的 leader。Meta Service 不仅负责存储和提供图数据的 meta 信息，如 Space、Schema、Partition、Tag 和 Edge 的属性的各字段的类型等，还同时负责指挥数据迁移及 leader 的变更等运维操作。&lt;/p></description></item><item><title>基于 KubeSphere 的 Spring Could 微服务 CI/CD 实践</title><link>https://openksc.github.io/zh/blogs/spring-cloud-on-kubeshpere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/spring-cloud-on-kubeshpere/</guid><description>&lt;p>本文以 Pig 为例介绍如何在 KubeSphere 上发布一个基于 Spring Cloud 微服务的 CI/CD 项目。&lt;/p>
&lt;h2 id="背景简介">背景简介&lt;/h2>
&lt;h3 id="pig">Pig&lt;/h3>
&lt;p>Pig (&lt;a href="http://pig4cloud.cn/" target="_blank" rel="noopener noreferrer">http://pig4cloud.cn/&lt;/a>) 是一个基于 Spring Cloud 的开源微服务开发平台，也是微服务最佳实践。在国内拥有大量拥护者。同时也有商业版本提供技术支持。&lt;/p>
&lt;h3 id="kubesphere">KubeSphere&lt;/h3>
&lt;p>KubeSphere (&lt;a href="https://Kubesphere.io" target="_blank" rel="noopener noreferrer">https://Kubesphere.io&lt;/a>) 是在 Kubernetes 之上构建的以应用为中心的多租户容器平台，完全开源，支持多云与多集群管理，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。KubeSphere 提供了运维友好的向导式操作界面，帮助企业快速构建一个强大和功能丰富的容器云平台。&lt;/p>
&lt;p>通过 KubeSphere 我们可以以简洁的方式将 Pig 项目部署至 Kubernetes 中。运维人员可以轻松的完成 Spring Cloud 运维任务。&lt;/p>
&lt;h3 id="前提条件">前提条件&lt;/h3>
&lt;p>具备 Spring Cloud 及 Pig 基础知识&lt;/p>
&lt;p>Jenkins 基础知识（非必备）&lt;/p>
&lt;p>KubeSphere 3.0 集群环境一套，并启用 DevOps 插件&lt;/p>
&lt;blockquote>
&lt;p>搭建 KubeSphere 集群不再本文覆盖范围，根据您的环境参考相关部署文档: &lt;a href="https://KubeSphere.com.cn/docs/installing-on-kubernetes/" target="_blank" rel="noopener noreferrer">https://KubeSphere.com.cn/docs/installing-on-kubernetes/&lt;/a>&lt;/p>&lt;/blockquote>
&lt;h2 id="架构设计">架构设计&lt;/h2>
&lt;p>Spring Cloud 有一个丰富、完备的插件体系，以实现各种运行时概念，作为应用栈的一部分。因此，这些微服务自身有库和运行时代理，来做客户端的服务发现，配置管理，负载均衡，熔断，监控，服务跟踪等功能。由于本篇重点在于如何快速建立 CI/CD 运维体系，因此对 Spring Cloud 与 Kubernetes 的深度整合不做过多讨论。我们将继续使用 Spring Cloud 底层的这些能力，同时利用 Kubernetes 实现滚动升级，健康检查，服务自动恢复等缺失的功能。&lt;/p></description></item><item><title>基于 KubeSphere 的分级管理实践</title><link>https://openksc.github.io/zh/blogs/kubesphere-level/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-level/</guid><description>&lt;blockquote>
&lt;p>作者：许伟，航天网信研发工程师&lt;/p>&lt;/blockquote>
&lt;p>K8s 是容器编排和分布式应用部署领域的领导者，在 K8s 环境中，我们只需要关心应用的业务逻辑，减轻了我们服务器网络以及存储等方面的管理负担。对于一个用户而言，K8s 是一个很复杂的容器编排平台，学习成本非常高。KubeSphere 抽象了底层的 K8s，并进行了高度的产品化，构建了一个全栈的多租户容器云平台，为用户提供了一个健壮、安全、功能丰富、具备极致体验的 Web 控制台，解决了 K8s 使用门槛高和云原生生态工具庞杂等痛点，使我们可以专注于业务的快速迭代，其多维度的数据监控，对于问题的定位，提供了很大的帮助。&lt;/p>
&lt;h3 id="为什么要在-kubersphere-上实现分级管理">为什么要在 KuberSphere 上实现分级管理&lt;/h3>
&lt;p>在 KubeSphere 中，资源可以在租户之间共享，根据分配的不同角色，可以对各种资源进行操作。租户与资源之间、资源与资源之间的自由度很高，权限粒度也比较大。在我们的系统中，资源是有权限等级的，像是低等级用户可以通过邀请、赋予权限等操作来操作高等级资源，或者像是低等级项目中的 Pod 可以调度到高等级的节点上，对资源。诸如此类跨等级操作资源等问题，我们在 KubeSphere 基础上来实现了分级管理。&lt;/p>
&lt;h3 id="什么是分级体系">什么是分级体系&lt;/h3>
&lt;p>分级，顾名思义就是按照既定的标准对整体进行分解、分类。我们将其抽象成一个金字塔模型，从地基到塔顶会有很多个层级，我们将公共资源作为金字塔的地基，拥有最高权限的 admin 作为塔顶，其他资源按照权限等级划分成不同等级。低层级资源是不能访问高等级资源，高等级资源可以获取它等级之下的所有资源，构建了这样一个权益递减、层级间隔离的分级体系。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/level-xuwei-1.jpg" alt="">&lt;/p>
&lt;h3 id="如何实现分级管理">如何实现分级管理&lt;/h3>
&lt;p>我们定义了一个代表等级的标签 &lt;code>kubernetes.io/level&lt;/code>。以一个多节点的集群为例，首先我们会给用户、企业空间、节点等资源打上代表等级的标签。在邀请用户加入企业空间或者项目时，要求加入的企业空间或者项目的等级不得高于用户的等级，同样项目在绑定企业空间时，也要求项目的等级不得高于企业空间的等级，才能对资源进行纳管；我们认为同一项目下的资源的等级是相同的，基于项目创建的负载、Pod、服务等资源的等级跟项目保持一致；同时 Pod 中加入节点亲和性，以使 Pod 调度到不高于其权限等级的节点上。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/level-xuwei-2.jpg" alt="">&lt;/p>
&lt;p>例如这里，我们创建了一个权限等级是 3 的用户 &lt;code>demo-user&lt;/code>，他可以加入权限等级不高于3的企业空间或者项目中。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">User&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">iam.kubesphere.io/v1alpha2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">demo-user&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">labels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kubernetes.io/level&lt;/span>: &lt;span style="color:#ae81ff">3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">email&lt;/span>: &lt;span style="color:#ae81ff">demo-user@kubesphere.io&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>创建一个权限等级是 2 的项目 &lt;code>demo-ns&lt;/code>，那么基于项目创建的负载、Pod、存储等资源的权限等级也是 2。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Namespace&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">demo-ns&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">labels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kubernetes.io/level&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>基于 &lt;code>demo-ns&lt;/code> 项目创建了一个&lt;code>nginx&lt;/code> 的 Pod，他的权限等级也是 2，同时加入节点亲和性，要求其调度到权限等级不高于 2 的节点上。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">labels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kubernetes.io/level&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">imagePullPolicy&lt;/span>: &lt;span style="color:#ae81ff">IfNotPresent&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">containerPort&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">protocol&lt;/span>: &lt;span style="color:#ae81ff">TCP&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">affinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">nodeAffinity&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">nodeSelectorTerms&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">matchExpressions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">kubernetes.io/level&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">operator&lt;/span>: &lt;span style="color:#ae81ff">Lt&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">values&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">matchExpressions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">kubernetes.io/level&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">operator&lt;/span>: &lt;span style="color:#ae81ff">In&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">values&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="如何实现资源的升降级">如何实现资源的升降级&lt;/h3>
&lt;p>在分级管理体系中，支持等级的无限划分，只需要定义一个中间值，就可以在两个等级之间插入一个新的等级，无需操作其他资源；在对资源进行升降级时，只需要修改对应资源的 &lt;code>label&lt;/code> 标签，就可以对资源进行升降级操作。当然，在对资源进行升降级的时候，我们需要对资源进行检测，保证升级时，其上层资源的权限等级不得低于目标等级；同时，降级时，其下层资源的权限等级不得高于目标等级。在不满足升降级操作条件时，需要将对应资源也做相应调整才可以。&lt;/p></description></item><item><title>基于 KubeSphere 的开源微服务开发平台 Pig 最佳实践</title><link>https://openksc.github.io/zh/blogs/kubesphere-pig-practice/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-pig-practice/</guid><description>&lt;blockquote>
&lt;p>作者：何昌涛，北京北大英华科技有限公司高级 Java 工程师，云原生爱好者。&lt;/p>&lt;/blockquote>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>近年来，为了满足越来越复杂的业务需求，我们从传统单体架构系统升级为微服务架构，就是把一个大型应用程序分割成可以独立部署的小型服务，每个服务之间都是松耦合的，通过 RPC 或者是 Rest 协议来进行通信，可以按照业务领域来划分成独立的单元。但是微服务系统相对于以往的单体系统更为复杂，当业务增加时，服务也将越来越多，服务的频繁部署、监控将变得复杂起来，尤其在上了 K8s 以后会更加复杂。那么有没有一款全栈的容器云平台来帮我们解决这些问题哩？那当然是有的，下面我们一起来揭秘一下吧。&lt;/p>
&lt;h2 id="介绍">介绍&lt;/h2>
&lt;h3 id="kubesphere">KubeSphere&lt;/h3>
&lt;p>KubeSphere 是在 Kubernetes 之上构建的开源容器平台，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。&lt;/p>
&lt;h3 id="pig">Pig&lt;/h3>
&lt;p>&lt;a href="https://gitee.com/log4j/pig" target="_blank" rel="noopener noreferrer">Pig&lt;/a> 是一个基于 Spring Boot 2.7、 Spring Cloud 2021 &amp;amp; Alibaba、 SAS OAuth2 的开源微服务开发平台，也是微服务最佳实践。在国内拥有大量拥护者，同时也有商业版本提供技术支持。&lt;/p>
&lt;h2 id="环境搭建">环境搭建&lt;/h2>
&lt;ul>
&lt;li>K8s 容器化环境一套，并部署完 KubeSphere v3.3.0 版本，启用 DevOps 插件。&lt;/li>
&lt;li>GitLab 代码仓库管理开源系统一套。&lt;/li>
&lt;li>Harbor 容器镜像开源系统一套。&lt;/li>
&lt;li>SonarQube 开源自动代码审查工具一套。&lt;/li>
&lt;li>一个更易于构建云原生应用的动态服务发现、配置管理和服务管理的 Nacos 开源平台一套（可选，Pig 已提供 Naocs 服务，即 Register 服务）。&lt;/li>
&lt;li>高性能的 key-value 数据库 Redis（3.2 +）一套（Pig 需要）。&lt;/li>
&lt;li>关系型开源数据库管理系统 MySQL 一套（Pig 需要）。&lt;/li>
&lt;li>高性能对象存储 Minio 一套（Pig 中文件上传需要，可选）。或者阿里云、华为云、腾讯对象存储也可。&lt;/li>
&lt;/ul>
&lt;h2 id="架构设计">架构设计&lt;/h2>
&lt;h3 id="kubesphere-架构">KubeSphere 架构&lt;/h3>
&lt;p>KubeSphere 将前端与后端分开，实现了面向云原生的设计，后端的各个功能组件可通过 REST API 对接外部系统。KubeSphere 无底层的基础设施依赖，可以运行在任何 Kubernetes、私有云、公有云、VM 或物理环境（BM）之上。 此外，它可以部署在任何 Kubernetes 发行版上。如下所示：&lt;/p></description></item><item><title>基于 KubeSphere 的应用容器化在智能网联汽车领域的实践</title><link>https://openksc.github.io/zh/blogs/kubesphere-best-practices-in-smart-connected-vehicles/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-best-practices-in-smart-connected-vehicles/</guid><description>&lt;h2 id="公司简介">公司简介&lt;/h2>
&lt;p>某国家级智能网联汽车研究中心成立于 2018 年，是担当产业发展咨询与建议、共性技术研发中心、创新成果转化的国家级创新平台，旨在提高我国在智能网联汽车及相关产业在全球价值链中的地位。&lt;/p>
&lt;p>目前着力建设基于大数据与云计算的智能汽车云端运营控制中心平台。推进云端运营控制中心建设的过程中，运控中心平台的集成、部署、运维方案经历了 3 代的升级迭代过程。&lt;/p>
&lt;p>第一代部署方案是直接将平台的前后端各个模块手动部署在自有物理机中，并将物理机托管在 ICT 的机房中。&lt;/p>
&lt;p>第二代方案是将物理机集群用 Vmware ESXi 做了虚拟化，平台前后端各模块部署在虚拟机，提升了资源利用率，降低了资源使用量。&lt;/p>
&lt;p>第三代，目前以容器化的方式部署在公有云的 KubeSphere 集群中。购买公有云的服务器资源，使用 KubeKey 安装 KubeSphere 集群，应用级服务采用 DevOps 流水线一键以容器化方式发布到 KubeSphere 集群中，真正实现了持续集成持续发布。应用研发工程师只需要在自己本地实现 feature 或者 fix bug，然后 commit 代码到 GitLab，之后通过 KubeSphere 的 DevOps 流水线一键发布到测试环境或者生产环境。通过使用 KubeSphere 以容器化的方式部署服务，减轻了各位研发工程师的发布工作负担，释放了研发资源。&lt;/p>
&lt;p>目前团队组成：1 名架构师负责架构设计、项目管理等全局工作，4 名研发工程师负责研发工作，1 名 DevOps 工程师负责 DevOps 建设和运维工作，这样的一个小团队就可以高效顺利完成大系统的建设工作。&lt;/p>
&lt;h2 id="背景介绍">背景介绍&lt;/h2>
&lt;p>云计算的发展已经逐渐成熟，基于云计算的大数据、人工智能行业发展的越来越成熟，汽车领域与云计算、大数据、人工智能的融合创新发展势不可挡，自动驾驶已经在全球范围内陆续落地。我国汽车科学家基于我国国情和汽车行业发展趋势，提出了自动驾驶汽车的中国方案，也即车路协同方案，以弥补国际上单车智能方案的不足。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/1202308151044.png" alt="">&lt;/p>
&lt;p>在这种行业发展背景下，推进建设车路协同的自动驾驶云端运营控制中心是亟待突破的行业共性关键技术。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/1202308151045.png" alt="">&lt;/p>
&lt;p>在建设自动驾驶云端运营控制中心的过程中，面临许多的实际困难，比如软硬件资源比较紧张，研发人员非常少，建设任务特别繁重，运控中心平台对车辆侧、道路侧物理基础设施的依赖比较种等方面的因素，为了提高有限的存储、计算、网络等硬件资源的利用率和减轻有限研发人员工作负担、高质高效完成运控中心平台的建设任务，建设团队的集成和部署经历了物理机部署、虚拟机部署直到当前的基于 KubeSphere 的容器化部署方案的迭代和升级过程。&lt;/p>
&lt;h2 id="选型说明">选型说明&lt;/h2>
&lt;p>在研究上云过程中，想过直接购买阿里云的 K8s 集群，但是由于公司本身有一些物理服务器要利用起来，所以就继续调研，最终选择 KubeSphere 作为容器化的解决方案。&lt;/p>
&lt;p>我们选择 KubeSphere 的原因有以下几点：&lt;/p>
&lt;ul>
&lt;li>得益于 KubeKey 这个安装工具，安装起来更加方便，比以前单纯安装 K8s 要简便、容易的多。&lt;/li>
&lt;li>KubeSphere 相当于给 K8s 做了图形界面，从 web 界面打开查看集群状态，对集群进行运维非常方便，比在命令行下敲命令简单明了的多。&lt;/li>
&lt;li>KubeSphere 支持流水线功能，在不安装额外的软件的情况下就可以实现持续发布功能，持续发布和 K8s 结合在一起，工作起来减轻很多繁琐的操作。&lt;/li>
&lt;/ul>
&lt;h2 id="实践过程">实践过程&lt;/h2>
&lt;p>由于使用 KubeSphere 和 K8s 以容器化方式部署应用对项目组成员来说都是第一次，无论是比较资深的专家架构师，还是各位研发和运维人员来说，都是在做了基本调研和学习后首次使用，所以，我们的应用容器化之路是学习中使用、使用中提高的一个过程。&lt;/p></description></item><item><title>基于 KubeSphere 的运管系统落地实践</title><link>https://openksc.github.io/zh/blogs/kubesphere-practice/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-practice/</guid><description>&lt;blockquote>
&lt;p>作者：任建伟，某知名互联网公司云原生工程师，容器技术信徒，云原生领域的实践者。&lt;/p>&lt;/blockquote>
&lt;h2 id="背景介绍">背景介绍&lt;/h2>
&lt;p>在接触容器化之前，我们团队内部的应用一直都是基于虚拟机运管，由开发人员自行维护。&lt;/p>
&lt;p>由于面向多开发部门服务，而开发人员运维能力参差不齐，所以每次部署新的环境时往往都要耗费大量时间。&lt;/p>
&lt;p>针对部署难的问题，我们将部分组件、服务容器化，采用 Docker 发布管理解决了部分问题，但仍未降低对开发人员的运维技能要求。&lt;/p>
&lt;p>下面是我们基于虚拟机管理开发环境的流程：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202212191343801.png" alt="">&lt;/p>
&lt;p>从上图中我们也能发现当前架构存在的问题：&lt;/p>
&lt;ul>
&lt;li>下发虚机由各部开发人员管理，虚机安全问题难以维护、保障；&lt;/li>
&lt;li>基于 &lt;code>shell&lt;/code> 运维，专业性过强；&lt;/li>
&lt;li>基于手动打包、发布，耗时耗力且不可靠。&lt;/li>
&lt;/ul>
&lt;h2 id="选型说明">选型说明&lt;/h2>
&lt;p>针对上述提到的痛点，我们决定对运维架构进行改造。新建运管平台，技术选型整体基于云原生，优先选取 CNCF 项目。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202212191344409.png" alt="">&lt;/p>
&lt;p>Kubernetes 成为了我们平台底座的不二选择， 但 Kubernetes 原生的 Dashboard 不太满足实际使用需求。&lt;/p>
&lt;p>而从头开发一套 &lt;code>workbench&lt;/code> 又耗时耗力，由此我们目光转向了开源社区。&lt;/p>
&lt;p>此时，一个集颜值 + 强大功能于一身的开源项目进入我们视野。是的，它便是 KubeSphere。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202212191345177.png" alt="">&lt;/p>
&lt;p>而 &lt;code>KubeSphere&lt;/code> 愿景是打造一个以 &lt;code>Kubernetes&lt;/code> 为内核的云原生分布式操作系统，它的架构可以非常方便地使第三方应用与云原生生态组件进行即插即用（plug-and-play）的集成，支持云原生应用在多云与多集群的统一分发和运维管理。&lt;/p>
&lt;p>对于 &lt;code>KubeSphere&lt;/code> 能否作为部署平台，最终结论如下：&lt;/p>
&lt;p>&lt;code>KubeSphere&lt;/code> 虽功能强大，但更适合作为管理端使用，不太适合面向普通用户。&lt;/p>
&lt;p>我们需要本地化一套 &lt;code>workbench&lt;/code> ，简化部分功能，屏蔽专业性术语（如工作负载、容器组、安全上下文等）。&lt;/p>
&lt;p>本地化部分内容如下：&lt;/p>
&lt;ul>
&lt;li>基于企业空间、命名空间，本地化租户、工作空间的概念，一个租户（企业空间）可管理一个到多个工作空间（命名空间），并接入独立用户体系。&lt;/li>
&lt;li>本地化应用发布流程： 由拆分的应用发布流程（构建镜像+创建负载），本地化为：创建应用 -&amp;gt; 上传 jar -&amp;gt; 指定配置 -&amp;gt; 启动运行的串行流程。&lt;/li>
&lt;li>本地化链路监控：构建镜像预先埋点，创建应用时选择是否开启链路追踪。&lt;/li>
&lt;li>本地化配置、应用路由等，添加版本管理功能。&lt;/li>
&lt;/ul>
&lt;p>事实上，我们本地化的重点是应用管理，但是 &lt;code>KubeSphere&lt;/code> 功能过于强大、特性过于灵活，导致配置起来项过于繁琐。&lt;/p>
&lt;p>针对部分配置项我们采用设置默认值的方式，而非交由用户去配置。（比如：容器安全上下文、同步主机时间、镜像拉取策略、更新策略、调度策略等）&lt;/p>
&lt;p>改造后的运维架构如下：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202212191345998.png" alt="">&lt;/p>
&lt;h2 id="实践过程">实践过程&lt;/h2>
&lt;p>基于 KubeSphere 的运管平台整体架构如下：&lt;/p></description></item><item><title>基于 KubeSphere 流水线的 GitOps 最佳实践</title><link>https://openksc.github.io/zh/blogs/kubesphere-gitops/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-gitops/</guid><description>&lt;h2 id="背景">背景&lt;/h2>
&lt;p>&lt;code>KubeSphere 3.3.0&lt;/code> 集成了 &lt;code>ArgoCD&lt;/code>，但与笔者目前使用的 K8S 版本不兼容。再者，目前 &lt;code>KubeSphere&lt;/code> 中持续集成和流水线打通还是不太友好，也缺少文档说明（可能是笔者没有找到）。&lt;/p>
&lt;p>&lt;strong>目前遇到最主要的问题就是流水线制作完成的镜像如何更新到 Git 仓库，然后触发 Application 的同步。&lt;/strong>&lt;/p>
&lt;p>基于上述问题，目前有两种方法：&lt;/p>
&lt;ul>
&lt;li>ArgoCD 官方的 &lt;a href="https://argocd-image-updater.readthedocs.io/en/stable/" title="argocd-image-updater" target="_blank" rel="noopener noreferrer">argocd-image-updater&lt;/a>（根据镜像仓库的镜像 Tag 变化，完成服务镜像更新）&lt;/li>
&lt;li>KubeSphere 提供了一个 &lt;a href="https://github.com/kubesphere-sigs/ks/blob/master/docs/app.md" title="ks app update 工具 " target="_blank" rel="noopener noreferrer">ks app update 工具&lt;/a>（支持 KubeSphere v3.3.0 中 Application，不支持原生 ArgoCD Application）&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>为此笔者基于 KubeSphere v3.1.1 的流水线，根据笔者的场景，实现了 GitOps 的服务发布流程，作此记录，暂且称之为最佳实践。&lt;/strong>&lt;/p>
&lt;h2 id="目标">目标&lt;/h2>
&lt;p>基于 KubeSphere 的流水线：&lt;/p>
&lt;ul>
&lt;li>自动创建服务部署清单&lt;/li>
&lt;li>自动创建服务 pipeline&lt;/li>
&lt;li>提交到服务部署清单仓库&lt;/li>
&lt;li>流水线风格统一&lt;/li>
&lt;li>通过服务流水线发布版本之后在一段时间内可以回滚&lt;/li>
&lt;li>实现 GitOps 方式管理服务部署清单和流水线清单，做到版本控制&lt;/li>
&lt;/ul>
&lt;h2 id="设计">设计&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/c0130af7-1a55-44ac-bce9-32af76e6162e.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/5a822de1-11cf-460d-8bf5-243c71d6fb06.png" alt="">&lt;/p>
&lt;h2 id="gitlab-项目规划">GitLab 项目规划&lt;/h2>
&lt;ul>
&lt;li>服务源代码和部署清单仓库分离，方便做权限管理；&lt;/li>
&lt;li>模板仓库 argocd-gitops-templates 是单独的 GitLab 仓库；&lt;/li>
&lt;li>每个 DevOps 项目对应一个 GitLab 仓库。（仓库名称为 &lt;code>argocd-gitops-{devops 项目名}&lt;/code>）；&lt;/li>
&lt;li>所有 GitLab 仓库都放在同一个 GitLab Group 下；&lt;/li>
&lt;li>每个仓库中包含了服务不同环境的清单，如：uat 和 prod；&lt;/li>
&lt;li>一个服务包含一个 pipeline Application 和服务部署清单 Application。服务部署清单通过 Application CR 管理；服务 pipeline 清单通过 pipeline Application CR 管理。&lt;/li>
&lt;/ul>
&lt;h2 id="模板仓库目录结构">模板仓库目录结构&lt;/h2>
&lt;p>&lt;code>argocd-gitops-templates&lt;/code> 项目存储了生成服务流水线和部署清单、argocd Application 的模板。&lt;/p></description></item><item><title>基于 WeDataSphere 与 KubeSphere 构建云原生机器学习平台</title><link>https://openksc.github.io/zh/blogs/wedatasphere-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/wedatasphere-kubesphere/</guid><description>&lt;blockquote>
&lt;p>本文是根据 KubeSphere 社区 2020 年度 Meetup 上讲师周可分享的内容整理而成。&lt;a href="https://player.bilibili.com/player.html?aid=288996625&amp;amp;bvid=BV1Mf4y1r7bv&amp;amp;cid=292045702&amp;amp;page=1&amp;amp;high_quality=1" target="_blank" rel="noopener noreferrer">视频回放&lt;/a>&lt;/p>&lt;/blockquote>
&lt;p>KubeSphere 开源社区的小伙伴们，大家好。我是微众银行大数据平台的工程师周可，接下来给大家分享的是基于 WeDataSphere 和 KubeSphere 这两个开源社区的产品去构建一个云原生机器学习平台 Prophecis。&lt;/p>
&lt;h2 id="prophecis-是什么">Prophecis 是什么？&lt;/h2>
&lt;p>首先我介绍一下什么是 Prophecis (Prophecy In WeDataSphere)，它的中文含义就是预言。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/WDS-1.png" alt="">&lt;/p>
&lt;p>Prophecis 是微众银行大数据平台团队开发的一站式机器学习平台，我们是基于 KubeSphere 管理的这一套多租户的容器化的高性能计算平台之上，搭建了我们提供给数据科学和算法工程师，以及我们的IT运维去使用的机器学习平台。
在交互界面层，大家可以看到最上面我们是有面向普通用户的一套机器学习应用开发界面，以及面向我们运维管理员的一套管理界面，其中管理员的界面基本上就是基于 KubeSphere 之上做了一些定制和开发；中间的服务层是我们机器学习平台的几个关键服务，主要为：&lt;/p>
&lt;ul>
&lt;li>Prophecis Machine Learning Flow：机器学习分布式建模工具，具备单机和分布式模式模型训练能力，支持 Tensorflow、Pytorch、XGBoost等多种机器学习框架，支持从机器学习建模到部署的完整Pipeline；&lt;/li>
&lt;li>Prophecis MLLabis：机器学习开发探索工具，提供开发探索服务，是一款基于 Jupyter Lab 的在线 IDE，同时支持 GPU 及 Hadoop 集群的机器学习建模任务，支持 Python、R、Julia 多种语言，集成 Debug、TensorBoard 多种插件；&lt;/li>
&lt;li>Prophecis Model Factory：机器学习模型工厂，提供机器学习模型存储、模型部署测试、模型管理等服务；&lt;/li>
&lt;li>Prophecis Data Factory：机器学习数据工厂，提供特征工程工具、数据标注工具和物料管理等服务；&lt;/li>
&lt;li>Prophecis Application Factory：机器学习应用工厂，由微众银行大数据平台团队和 AI 部门联合共建，基于青云QingCloud 开源的 KubeSphere 定制开发，提供 CI/CD 和 DevOps 工具，GPU 集群的监控及告警能力。&lt;/li>
&lt;/ul>
&lt;p>最底层的基础平台就是 KubeSphere 管理的高性能容器化计算平台。&lt;/p></description></item><item><title>基于Jenkins + Argo 实现多集群的持续交付</title><link>https://openksc.github.io/zh/blogs/jenkins+argo-for-multi-cluster-cd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/jenkins+argo-for-multi-cluster-cd/</guid><description>&lt;blockquote>
&lt;p>作者：周靖峰，青云科技容器顾问，云原生爱好者，目前专注于 DevOps，云原生领域技术涉及 Kubernetes、KubeSphere、Argo。&lt;/p>&lt;/blockquote>
&lt;h2 id="前文概述">前文概述&lt;/h2>
&lt;p>前面我们已经掌握了如何通过 Jenkins + Argo CD 的方式实现单集群的持续交付，明白了整个 CI/CD 过程中不同工具在流水线中的关系。所以接下来我们将更深入的了解 Argo CD 的特性。&lt;/p>
&lt;p>前文链接：&lt;a href="https://www.kubesphere.io/zh/blogs/jenkins&amp;#43;argo-for-single-cluster-cd/" target="_blank" rel="noopener noreferrer">KubeSphere DevOps 基于 Jenkins + Argo 实现单集群的持续交付实践&lt;/a>。&lt;/p>
&lt;h2 id="kubesphere-配置">KubeSphere 配置&lt;/h2>
&lt;h3 id="集群配置">集群配置&lt;/h3>
&lt;p>这里我们需要准备至少 2 个集群，并且需要开启多集群组件、DevOps 组件。&lt;/p>
&lt;p>因为 KubeSphere 已经内置了 Argo，所以只要被 KubeSphere 所管理的集群会自动注册上 Argo。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/image-20240320-1.png" alt="">&lt;/p>
&lt;h3 id="devops-配置">DevOps 配置&lt;/h3>
&lt;p>这里我们依旧要准备一个 Git 仓库， 这里仍然是使用我们之前的仓库例子。&lt;/p>
&lt;pre tabindex="0">&lt;code>https://github.com/Feeeenng/devops-maven-sample.git
&lt;/code>&lt;/pre>&lt;p>不过需要注意，这次我们需要选择 &lt;code>multi-cluster&lt;/code> 分支。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/image-20240320-2.png" alt="">&lt;/p>
&lt;h2 id="argo-cd-部分">Argo CD 部分&lt;/h2>
&lt;h3 id="applicationset">ApplicationSet&lt;/h3>
&lt;p>这里主要介绍 ArgoCD 的一个控制器 &lt;code>ApplicationSet controller&lt;/code>。&lt;/p>
&lt;p>此控制器追加了对跨多集群以及 &lt;code>monorepos&lt;/code> 的支持。该项目以前是一个独立项目，后在 Argo CD v2.3 版本中合入主分支。&lt;/p>
&lt;p>ApplicationSet 控制器主要应用场景：&lt;/p>
&lt;ul>
&lt;li>通过 Argo CD 单一 Kubernetes 资源管理应用发布多集群；&lt;/li>
&lt;li>单一 Kubernetes 资源发布一个 Git 或者多个 Git 仓库来部署多个应用；&lt;/li>
&lt;li>增加了 monorepos 的支持；&lt;/li>
&lt;li>多租户集群模式下，提高了单个集群租户使用 Argo CD 部署能力。&lt;/li>
&lt;/ul>
&lt;h3 id="generators">Generators&lt;/h3>
&lt;p>ApplicationSet 主要通过 &lt;code>generators&lt;/code> 来实现对资源的定义， 通过 &lt;code>template&lt;/code> 来实现参数值的替换。目前主要支持以下几种：&lt;/p></description></item><item><title>基于云原生的私有化 PaaS 平台交付实践</title><link>https://openksc.github.io/zh/blogs/cloudnative-paas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/cloudnative-paas/</guid><description>&lt;blockquote>
&lt;p>作者：牛玉富，某知名互联网公司专家工程师。喜欢开源，热衷分享，对 K8s 及 golang 网关有较深入研究。&lt;/p>&lt;/blockquote>
&lt;p>本文将解读如何利用云原生解决私有化交付中的问题，进而打造一个 PaaS 平台，提升业务平台的复用性。在进入正题之前，有必要先明确两个关键词：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>PaaS 平台&lt;/strong>：多个核心业务服务作为一个整体平台去封装，以平台形式提供服务。&lt;/li>
&lt;li>&lt;strong>私有化交付&lt;/strong>：平台需要部署私有云环境中，要面对无网情况下依然可以运转。&lt;/li>
&lt;/ul>
&lt;h2 id="传统交付痛点">传统交付痛点&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/1fe2f401861442089a1a41056bfec05f.png" alt="">&lt;/p>
&lt;p>如上图：私有云会有明确的安全性要求&lt;/p>
&lt;ol>
&lt;li>私有云服务无法连接外网，数据只能通过单向网闸形式进行摆渡到内网私有云。&lt;/li>
&lt;li>源代码只能存储在公司机房中，私有云只部署编译文件。&lt;/li>
&lt;li>服务会不定期迭代，另外为了保证服务稳定性需要自建独立业务监控。&lt;/li>
&lt;/ol>
&lt;p>基于以上要求面临的挑战大概有几点：&lt;/p>
&lt;ol>
&lt;li>架构可迁移性差：服务之间配置复杂，多种异构语言需要修改配置文件，无固定服务 DNS。&lt;/li>
&lt;li>部署运维成本高：服务依赖环境需支持离线安装，服务更新需本地运维人员手动完成，复杂场景下，完整一次部署大概需要 数人 / 月 的时间。&lt;/li>
&lt;li>监控运维成本高：监控需支持系统级 / 服务级 / 业务级监控，通知方式需支持短信、Webhook 等多种类型。&lt;/li>
&lt;/ol>
&lt;h2 id="架构方案">架构方案&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/bdf9b1640d1049d99cb66b1d91d3368e.png" alt="">&lt;/p>
&lt;p>我们的原则是 拥抱云原生和复用已有能力，近可能使用业界已存在且成熟技术方案。
我们采用 KubeSphere+K8S 作为服务编排，处于安全性及简洁性考虑对 Syncd 进行二次开发完整 DevOps 能力，监控系统上采用 Nightingale+Prometheus 方案。&lt;/p>
&lt;p>如上图架构图&lt;/p>
&lt;ol>
&lt;li>蓝色框内是我们底层 PaaS 集群，我们对业务服务通用服务统一进行了服务编排升级，用以解决架构迁移性差问题。&lt;/li>
&lt;li>红色框内，监控系统作为一种编排服务形式存在，所有监控项交付前配置好。用以解决监控系统运维成本高问题。&lt;/li>
&lt;li>紫色框内，服务容器可以实现跨网段自动拉取并自动化部署。用以解决服务服务部署成本高问题。&lt;/li>
&lt;/ol>
&lt;p>下面我们针对这三部分做下介绍。&lt;/p>
&lt;h2 id="服务编排kubesphere">服务编排：KubeSphere&lt;/h2>
&lt;p>KubeSphere 的愿景是打造一个以 K8s 为内核的云原生分布式操作系统，它的架构可以非常方便地使第三方应用与云原生生态组件进行即插即用（plug-and-play）的集成，支持云原生应用在多云与多集群的统一分发和运维管理，同时它还拥有活跃的社区。&lt;/p>
&lt;p>KubeSphere 选型理由有以下几点：&lt;/p>
&lt;h3 id="基于制品的方式定制自己的私有化交付方案">基于制品的方式定制自己的私有化交付方案&lt;/h3>
&lt;h4 id="私有化镜像文件打包">私有化镜像文件打包&lt;/h4>
&lt;p>创建制品清单 :&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">kubekey.kubesphere.io/v1alpha2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Manifest&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">sample&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">arches&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">amd64&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">kubernetes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">v1.21.5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">components&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">helm&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">v3.6.3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cni&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">v0.9.1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">etcd&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">v3.4.13&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containerRuntimes&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">docker&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">20.10.8&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">crictl&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">v1.22.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">harbor&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">v2.4.1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">docker-compose&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">version&lt;/span>: &lt;span style="color:#ae81ff">v2.2.2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">images&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">dockerhub.kubekey.local/kubesphere/kube-apiserver:v1.22.1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>然后我们就可以通过命令进行导出了。&lt;/p></description></item><item><title>将传统应用改造成微服务，启用流量治理功能</title><link>https://openksc.github.io/zh/blogs/transform-traditional-applications-into-microservices/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/transform-traditional-applications-into-microservices/</guid><description>&lt;h2 id="现状">现状&lt;/h2>
&lt;p>目前大多数用户，在使用 KubeSphere 微服务治理功能时，仅仅停留在部署 Bookinfo，“体验”一把微服务治理的功能而已。如果要完全使用微服务，仍然无法上手；更不知如何将传统服务改造成微服务。&lt;/p>
&lt;p>本文将告诉你，如何将一个传统应用转化成微服务，从而来享受 Service Mesh 的各种功能，如灰度发布、服务治理、流量拓扑、Tracing 等功能。&lt;/p>
&lt;h2 id="介绍">介绍&lt;/h2>
&lt;p>KubeSphere 微服务使用 Application CRD，将相关联的资源抽象成了一个具体的应用，使用 Istio Application 功能，实现微服务流量治理、灰度发布、Tracing 等功能。屏蔽了 Istio 复杂的 Destination Rule 及 Virtual Service 概念，能根据流量治理设置及灰度发布策略自动生成这些资源。&lt;/p>
&lt;p>使用 KubeSphere 微服务，需满足以下条件：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Deployment 有 &lt;code>app&lt;/code> &lt;code>version&lt;/code> 这两个 label；Service 有 &lt;code>app&lt;/code> Label；且 Deploy 与 service 的 App Label 一致，等于 Service Name（Istio 需要）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在一个应用内，所有资源需要有这两个标签 app.kubernetes.io/name=&lt;applicationName>, app.kubernetes.io/version=&lt;Version>（Application 需要）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Deployment Name 为 Service Name 后面加 v1；如 Serevice 为 nginx, deployment 为 nginx-v1 （v3.0 及以前版本）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Deployment Template 中有相应 Annotation （Istio Sidecar 自动注入需要）&lt;/p></description></item><item><title>揭秘！KubeSphere 背后的“超级大脑”：etcd 的魅力与力量</title><link>https://openksc.github.io/zh/blogs/kubesphere-etcd-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-etcd-guide/</guid><description>&lt;blockquote>
&lt;p>作者：尹珉，KubeSphere Ambassador &amp;amp; Contributor，KubeSphere 社区用户委员会杭州站站长。&lt;/p>&lt;/blockquote>
&lt;h2 id="1-开篇揭开神秘面纱etcd-如何驱动-kubesphere-高效运转">1. 开篇：揭开神秘面纱，etcd 如何驱动 KubeSphere 高效运转&lt;/h2>
&lt;p>在云原生时代，etcd 作为 Kubernetes 生态中不可或缺的核心组件，扮演着 KubeSphere 集群“神经系统”的角色。它利用 Raft 一致性算法提供强大的分布式键值存储能力，确保集群状态信息的实时同步和持久化。&lt;/p>
&lt;p>每当在 KubeSphere 中执行资源操作时，这些指令首先通过 etcd 进行处理和分发，从而实现对整个集群状态的瞬时更新与管理。正是由于 etcd 的存在，KubeSphere 才得以在大规模容器编排中展现卓越的性能和稳定性。&lt;/p>
&lt;p>接下来，我们将深入探索 etcd 如何巧妙地融入 KubeSphere 生态系统，并通过实际应用场景展示其对提升平台工作效率和可靠性的关键作用。&lt;/p>
&lt;h2 id="2-时光机从诞生到崛起etcd-如何在云原生时代崭露头角">2. 时光机：从诞生到崛起，etcd 如何在云原生时代崭露头角&lt;/h2>
&lt;p>etcd 的旅程始于 2013 年 CoreOS 团队的一项创新尝试，随着其 V1 和 V2 版本的发展，逐渐奠定了在分布式系统数据一致性解决方案中的地位。从 etcd V1、V2 到 V3 版本的迭代过程中，性能不断提升，稳定性日益增强，功能上也不断丰富和完善。&lt;/p>
&lt;p>经历数次重要升级后，etcd V3 版本尤其显著地解决了 Kubernetes 发展过程中面临的存储瓶颈问题。在性能方面，通过优化实现了更快的数据读写速度；在稳定性上，引入了更为健壮的一致性保证机制；在功能上，则扩展了 API 接口，增强了安全性与可管理性。&lt;/p>
&lt;p>因此，etcd 凭借这些改进，在性能、稳定性和功能上的卓越表现成功捍卫了作为 Kubernetes 核心存储组件的地位，并在云原生时代中扮演着不可或缺的角色，持续推动整个生态系统的进步与发展。&lt;/p>
&lt;h2 id="3-深度剖析etcd-核心原理与架构设计它是如何做到数据存储的万无一失">3. 深度剖析：etcd 核心原理与架构设计，它是如何做到数据存储的万无一失&lt;/h2>
&lt;h3 id="31-基础架构图">3.1 基础架构图&lt;/h3>
&lt;p>etcd 是典型的读多写少存储，实际业务场景中，读一般占据 2/3 以上的请求。为了让大家对 etcd 每个模块有一定的初步了解，简单介绍一下每个模块的功能作用。&lt;/p></description></item><item><title>开启 Calico eBPF 数据平面实践</title><link>https://openksc.github.io/zh/blogs/calico-ebpf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/calico-ebpf/</guid><description>&lt;h2 id="简介">简介&lt;/h2>
&lt;p>Calico 从 v3.13 开始，集成了 &lt;code>eBPF&lt;/code> 数据平面。&lt;/p>
&lt;p>关于什么是 &lt;code>eBPF&lt;/code>, 以及 &lt;code>Calico&lt;/code> 为什么引入了 &lt;code>eBPF&lt;/code> , 并不是本篇文章的重点，感兴趣的朋友可以自行阅读&lt;a href="https://www.projectcalico.org/introducing-the-calico-ebpf-dataplane/" target="_blank" rel="noopener noreferrer">相关文档&lt;/a>。&lt;/p>
&lt;p>相比于 Calico 的默认基于 &lt;code>iptables&lt;/code> 数据平面，&lt;code>eBPF&lt;/code> 具有更高的吞吐量以外， 还具有 &lt;code>source IP preservation&lt;/code> 这个功能。&lt;/p>
&lt;p>在 K8s 中通常都是直接或者间接以 &lt;code>NodePort&lt;/code> 方式对外暴露接口的。而对于 K8s 这个分布式集群来讲，通常情况下，客户端连接 Node Port 端口的节点和负责响应请求的后端业务 Pod 所在的节点不是同一个节点，为了打通整个数据链路，就不可避免的引入了 &lt;code>SNAT&lt;/code>。但是这样显然也会带来一个副作用，即业务 Pod 在收到 Packet 以后，其 SRC IP 已经不再是客户端的实际 IP（被伪装成节点的内网 IP ）。另一方面，对于一些业务应用来讲，获取客户端 IP 是一个实实在在的刚需。比如：业务应用需要通过客户端 IP 来获取客户登陆的 geo 信息。&lt;/p>
&lt;p>目前 K8s 主要是通过设置 &lt;code>externaltrafficpolicy&lt;/code> 来规避这个问题的，但是这个方案本身并不能完全令人满意。Calico 从 v3.13 开始通过集成 &lt;code>eBPF&lt;/code> 优雅地解决了这个问题。&lt;/p>
&lt;p>在本篇文章中，我们将首先演示通过 &lt;a href="https://github.com/kubesphere/kubekey" target="_blank" rel="noopener noreferrer">KubeKey&lt;/a> 创建一个标准的 K8s 集群，并切换数据平面到 &lt;code>eBPF&lt;/code>，最后基于该数据平面做一个简单的演示。&lt;/p></description></item><item><title>鲲鹏（ARM64）+麒麟（Kylin v10）离线部署 KubeSphere</title><link>https://openksc.github.io/zh/blogs/deploy-kubesphere--offline-on-arm-kylin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deploy-kubesphere--offline-on-arm-kylin/</guid><description>&lt;p>本文将详细介绍，如何基于鲲鹏 CPU（ARM64）和操作系统 Kylin V10 SP2/SP3，利用 KubeKey 制作 KubeSphere 和 Kubernetes 离线安装包，并实战部署 KubeSphere 3.3.1 和 Kubernetes 1.22.12 集群。&lt;/p>
&lt;p>实战服务器配置&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>&lt;strong>主机名&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>IP&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>CPU&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>OS&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>用途&lt;/strong>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>master-1&lt;/td>
 &lt;td>192.168.10.2&lt;/td>
 &lt;td>Kunpeng-920&lt;/td>
 &lt;td>Kylin V10 SP2&lt;/td>
 &lt;td>离线环境 KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>master-2&lt;/td>
 &lt;td>192.168.10.3&lt;/td>
 &lt;td>Kunpeng-920&lt;/td>
 &lt;td>Kylin V10 SP2&lt;/td>
 &lt;td>离线环境 KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>master-3&lt;/td>
 &lt;td>192.168.10.4&lt;/td>
 &lt;td>Kunpeng-920&lt;/td>
 &lt;td>Kylin V10 SP2&lt;/td>
 &lt;td>离线环境 KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>deploy&lt;/td>
 &lt;td>192.168.200.7&lt;/td>
 &lt;td>Kunpeng-920&lt;/td>
 &lt;td>Kylin V10 SP3&lt;/td>
 &lt;td>联网主机用于制作离线包&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>实战环境涉及软件版本信息&lt;/p>
&lt;ul>
&lt;li>服务器芯片: &lt;strong>Kunpeng-920&lt;/strong>&lt;/li>
&lt;li>操作系统：&lt;strong>麒麟 V10 SP2 aarch64&lt;/strong>&lt;/li>
&lt;li>Docker: &lt;strong>24.0.7&lt;/strong>&lt;/li>
&lt;li>Harbor: &lt;strong>v2.7.1&lt;/strong>&lt;/li>
&lt;li>KubeSphere：&lt;strong>v3.3.1&lt;/strong>&lt;/li>
&lt;li>Kubernetes：&lt;strong>v1.22.12&lt;/strong>&lt;/li>
&lt;li>KubeKey: &lt;strong>v2.3.1&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="1-本文简介">1. 本文简介&lt;/h2>
&lt;p>本文介绍了如何在 &lt;strong>麒麟 V10 aarch64&lt;/strong> 架构服务器上制品和离线部署 KubeSphere 和 Kubernetes 集群。我们将使用 KubeSphere 开发的 KubeKey 工具实现自动化部署，在三台服务器上实现高可用模式最小化部署 Kubernetes 集群和 KubeSphere。&lt;/p></description></item><item><title>利用 KubeSphere 部署 Kasten K10</title><link>https://openksc.github.io/zh/blogs/kasten-k10-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kasten-k10-kubesphere/</guid><description>&lt;h2 id="kasten-on-kubesphere">Kasten on KubeSphere&lt;/h2>
&lt;p>&lt;strong>Kasten K10&lt;/strong> 是 Veeam 在 Kubernetes 平台的数据管理解决方案，通过部署 Kasten K10 企业可以安全地备份和还原、执行灾难恢复以及迁移云原生的应用。Kubernetes 集群资源和持久卷等存储资源。解决用户备份、灾难恢复、迁移过程中的数据管理问题，提高云原生环境数据管理的便捷性，帮助用户降低灾备成本，提高生产执行效率。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/dateplatform-kasten.jpg" alt="">&lt;/p>
&lt;p>&lt;strong>KubeSphere&lt;/strong> 是在 Kubernetes 之上构建的以应用为中心的多租户容器平台，完全开源，支持多云与多集群管理，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。KubeSphere 提供了运维友好的向导式操作界面，帮助企业快速构建一个强大和功能丰富的容器云平台。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubesphere-feature-overview.jpeg" alt="">&lt;/p>
&lt;p>本文将介绍如何在 KubeSphere 上部署 Kasten K10。&lt;/p>
&lt;h2 id="部署环境准备">部署环境准备&lt;/h2>
&lt;p>KubeSphere 是由青云QingCloud 开源的容器管理台，支持在任何基础设施上安装部署。在青云公有云上支持一键部署 KubeSphere（QKE）。
下面以在青云云平台快速启用 KubeSphere 容器平台为例部署 Kasten 云原生数据管理平台，至少需要准备 3 个可调度的 node 节点。你也可以在任何 Kubernetes 集群或 Linux 系统上安装 KubeSphere，可以点击参考 &lt;a href="https://kubesphere.io/zh/docs/" target="_blank" rel="noopener noreferrer">KubeSphere 官方文档&lt;/a>。&lt;/p>
&lt;p>创建完成后登录到 KubeSphere 平台界面：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubesphere-platform.jpg" alt="">&lt;/p>
&lt;p>点击左上角平台管理，选择访问控制，新建企业空间，这里命名为 kasten-workspace。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kasten-workspace.jpg" alt="">&lt;/p>
&lt;p>进入企业空间，选择应用仓库，添加一个 Kasten 的应用仓库：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/Kasten-helm.jpg" alt="">&lt;/p>
&lt;p>将 Kasten 官方 Helm 仓库添加到 KubeSphere 容器平台，&lt;a href="https://charts.kasten.io/" target="_blank" rel="noopener noreferrer">Helm 仓库地址&lt;/a>：https://charts.kasten.io/。&lt;/p></description></item><item><title>某物联网数智化园区行业基于 KubeSphere 的云原生实践</title><link>https://openksc.github.io/zh/blogs/kubesphere-practice-iot-dici/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-practice-iot-dici/</guid><description>&lt;h2 id="公司简介">公司简介&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/0-20230831.png" alt="">&lt;/p>
&lt;p>作为物联网 + 数智化园区一体化解决方案提供商，我们致力于为大中型园区、停车场提供软硬件平台，帮助园区运营者实现数字化、智能化运营。&lt;/p>
&lt;p>在使用 K8s 之前我们使用传统的方式部署上线，使用 spug（一款轻量级无 Agent 的自动化运维平台） 自动化在单节点完成代码部署上线，也没有进行容器化，随着产品上线提上日程，对稳定性要求提高，以及私有化部署环境管理问题，我们开始使用 Docker 以及 K8s。&lt;/p>
&lt;h2 id="背景介绍">背景介绍&lt;/h2>
&lt;p>降本增效是每个企业的目标，而 DevOps、容器化、云原生就是研发团队降本增效的方法论。在这个趋势下，使用 Docker、K8s 几乎是每个开发团队的必经之路。&lt;/p>
&lt;p>物联网平台对稳定性要求非常高，一旦停机，所有设备都将掉线重连，因此保证服务的稳定性，减少停机时间就非常重要。&lt;/p>
&lt;p>在使用 K8s 之前，我们很多时间都要人工处理各种繁琐重复的服务维护问题，这种枯燥且毫无技术含量琐碎极大的消磨开发团队的激情。为了将人力从大量重复的环境配置、服务维护中解放出来从而提高开发迭代效率，我们就决定全面容器化，拥抱云原生。&lt;/p>
&lt;p>总结来说就是：&lt;/p>
&lt;ul>
&lt;li>服务稳定性，自动化运维，减少停机时间；&lt;/li>
&lt;li>分布式部署，弹性伸缩；&lt;/li>
&lt;li>DevOps 规范的部署上线流程。&lt;/li>
&lt;/ul>
&lt;p>这些问题迫使我们开始调研容器化、Docker、K8s 的应用。&lt;/p>
&lt;h2 id="选型说明">选型说明&lt;/h2>
&lt;p>由于没有相关经验，因此一开始我们就希望找到一款能够帮助快速上手 K8s 的工具，在调研 KubeSphere、Zadig、Rancher、KubeVela、Kubeadm 等多款工具后，我们最终选择了 KubeSphere。&lt;/p>
&lt;p>选择 KubeSphere 最主要的原因首先是它的社区活跃，有问题能够找到解决方案。同时它集成了很多开箱即用的插件如 DevOps，这正是我们所需要的。当然第一眼就选中 KubeSphere 还是因为它的颜值，能看得出来 KubeSphere 的 UI 是经过精心设计过的，这在开发工具领域中是极为难得的，从这点上就能够看出背后的开发团队对于打造一款基于 K8s 的云原生操作系统的理念与决心。&lt;/p>
&lt;p>使用 KubeSphere 让我们立马就拥有了成熟 DevOps 工作流了，而无需额外的搭建成本，这对于我们毫无 K8s 经验的团队来说太重要了，极大的降低了上手门槛。&lt;/p>
&lt;p>目前我们将所有无状态应用全部容器化，使用 K8s 负载，提交代码 Webhook 触发 KubeSphere 流水线自动发布，对于不习惯命令行操作的用户，KubeSphere 后台能满足所有需求。&lt;/p>
&lt;h2 id="实践过程">实践过程&lt;/h2>
&lt;h3 id="容器化及迁移到-k8skubesphere">容器化及迁移到 K8s、KubeSphere&lt;/h3>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/1-20230831.png" alt="">&lt;/p>
&lt;p>第一步就是将应用全部 Docker 容器化，然后使用 K8s 的 deployment 进行部署。实现分布式高可用的服务部署。&lt;/p></description></item><item><title>某制造企业基于 KubeSphere 的云原生实践</title><link>https://openksc.github.io/zh/blogs/best-practice-kubesphere-a-manufacturing-company/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/best-practice-kubesphere-a-manufacturing-company/</guid><description>&lt;h2 id="背景介绍">背景介绍&lt;/h2>
&lt;p>随着业务升级改造与软件产品专案的增多，常规的物理机和虚拟机方式逐渐暴露出一些问题：&lt;/p>
&lt;ul>
&lt;li>大量服务部署在虚拟机上，资源预估和硬件浪费较大；&lt;/li>
&lt;li>大量服务部署在虚拟机上，部署时间和难度较大，自动化程度较低；&lt;/li>
&lt;li>开发人员和运维人员，由于开发和部署服务环境不同，服务不稳定经常报错，产生的隔阂问题较多，效率较低；&lt;/li>
&lt;li>排查问题原因不便利，开发没权限上生产环境，服务日志和服务监控状态无法定位。&lt;/li>
&lt;/ul>
&lt;p>在竞争日益激烈和不断变化的市场环境下，公司需要在产品上不停的迭代开发，来满足业务的需求，快速进行响应变化，所以解决上述问题变得愈发迫切。&lt;/p>
&lt;h2 id="选型说明">选型说明&lt;/h2>
&lt;p>我们调研了两款开源产品。经过综合评估和比较，我们最终选择了 KubeSphere。KubeSphere 的定位是以应用为中心的容器平台，提供了简单易用的操作界面，一定程度上降低了学习成本，同时集成了原生 Istio 等功能，更加符合开发的使用习惯。&lt;/p>
&lt;h2 id="实践过程">实践过程&lt;/h2>
&lt;p>加快开发对应用需求的响应，快速交付价值，快速响应变化。敏捷开发是用短的迭代周期来适应更快的变化，而且保持增量的持续改进的过程，Kubernetes + Docker 是 Dev 和 Ops 融合的一个桥梁，反过来说，敏捷开发与自动化运维，推动企业 DevOps 落地，提供端对端的从需求分析到部署监控的全流程开发运维一体化。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/AgAABXCIwKlWpQIRdbNEoYpHDK9Dr4bp.png" alt="">&lt;/p>
&lt;h3 id="基础设施与部署架构">基础设施与部署架构&lt;/h3>
&lt;p>KubeSphere 的搭建也非常简单，通过 KubeAdmin 安装 Kubernetes，然后用 KubeSphere 官网推荐的方式安装 KubeSphere。私有内部云平台环境来搭建 Kubernetes 与 KubeSphere。基础服务器采用的是 Linux Centos 7，内核版本是 5.6。&lt;/p>
&lt;p>在搭建 Kubernetes 集群时，我们选择使用 Keepalived 和 HAproxy 创建高可用 Kubernetes 集群 master，其中包括负载均衡入口。&lt;/p>
&lt;p>部署参考图:&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/AgAABXCIwKkQv1hzIOxEPJ7FOB9fj91b.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/AgAABXCIwKkkFUTK7TNBbalxyOIq_qVZ.png" alt="">&lt;/p>
&lt;h3 id="存储与网络">存储与网络&lt;/h3>
&lt;p>目前我们主要对接的是 Ceph 的分布式存储，服务于各种持久化服务，比如我们会做一些 Harbor 的镜像，主要是 Rabbitmq、Redis、MySQL 等，生产环境主要是一些无状态的开发的服务，比如 Springboot、SpringCloud 开发的微服务，还有 Python 服务。Python 服务主要是用来做 AI 模型的简单分析。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/AgAABXCIwKk1wPMGdrlHO7hWd0lT30Ka.png" alt="">&lt;/p></description></item><item><title>你真的理解 K8s 中的 requests 和 limits 吗？</title><link>https://openksc.github.io/zh/blogs/deep-dive-into-the-k8s-request-and-limit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deep-dive-into-the-k8s-request-and-limit/</guid><description>&lt;p>在 K8s 集群中部署资源的时候，你是否经常遇到以下情形：&lt;/p>
&lt;ol>
&lt;li>经常在 K8s 集群种部署负载的时候不设置 CPU &lt;code>requests&lt;/code> 或将 CPU &lt;code>requests&lt;/code> 设置得过低（这样“看上去”就可以在每个节点上容纳更多 Pod ）。在业务比较繁忙的时候，节点的 CPU 全负荷运行。业务延迟明显增加，有时甚至机器会莫名其妙地进入 CPU 软死锁等“假死”状态。&lt;/li>
&lt;li>类似地，部署负载的时候，不设置内存 &lt;code>requests&lt;/code> 或者内存 &lt;code>requests&lt;/code> 设置得过低，这时会发现有些 Pod 会不断地失败重启。而不断重启的这些 Pod 通常跑的是 Java 业务应用。但是这些 Java 应用本地调试运行地时候明明都是正常的。&lt;/li>
&lt;li>在 K8s 集群中，集群负载并不是完全均匀地在节点间分配的，通常内存不均匀分配的情况较为突出，集群中某些节点的内存使用率明显高于其他节点。 K8s 作为一个众所周知的云原生分布式容器编排系统，一个所谓的事实上标准，其调度器不是应该保证资源的均匀分配吗？&lt;/li>
&lt;/ol>
&lt;p>如果在业务高峰时间遇到上述问题，并且机器已经 hang 住甚至无法远程 ssh 登陆，那么通常留给集群管理员的只剩下重启集群这一个选项。如果你遇到过上面类似的情形，想了解如何规避相关问题或者你是 K8s 运维开发人员，想对这类问题的本质一探究竟，那么请耐心阅读下面的章节。我们会先对这类问题做一个定性分析，并给出避免此类问题的最佳实践，最后如果你对 K8s &lt;code>requests&lt;/code> 和 &lt;code>limits&lt;/code> 的底层机制感兴趣，我们可以从源码角度做进一步地分析，做到“知其然也知其所以然”。&lt;/p>
&lt;h2 id="问题分析">问题分析&lt;/h2>
&lt;p>首先我们需要知道对于 CPU 和内存这 2 类资源，他们是有一定区别的。 CPU 属于可压缩资源，其中 CPU 资源的分配和管理是 Linux 内核借助于完全公平调度算法（ CFS ）和 Cgroup 机制共同完成的。简单地讲，如果 pod 中服务使用 CPU 超过设置的 CPU &lt;code>limits&lt;/code>， pod 的 CPU 资源会被限流（ throttled ）。对于没有设置&lt;code>limit&lt;/code>的 pod ，一旦节点的空闲 CPU 资源耗尽，之前分配的 CPU 资源会逐渐减少。不管是上面的哪种情况，最终的结果都是 Pod 已经越来越无法承载外部更多的请求，表现为应用延时增加，响应变慢。这种情形对于上面的情形 1 。内存属于不可压缩资源， Pod 之间是无法共享的，完全独占的，这也就意味着资源一旦耗尽或者不足，分配新的资源一定是会失败的。有的 Pod 内部进程在初始化启动时会提前开辟出一段内存空间。比如 JVM 虚拟机在启动的时候会申请一段内存空间。如果内存 &lt;code>requests&lt;/code> 指定的数值小于 JVM 虚拟机向系统申请的内存，导致内存申请失败（ oom-kill ），从而 Pod 出现不断地失败重启。这种情形对应于上面的情形 2 。对于情形 3 ，实际上在创建 pod 的过程中，一方面， K8s 需要拨备包含 CPU 和内存在内的多种资源，这里的资源均衡是包含 CPU 和内存在内的所有资源的综合考量。另一方面， K8s 内置的调度算法不仅仅涉及到“最小资源分配节点”，还会把其他诸如 Pod 亲和性等因素考虑在内。并且 k8s 调度基于的是资源的 &lt;code>requests&lt;/code> 数值，而之所以往往观察到的是内存分布不够均衡，是因为对于应用来说，相比于其他资源，内存一般是更紧缺的一类资源。另一方面， K8s 的调度机制是基于当前的状态。比如当出现新的 Pod 进行调度时，调度程序会根据其当时对 Kubernetes 集群的资源描述做出最佳调度决定。但是 Kubernetes 集群是非常动态的，由于整个集群范围内的变化，比如一个节点为了维护，我们先执行了驱逐操作，这个节点上的所有 Pod 会被驱逐到其他节点去，但是当我们维护完成后，之前的 Pod 并不会自动回到该节点上来，因为 Pod 一旦被绑定了节点是不会触发重新调度的。&lt;/p></description></item><item><title>去哪儿网业务大规模容器化最佳实践</title><link>https://openksc.github.io/zh/blogs/qunar-kubesphere-best-practice/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/qunar-kubesphere-best-practice/</guid><description>&lt;p>近几年随着云原生技术的成熟，Qunar 为了实现整个技术体系的演进，Qunar 在 2021 年向云原生迈出了第一步 -- 容器化 。 落地过程包括了价值评估、基础设施建设，CI/CD 流程改造、中间件的适配、可观测性工具、应用自动化迁移等。迁移过程涉及到了 3000 个应用左右，涉及千人级研发团队，是一个难度非常大的系统工程。这篇文章会讲述迁移过程涉及到的 CI/CD 模型改造、自动化应用容器化改造等最佳实践。&lt;/p>
&lt;h2 id="背景">背景&lt;/h2>
&lt;h3 id="容器化落地前的业务痛点">容器化落地前的业务痛点&lt;/h3>
&lt;p>在容器化落地之前，我们经常会听到来自不同角色的各种各样的抱怨与吐槽：&lt;/p>
&lt;ul>
&lt;li>领导层的声音：服务器资源和维护成本太高了，我们必须降低成本！&lt;/li>
&lt;li>OPS 的声音：有一台宿主故障了，需要把上边的服务赶快恢复！&lt;/li>
&lt;li>QA 的声音：测试环境好好的，为啥上线失败了？&lt;/li>
&lt;li>研发人员的声音：业务高峰来啦，资源不够用啦！赶快去扩容！哎! 扩容太慢了。&lt;/li>
&lt;/ul>
&lt;p>造成这些问题的因素主要包含：资源利用率、服务无法自愈、测试环境与线上环境不一致，运行环境缺少弹性能力。&lt;/p>
&lt;p>企业所面临的商业环境是复杂多变的，&lt;strong>当今环境的竞争异常激烈，而且不再是大鱼吃小鱼，而是快鱼吃慢鱼&lt;/strong>。通用电气前 CEO Jack Welch 曾经说过：“如果外部变化的速度超过内部变化的速度，终结就不远了”。我们的企业如何能在这样的环境中存活发展呢？各个企业都在不断的探索和实践中前行。今年来，云原生技术日趋成熟，已经被广大企业广泛接受并采用。云原生技术可以帮助企业降低成本，加快业务迭代，对赋能企业的产业升级提供强有力的技术支撑。容器化作为云原生技术之一，成为 Qunar 拥抱云原生进行技术升级的重要一环。&lt;/p>
&lt;h3 id="容器化落地过程的挑战与应对">容器化落地过程的挑战与应对&lt;/h3>
&lt;p>全司范围内进行容器化全面落地并不是一件容易的事，我们面临了重重困难，但是我们为了最终目标的达成，想尽一切办法去一一化解。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>首先，涉及部门多: 容器化迁移涉及 OPS、基础架构、数据组等基础设施部门以及 20+业务部门。这么多的部门达成对目标的一致认同，并保持行动的协调一致是非常不容容易的。好在我们这个项目得到了公司高层的认可，并将此作为 2021 年的企业级目标。在企业级目标的引领下，各个部门协同一致，通力配合保障了项目的成功。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>其次， 改造范围大：由于历史原因，我们的服务多是有状态的。从中间件、发布系统到网络、存储、日志收集、监控告警以及业务服务本身等等各个环节对机器名、IP、本地存储等状态有着强依赖。而容器本身是无状态的，这就意味着我们需要从基础设施、发布、运维的工具、平台进行整体改造。针对这重重问题，我们进行了一一列举，逐个击破，最终满足容器化迁移的条件。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>再次，业务迁移成本高：本次迁移涉及应用数量大概有 3000 多个，迁移过程需要对应用进行升级改造、测试回归及上线，这个过程将花费大量人力。如何降低业务的迁移成本呢？我们支持将大部分的适配工作在中间件层进行统一支持，然后支持在业务代码中进行中间件的自动升级，自动进行容器化迁移，通过持续交付流水线对迁移过程中的变更进行自动化的测试与验证，经过容器与虚机灰度混部观察等手段大大降低了业务人工迁移的成本。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最后，学习成本高：我们的研发团队有千人级的规模，对于新技术的引入与升级，研发同学需要花费额外的成本进行学习和使用。为了降低大家的学习成本，我们通过平台工具屏蔽差异性操作以及技术细节，通过可视化配置、引导式操作，优化持续交付流程等方式来降低业务的学习成本。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="容器化后的收益">容器化后的收益&lt;/h3>
&lt;p>经过 2021 年一年时间，我们完成了容器化基础设施建设，工具平台的升级改造以及 90% 应用的容器化迁移（应用总数 3000+）。从效果数据来看，容器化虚拟比例从 1：17 提升到 1：30， 资源利用率提升 76%；宿主运维时间之前以天为单位，容器化以后变成了分钟级，运维效率提升了 400 倍；由于容器化启动时间的缩短以及部署策略的优化，应用的交付速度提升 40%；K8s 集群提供了服务自愈能力，应用运行过程中平均自愈次数达到 2000 次/月；另外，容器化落地也为公司进行下一步云原生技术的深入推广与落地奠定了基础。&lt;/p>
&lt;h2 id="持续交付">持续交付&lt;/h2>
&lt;h3 id="项目研发流程">项目研发流程&lt;/h3>
&lt;p>Qunar 采用了业务驱动的价值流与以应用为中心的持续交付工作流的双流模型。企业以业务价值交付为目标，在交付过程中，各个阶段的交付物会在多个角色中的流转，在流转过程中难免会出现不同程度的浪费。为了有效对价值交付的效率进行有效度量与优化提升，我们不仅仅需要关注开发过程中的效率提升，还需要关注开发前的效率提升。我们将持续交付工作流的流转与价值流的流动进行联通，希望可以实现从项目域、开发域、测试域到运维域的流程自动流转的。但是在容器化之前，由于环境不一致、各阶段配置的不一致性等原因，导致交付过程中，交付流程在各阶段间流转时不可避免的存在人工干预的情况。容器化之后，我们参照云原生的 OAM 模型，对应用进行了规范化定义，建立应用画像，统一术语，消除数据孤岛，使流程可以顺畅高速流转。&lt;/p></description></item><item><title>让容器通信变得简单：深度解析 Containerd 中的 CNI 插件</title><link>https://openksc.github.io/zh/blogs/containerd-cni/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/containerd-cni/</guid><description>&lt;h2 id="引言">引言&lt;/h2>
&lt;p>在&lt;a href="https://kubesphere.io/zh/blogs/containerd-kubernetes/" target="_blank" rel="noopener noreferrer">上一篇文章&lt;/a>中，我们详细讨论了 Kubernetes 中 containerd 的使用方法和一些核心概念。今天，我们将继续深入，探索 containerd 中的 CNI 插件。作为容器网络的关键组成部分，CNI 插件在实现容器之间的网络通信中扮演了重要角色。了解 CNI 插件不仅可以帮助我们更好地管理 Kubernetes 集群，还能提升我们的整体运维效率。&lt;/p>
&lt;h2 id="探索-cni-插件的奥秘">探索 CNI 插件的奥秘&lt;/h2>
&lt;h3 id="1-什么是-cni-插件">1. 什么是 CNI 插件？&lt;/h3>
&lt;p>CNI（Container Network Interface）插件是独立的可执行文件，遵循 CNI 规范。Kubernetes 通过 kubelet 调用这些插件来创建和管理容器的网络接口。CNI 插件的主要职责包括网络接口的创建和删除、IP 地址的分配和回收、以及相关网络资源的配置和清理。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/3e21950f-e076-42b4-8f3b-8605e79aee3b.jpg" alt="">&lt;/p>
&lt;h3 id="2-cni-插件的工作流程简述">2. CNI 插件的工作流程简述&lt;/h3>
&lt;h4 id="21-pod-创建">2.1 Pod 创建&lt;/h4>
&lt;p>当一个新的 Pod 被创建时，kubelet 会调用 CNI 插件的 ADD 命令。CNI 插件会为 Pod 分配一个网络接口，并设置相关的网络配置，如 IP 地址和路由。这包括配置 Pod 的网络命名空间，使其能够与其他 Pod 进行通信。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/image-20240717-2.png" alt="">&lt;/p>
&lt;h4 id="22-pod-删除">2.2 Pod 删除&lt;/h4>
&lt;p>当 Pod 被删除时，kubelet 会调用 CNI 插件的 DEL 命令。CNI 插件会清理之前为该 Pod 分配的网络资源，如回收 IP 地址和删除网络接口。这一过程确保资源不会被浪费，并且系统能够持续高效地运行。&lt;/p></description></item><item><title>如何创建跨 Kubernetes 集群的流水线</title><link>https://openksc.github.io/zh/blogs/create-pipeline-across-multi-clusters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/create-pipeline-across-multi-clusters/</guid><description>&lt;p>随着 Kubernetes 被广泛的作为基础设施，各大云厂商都相继推出了各自的 Kubernetes 集群服务。那么在多个集群上，如何跨集群实践 DevOps 流水线呢？本文将主要以示例的形式给出回答。&lt;/p>
&lt;blockquote>
&lt;p>提示，本文需要对 KubeSphere 和 DevOps 相关的知识具有一定了解，具体包括 Kubernetes 资源创建、生成 Sonarqube Token、获取集群 kubeconfig 等。&lt;/p>&lt;/blockquote>
&lt;h2 id="kubesphere-devops-跨集群架构概览">KubeSphere DevOps 跨集群架构概览&lt;/h2>
&lt;h3 id="集群角色说明">集群角色说明&lt;/h3>
&lt;p>在多集群架构中，我们对集群进行了角色定义。每个集群的角色，在安装的配置文件中指定。具体详情，可以参考文档: &lt;a href="../../docs/multicluster-management/introduction/kubefed-in-kubesphere/">kubefed-in-kubesphere&lt;/a> 。一共有三种角色，host、member、none 。&lt;/p>
&lt;ul>
&lt;li>host&lt;/li>
&lt;/ul>
&lt;p>安装完整的 KubeSphere 核心组件，通过前端页面可以对各个集群进行管理。&lt;/p>
&lt;ul>
&lt;li>member&lt;/li>
&lt;/ul>
&lt;p>没有安装 KubeSpher Console 前端组件，不提供独立的页面管理入口，可以通过 host 集群的前端入口进行管理。&lt;/p>
&lt;ul>
&lt;li>none&lt;/li>
&lt;/ul>
&lt;p>没有定义角色，主要是为了兼容单集群模式。&lt;/p>
&lt;h3 id="多集群下-devops-的部署架构">多集群下 DevOps 的部署架构&lt;/h3>
&lt;p>&lt;img src="https://openksc.github.io/images/blogs/create-pipeline-across-multi-clusters/multicluster-devops.svg" alt="MultiCluster DevOps">&lt;/p>
&lt;p>上图，是 KubeSphere DevOps 在多集群上的部署架构。在每一个集群上，我们都可以选择性安装 DevOps 组件。具体安装步骤，与单集群开启 DevOps 组件没有区别。&lt;/p>
&lt;p>在创建 DevOps 工程时，前端会对可选集群进行标记，仅开启 DevOps 组件的集群能够被选择。&lt;/p>
&lt;h2 id="用户场景描述">用户场景描述&lt;/h2>
&lt;p>&lt;img src="https://openksc.github.io/images/blogs/create-pipeline-across-multi-clusters/use-case-for-multicluster.svg" alt="Use Case for MultiCluster">&lt;/p>
&lt;p>上图是一个 Demo 场景，通过多集群隔离不同的部署环境。一共有三个集群，开发集群、测试集群、生成集群。&lt;/p>
&lt;p>开发人员在提交代码之后，可以触发流水线执行，依次完成，单元测试、代码检测、构建和推送镜像，然后直接部署到开发集群。开发集群交给开发人员自主管控，作为他们的自测验证环境。经过审批之后，可以发布到测试环境，进行更严格的验证。最后，经过授权之后，发布到正式环境，用于对外提供服务。&lt;/p>
&lt;h2 id="创建一条跨集群的流水线">创建一条跨集群的流水线&lt;/h2>
&lt;h3 id="准备集群">准备集群&lt;/h3>
&lt;p>这里准备了三个集群，分别为：&lt;/p>
&lt;ul>
&lt;li>shire, 正式环境部署集群，角色: memeber&lt;/li>
&lt;li>gondor, 测试环境部署集群，角色: host, 需要开启 DevOps 组件&lt;/li>
&lt;li>rohan, 开发环境部署集群，角色: member&lt;/li>
&lt;/ul>
&lt;p>集群可以部署在不同的云厂商，也可以使用不同的 Kubernetes 版本。&lt;/p></description></item><item><title>三步搞定 ARM64 离线部署 Kubernetes + KubeSphere</title><link>https://openksc.github.io/zh/blogs/arm64-kubernetes-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/arm64-kubernetes-kubesphere/</guid><description>&lt;h3 id="背景">背景&lt;/h3>
&lt;p>由于 ARM 架构具有低功耗和并行好的特点，其应用也将会越来越广泛。KubeSphere 作为一款深受国内外开发者所喜爱的开源容器平台，也将积极参与并探索在 ARM 架构下的应用与创新。本文将主要介绍如何在 ARM64 环境下部署 Kubernetes 和 KubeSphere。&lt;/p>
&lt;h3 id="环境准备">环境准备&lt;/h3>
&lt;h4 id="节点">节点&lt;/h4>
&lt;p>KubeSphere 支持的操作系统包括：&lt;/p>
&lt;ul>
&lt;li>Ubuntu 16.04, 18.04&lt;/li>
&lt;li>Debian Buster, Stretch&lt;/li>
&lt;li>CentOS/RHEL 7&lt;/li>
&lt;li>SUSE Linux Enterprise Server 15&lt;/li>
&lt;li>openEuler&lt;/li>
&lt;/ul>
&lt;p>这里以一台 openEuler 20.09 64bit 为例：&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>name&lt;/th>
 &lt;th>ip&lt;/th>
 &lt;th>role&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>node1&lt;/td>
 &lt;td>172.169.102.249&lt;/td>
 &lt;td>etcd, control plane, worker&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>确保机器已经安装所需依赖软件（sudo curl openssl ebtables socat ipset conntrack docker）&lt;/p>
&lt;p>&lt;a href="https://github.com/kubesphere/kubekey/tree/release-1.0#requirements-and-recommendations" target="_blank" rel="noopener noreferrer">具体环境要求参见&lt;/a>&lt;/p>
&lt;p>关于多节点安装请参考 &lt;a href="https://kubesphere.com.cn/docs/installing-on-linux/introduction/multioverview/" target="_blank" rel="noopener noreferrer">KubeSphere 官方文档&lt;/a>。&lt;/p>
&lt;blockquote>
&lt;p>建议：可将安装了所有依赖软件的操作系统制作成系统镜像使用，避免每台机器都安装依赖软件，即可提升交付部署效率，又可避免依赖问题的发生。&lt;/p>&lt;/blockquote>
&lt;blockquote>
&lt;p>提示：如使用 centos7.x、ubuntu18.04，则可以选择使用 kk 命令对机器进行初始化。
解压安装包，并创建好配置文件之后（创建方法请看下文），可执行如下命令对节点进行初始化：
&lt;code>./kk init os -s ./dependencies -f config-example.yaml&lt;/code>
如使用该命令遇到依赖问题，可自行安装相关依赖软件。&lt;/p></description></item><item><title>删除 KubeSphere 中一直卡在 Terminating 的 Namespace</title><link>https://openksc.github.io/zh/blogs/kubesphere-terminating-namespace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-terminating-namespace/</guid><description>&lt;h2 id="介绍">介绍&lt;/h2>
&lt;p>最近一直在玩 EKS（Elastic Kubernetes Service -- Amazon EKS） 和 KubeSphere。 因为之前没有使用过 EKS 和 KubeSphere，所以这个过程也是一个试错的过程，在我使用 KubeSphere 的时候发现有一个日志服务，在好奇心的驱使下，我创建了它。&lt;/p>
&lt;p>在我创建了日志服务（KubeSphere Logging System）以后，我发现我并不想使用它。（可能我只是想看看它到底是什么吧。）强迫症的我就想把它给删除掉。于是我在我的 EKS 中对他进行了强制删除：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl delete ns kubesphere-logging-system --force --grace-period&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>让人尴尬的是，这个 Namespace 并没有立马删除，我自我安慰道，可能 Namespace 下边有其他没有删除的资源在等待删除，我再等等。。。&lt;/p>
&lt;p>过了半个小时，再次查看删除进度：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get ns kubesphere-logging-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STATUS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubesphere-logging-system Terminating 6d19h
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>它好像这地卡在了 Terminating 的状态。我试着寻找解决方法，参考这个 issue： &lt;a href="https://github.com/kubernetes/kubernetes/issues/60807" target="_blank" rel="noopener noreferrer">https://github.com/kubernetes/kubernetes/issues/60807&lt;/a>。但是这种方法要通过 API 才可以实现。EKS 是托管在 AWS 中的，我根本没有办法去操作 EKS 的后台。&lt;/p>
&lt;p>终于我在这个 issue 中找到了答案： &lt;a href="https://github.com/kubernetes/kubernetes/issues/60807#issuecomment-663853215" target="_blank" rel="noopener noreferrer">https://github.com/kubernetes/kubernetes/issues/60807#issuecomment-663853215&lt;/a>&lt;/p>
&lt;h2 id="如何彻底删除-namespace">如何彻底删除 namespace&lt;/h2>
&lt;h3 id="获取-namespace-的详情信息并转为-json">获取 namespace 的详情信息并转为 json&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get namespace kubesphere-logging-system -o json &amp;gt; kubesphere-logging-system.json
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="打开-json-文件编辑">打开 json 文件编辑&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Namespace&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;metadata&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;creationTimestamp&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2021-12-31T05:03:58Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;deletionTimestamp&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2022-01-05T08:05:40Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;labels&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kubesphere.io/namespace&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;kubesphere-logging-system&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kubesphere.io/workspace&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;system-workspace&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;managedFields&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;fieldsType&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;FieldsV1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;fieldsV1&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:metadata&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:labels&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;.&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:kubesphere.io/namespace&amp;#34;&lt;/span>: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:ownerReferences&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;.&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;k:{\&amp;#34;uid\&amp;#34;:\&amp;#34;6d535470-2592-4f3c-a155-eabc362c339d\&amp;#34;}&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;.&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:apiVersion&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:blockOwnerDeletion&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:controller&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:kind&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:name&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:uid&amp;#34;&lt;/span>: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;manager&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;controller-manager&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;operation&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Update&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;time&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2021-12-31T05:04:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;fieldsType&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;FieldsV1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;fieldsV1&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:metadata&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:labels&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:kubesphere.io/workspace&amp;#34;&lt;/span>: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:status&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:phase&amp;#34;&lt;/span>: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;manager&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;kubectl&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;operation&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Update&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;time&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2021-12-31T05:04:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;fieldsType&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;FieldsV1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;fieldsV1&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:status&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:conditions&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;.&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;k:{\&amp;#34;type\&amp;#34;:\&amp;#34;NamespaceContentRemaining\&amp;#34;}&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;.&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:lastTransitionTime&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:message&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:reason&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:status&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:type&amp;#34;&lt;/span>: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;k:{\&amp;#34;type\&amp;#34;:\&amp;#34;NamespaceDeletionContentFailure\&amp;#34;}&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;.&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:lastTransitionTime&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:message&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:reason&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:status&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:type&amp;#34;&lt;/span>: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;k:{\&amp;#34;type\&amp;#34;:\&amp;#34;NamespaceDeletionDiscoveryFailure\&amp;#34;}&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;.&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:lastTransitionTime&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:message&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:reason&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:status&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:type&amp;#34;&lt;/span>: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;k:{\&amp;#34;type\&amp;#34;:\&amp;#34;NamespaceDeletionGroupVersionParsingFailure\&amp;#34;}&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;.&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:lastTransitionTime&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:message&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:reason&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:status&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:type&amp;#34;&lt;/span>: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;k:{\&amp;#34;type\&amp;#34;:\&amp;#34;NamespaceFinalizersRemaining\&amp;#34;}&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;.&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:lastTransitionTime&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:message&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:reason&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:status&amp;#34;&lt;/span>: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;f:type&amp;#34;&lt;/span>: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;manager&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;kube-controller-manager&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;operation&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Update&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;time&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2022-01-05T08:05:47Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;kubesphere-logging-system&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;ownerReferences&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tenant.kubesphere.io/v1alpha1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;blockOwnerDeletion&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;controller&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Workspace&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;system-workspace&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;uid&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;6d535470-2592-4f3c-a155-eabc362c339d&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;resourceVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;7376520&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;uid&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2b76e9b1-75f2-4a2e-a819-73b36aea188e&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;spec&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;finalizers&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;kubernetes&amp;#34;&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">#&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">将此行删除&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;status&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;conditions&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;lastTransitionTime&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2022-01-05T08:05:47Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;message&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;All resources successfully discovered&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;reason&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ResourcesDiscovered&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;False&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;NamespaceDeletionDiscoveryFailure&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;lastTransitionTime&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2022-01-05T08:05:47Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;message&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;All legacy kube types successfully parsed&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;reason&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ParsedGroupVersions&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;False&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;NamespaceDeletionGroupVersionParsingFailure&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;lastTransitionTime&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2022-01-05T08:05:47Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;message&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;All content successfully deleted, may be waiting on finalization&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;reason&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;ContentDeleted&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;False&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;NamespaceDeletionContentFailure&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;lastTransitionTime&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2022-01-05T08:05:47Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;message&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Some resources are remaining: fluentbits.logging.kubesphere.io has 1 resource instances&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;reason&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;SomeResourcesRemain&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;True&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;NamespaceContentRemaining&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;lastTransitionTime&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;2022-01-05T08:05:47Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;message&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Some content in the namespace has finalizers remaining: fluentbit.logging.kubesphere.io in 1 resource instances&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;reason&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;SomeFinalizersRemain&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;True&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;NamespaceFinalizersRemaining&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;phase&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Terminating&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>找到 spec 将 finalizers 下的 kubernetes 删除。&lt;/p></description></item><item><title>深入解读云原生可观测性之日志与告警通知</title><link>https://openksc.github.io/zh/blogs/logging-alert-notifaction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/logging-alert-notifaction/</guid><description>&lt;blockquote>
&lt;p>本文是 KubeSphere 可观测性研发工程师雷万钧在 CIC 大会上分享内容整理而成。点击观看&lt;a href="https://kubesphere.com.cn/live/nm-cic/" target="_blank" rel="noopener noreferrer">视频回放&lt;/a>。&lt;/p>&lt;/blockquote>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>作为可观测性的重要组成部分，告警通知可以帮助我们及时发现问题，日志可以帮助我们快速定位问题。作为一款开源的容器编排平台， KubeSphere 提供了强大的日志收集和查询功能，以及灵活的告警通知能力。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubesphere-feature-overview-9.jpg" alt="">&lt;/p>
&lt;p>本文将为大家介绍 KubeSphere 的日志和通知功能是如何实现的。&lt;/p>
&lt;h2 id="日志">日志&lt;/h2>
&lt;p>KubeSphere 的日志收集是通过 Fluent Bit 实现的，Fluent Bit 将 Pod 日志收集到 ElasticSearch 集群，KubeSphere 通过查询 ElasticSearch 集群实现日志检索。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubesphere-log-console.png" alt="">&lt;/p>
&lt;h3 id="fluent-bit">Fluent Bit&lt;/h3>
&lt;p>Fluent Bit 是一个开源的日志处理器和转发器，它可以从不同来源收集任何数据，如指标和日志，用过滤器处理它们并将它们发送到多个目的地。它是 Kubernetes 等容器化环境的首选。&lt;/p>
&lt;p>Fluent Bit 的设计考虑到了性能：高吞吐量、低 CPU 和内存使用率。它是用 C 语言编写的，具有可插拔架构，支持 70 多种输入、过滤器和输出扩展。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/fluentbit-cic.png" alt="">&lt;/p>
&lt;p>日志通过数据管道从数据源发送到目的地，一个数据管道通常由 Input、Parser、Filter、Buffer、Routing 和 Output组成。&lt;/p>
&lt;ul>
&lt;li>Input：用于从数据源抽取数据，一个数据管道中可以包含多个 Input。&lt;/li>
&lt;li>Parser：负责将 Input 抽取的非结构化数据转化为标准的结构化数据，每个 Input 均可以定义自己的 Parser。&lt;/li>
&lt;li>Filter：负责对格式化数据进行过滤和修改。一个数据管道中可以包含多个 Filter，Filter 会顺序执行，其执行顺序与配置文件中的顺序一致。&lt;/li>
&lt;li>Buffer：用户缓存经过 Filter 处理的数据。&lt;/li>
&lt;li>Routing：将 Buffer 中缓存的数据路由到不同的 Output。&lt;/li>
&lt;li>Output：负责将数据发送到不同的目的地，一个数据管道中可以包含多个 Output。&lt;/li>
&lt;/ul>
&lt;p>Fluent Bit 支持多种类型的 Input、Parser、Filter、Output 插件，可以应对各种场景。&lt;/p></description></item><item><title>深入浅出 Kubernetes 项目网关与应用路由</title><link>https://openksc.github.io/zh/blogs/how-to-use-kubesphere-project-gateways-and-routes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/how-to-use-kubesphere-project-gateways-and-routes/</guid><description>&lt;p>KubeSphere 项目网关与应用路由提供了一种聚合服务的方式，将集群的内部服务通过一个外部可访问的 IP 地址以 HTTP 或 HTTPs 暴露给集群外部。应用路由定义了这些服务的访问规则，用户可以定义基于 host 主机名称和 URL 匹配的规则。同时还可以配置 HTTPs offloading 等选项。项目网关则是应用路由的具体实现，它承载了流量的入口并根据应用路由规则将匹配到的请求转发至集群内的服务。&lt;/p>
&lt;h2 id="整体架构">整体架构&lt;/h2>
&lt;p>用户的服务和应用路由的架构密不可分，因此我们需要结合用户服务来理解项目网关的整体架构。一个典型生产环境中，项目网关架构如下图所示：&lt;/p>
&lt;p>&lt;img src="https://ask.kubesphere.io/forum/assets/files/2021-07-27/1627370451-193428-kubernetes-ingress.png" alt="">&lt;/p>
&lt;p>图中组件共分为四个部分:&lt;/p>
&lt;ol>
&lt;li>&lt;code>Nginx Ingress Controller&lt;/code> 是应用网关的核心组件。KubeSphere 项目网关基于 &lt;code>Nginx Ingress Controller&lt;/code> 实现，它通过获 &lt;code>Ingress&lt;/code> 对象生成 Nginx 反向代理规则配置并配置应用于 Nginx 服务。应用路由是一个 &lt;code>Ingress&lt;/code> 对象。应用网关依赖于 &lt;code>Service&lt;/code> 对外暴露 Nginx 服务，因此 &lt;code>Service&lt;/code> 在生产环境中一般设置为 &lt;code>LoadBalancer&lt;/code> 类型，由云服务商配置其公有云 IP 地址及外部负载均衡器，用以保障服务的高可用性。&lt;/li>
&lt;li>外部负载均衡器，应用网关的 &lt;code>Service&lt;/code> 生成的外部负载均衡器，一般由各个云服务商提供。因此每种负载均衡器的特性有很多差别，比如 SLA、带宽、IP 配置等等。我们一般可以通过服务商提供的注解对其进行配置，在设置网关时，我们通常需要了解这些特性。&lt;/li>
&lt;li>DNS 域名解析服务， 一般由域名服务商提供服务，我们可以配置域名解析纪录将域名指向 &lt;code>LoadBalancer&lt;/code> 的公网 IP。如果子域名也指向同一 IP，我们可以可使用泛域名解析方式。&lt;/li>
&lt;li>用户服务与应用路由，用户需要为应用程序创建 &lt;code>Service&lt;/code> 用于暴露集群内的服务，然后创建应用路由对外暴露服务。注，&lt;code>Nginx Ingress Controller&lt;/code> 并不通过 &lt;code>Kube-proxy&lt;/code> 访问服务 IP。它通过服务查找与之关联 &lt;code>POD&lt;/code> 的 &lt;code>EndPoint&lt;/code>，并将其设置为 &lt;code>Nginx&lt;/code> 的 &lt;code>Upstream&lt;/code>。Nginx 直接连接 &lt;code>POD&lt;/code> 可以避免由 &lt;code>Service&lt;/code> 带来的额外网络开销。&lt;/li>
&lt;/ol>
&lt;h3 id="应用路由-vs-servicetypeloadbalancer">应用路由 vs Service(type=LoadBalancer)&lt;/h3>
&lt;p>在实践过程中，应用路由与 &lt;code>Service&lt;/code> 的应用场景常常令人混淆。它们都可以向集群外暴露集群内服务，并提供负载均衡功能。并且应用路由看起来也是&lt;em>依赖&lt;/em>于服务的，那么他们究竟有何区别呢？这个问题我们需要从以下几个角度理解。&lt;/p></description></item><item><title>使用 Apache APISIX 作为 Kubernetes 的 Ingress Controller</title><link>https://openksc.github.io/zh/blogs/kubesphere-apacheapisix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-apacheapisix/</guid><description>&lt;h2 id="kubesphere-介绍">KubeSphere 介绍&lt;/h2>
&lt;p>KubeSphere 是在 Kubernetes 之上构建的面向云原生应用的系统，完全开源，支持多云与多集群管理，提供全栈的 IT 自动化运维能力，简化企业的 DevOps 工作流。它的架构可以非常方便地使第三方应用与云原生生态组件进行即插即用 (plug-and-play) 的集成。&lt;/p>
&lt;p>作为全栈的多租户容器平台，KubeSphere 提供了运维友好的向导式操作界面，帮助企业快速构建一个强大和功能丰富的容器云平台。KubeSphere 为用户提供构建企业级 Kubernetes 环境所需的多项功能，例如多云与多集群管理、Kubernetes 资源管理、DevOps、应用生命周期管理、微服务治理（服务网格）、日志查询与收集、服务与网络、多租户管理、监控告警、事件与审计查询、存储管理、访问权限控制、GPU 支持、网络策略、镜像仓库管理以及安全管理等。&lt;/p>
&lt;h2 id="apache-apisix-介绍">Apache APISIX 介绍&lt;/h2>
&lt;p>Apache APISIX 是一款开源的高性能、动态云原生网关，由深圳支流科技有限公司于 2019 年捐赠给 Apache 基金会，当前已经成为 Apache 基金会的顶级开源项目，也是 GitHub 上最活跃的网关项目。Apache APISIX 当前已经覆盖了 API 网关，LB，Kubernetes Ingress，Service Mesh 等多种场景。&lt;/p>
&lt;h2 id="前置条件">前置条件&lt;/h2>
&lt;p>将现有 Kubernetes 集群已纳入 KubeSphere 管理。&lt;/p>
&lt;h2 id="部署-apache-apisix-和-apache-apisix-ingress-controller">部署 Apache APISIX 和 Apache APISIX Ingress Controller&lt;/h2>
&lt;p>我们可以参考 KubeSphere 的文档启用 KubeSphere 的 &lt;a href="https://kubesphere.io/docs/pluggable-components/app-store/" target="_blank" rel="noopener noreferrer">AppStore&lt;/a>，或者使用使用 Apache APISIX 的 Helm 仓库来进行部署。这里，我们直接使用 Apache APISIX 的 Helm 仓库进行部署。&lt;/p></description></item><item><title>使用 Cilium 作为网络插件部署 Kubernetes + KubeSphere</title><link>https://openksc.github.io/zh/blogs/cilium-kubesphere-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/cilium-kubesphere-kubernetes/</guid><description>&lt;h2 id="cilium-简介">Cilium 简介&lt;/h2>
&lt;p>Cilium 是一个用于容器网络领域的开源项目，主要是面向容器而使用，用于提供并透明地保护应用程序工作负载（如应用程序容器或进程）之间的网络连接和负载均衡。&lt;/p>
&lt;p>Cilium 在第 3/4 层运行，以提供传统的网络和安全服务，还在第 7 层运行，以保护现代应用协议（如 HTTP, gRPC 和 Kafka）的使用。 Cilium 被集成到常见的容器编排框架中，如 Kubernetes 和 Mesos。&lt;/p>
&lt;p>Cilium 的底层基础是 BPF，Cilium 的工作模式是生成内核级别的 BPF 程序与容器直接交互。区别于为容器创建 overlay 网络，Cilium 允许每个容器分配一个 IPv6 地址（或者 IPv4 地址），使用容器标签而不是网络路由规则去完成容器间的网络隔离。它还包含创建并实施 Cilium 规则的编排系统的整合。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/cilium.png" alt="">&lt;/p>
&lt;blockquote>
&lt;p>以上简介来源于 oschina&lt;/p>&lt;/blockquote>
&lt;h2 id="系统要求">系统要求&lt;/h2>
&lt;p>Linux Kernel &amp;gt;= 4.9.17
更多信息请查看 &lt;a href="https://docs.cilium.io/en/v1.9/operations/system_requirements/" target="_blank" rel="noopener noreferrer">Cilium 系统要求&lt;/a>&lt;/p>
&lt;h2 id="环境">环境&lt;/h2>
&lt;p>以一台 Ubuntu Server 20.04.1 LTS 64bit 为例&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>name&lt;/th>
 &lt;th>ip&lt;/th>
 &lt;th>role&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>node1&lt;/td>
 &lt;td>10.160.6.136&lt;/td>
 &lt;td>etcd, master, worker&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h2 id="下载安装包">下载安装包&lt;/h2>
&lt;pre tabindex="0">&lt;code>sudo wget https://github.com/kubesphere/kubekey/releases/download/v1.1.0/kubekey-v1.1.0-linux-64bit.deb
&lt;/code>&lt;/pre>&lt;h2 id="使用-cilium-作为网络插件部署-kubesphere">使用 cilium 作为网络插件部署 KubeSphere&lt;/h2>
&lt;ol>
&lt;li>安装 KubeKey&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>sudo dpkg -i kubekey-v1.1.0-linux-64bit.deb
&lt;/code>&lt;/pre>&lt;ol start="2">
&lt;li>生成配置文件&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>sudo kk create config --with-kubernetes v1.19.8
&lt;/code>&lt;/pre>&lt;ol start="3">
&lt;li>修改配置文件，将网络插件修改为 cilium&lt;/li>
&lt;/ol>
&lt;p>注意将 spec.network.plugin 的值修改为 &lt;strong>cilium&lt;/strong>&lt;/p></description></item><item><title>使用 frp 接入远程 macOS 物理机进行流水线构建</title><link>https://openksc.github.io/zh/blogs/add-remote-macos-physical-machine-to-run-pipelines-using-frp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/add-remote-macos-physical-machine-to-run-pipelines-using-frp/</guid><description>&lt;blockquote>
&lt;p>本文同样适用于接入 ARM、MIPS 架构，FreeBSD、Windows 系统的物理机，如果 Jenkins 能连上构建机，可以跳过 Frp 部分，直接增加物理节点。&lt;/p>&lt;/blockquote>
&lt;h2 id="遇到的问题">遇到的问题&lt;/h2>
&lt;p>在以 Kubernetes 为基础设施的场景下，Jenkins 构建流水线时，将为每一条流水线单独创建一个 Pod 用于构建。Pod 中的容器环境可以根据需要自定义设置，扩展非常方便，能够满足绝大多数的需求。&lt;/p>
&lt;p>其中有一个特例，那就是构建苹果生态链的应用，例如 IOS、macOS 应用。由于没有 macOS 的容器镜像，只能采用物理机进行构建。还有一种方式是，将 macOS 安装在虚拟机中，将虚拟机接入 Jenkins 进行构建，当然也可以直接导入其他人共享的 macOS VM 。&lt;/p>
&lt;p>这都会遇到一个问题，那就是 Jenkins Master 无法直接访问 macOS 系统，网络不通，无法添加 macOS 的构建节点。&lt;/p>
&lt;p>本篇主要是以 Frp 作为穿透工具，打通网络，对 IOS 应用、macOS 应用提供 Jenkins 流水线构建的解决方案。&lt;/p>
&lt;h2 id="解决方案">解决方案&lt;/h2>
&lt;p>如下图，通过 Frp 可以打通 Jenkins 与物理机之间的网络。&lt;/p>
&lt;p>&lt;img src="../../../images/blogs/add-remote-macos-physical-machine-to-run-pipelines-using-frp/add-mac-to-jenkins.png" alt="">&lt;/p>
&lt;ul>
&lt;li>第一步，需要将 Frp 的 Server 端部署到 Jenkins Master 可以直接访问的环境上，这些环境包括物理机、VM、容器环境。&lt;/li>
&lt;li>第二步，在 Mac 物理机上运行 Frp Client ，将 macOS 的 SSH 服务暴露在 Frp Server 上。&lt;/li>
&lt;li>第三步，在 Jenkins 上添加 macOS 节点，使用 Label 选择 Mac 机器进行构建。&lt;/li>
&lt;/ul>
&lt;h2 id="配置相关组件">配置相关组件&lt;/h2>
&lt;h3 id="macos-系统配置">macOS 系统配置&lt;/h3>
&lt;p>下图是我测试的 macOS 系统版本:&lt;/p></description></item><item><title>使用 GPU-Operator 与 KubeSphere 简化深度学习训练与监控 GPU</title><link>https://openksc.github.io/zh/blogs/gpu-operator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/gpu-operator/</guid><description>&lt;p>本文将从 GPU-Operator 概念介绍、安装部署、深度训练测试应用部署，以及在 KubeSphere 使用自定义监控面板对接 GPU 监控，从原理到实践，逐步浅析介绍与实践 GPU-Operator。&lt;/p>
&lt;h2 id="gpu-operator简介">GPU-Operator简介&lt;/h2>
&lt;p>众所周知，Kubernetes 平台通过设备插件框架提供对特殊硬件资源的访问，如 NVIDIA GPU、网卡、Infiniband 适配器和其他设备。然而，使用这些硬件资源配置和管理节点需要配置多个软件组件，如驱动程序、容器运行时或其他依赖库，这是比较困难的和容易出错的。&lt;/p>
&lt;p>NVIDIA GPU Operator 由 Nvidia 公司开源，利用了 Kubernetes 的 Operator 控制模式，方便地自动化集成管理 GPU 所需的 NVIDIA 设备组件，有效地解决了上述GPU设备集成的痛点。这些组件包括 NVIDIA 驱动程序(用于启用 CUDA )、用于 GPU 的 Kubernetes 设备插件、NVIDIA Container 运行时、自动节点标签、基于 DCGM 的监控等。&lt;/p>
&lt;p>NVIDIA GPU Operator 的不仅实现了设备和组件一体化集成，而且它管理 GPU 节点就像管理 CPU 节点一样方便，无需单独为 GPU 节点提供特殊的操作系统。值得关注的是，它将 GPU 各组件容器化，提供 GPU 能力，非常适合快速扩展和管理规模 GPU 节点。当然，对于已经为 GPU 组件构建了特殊操作系统的应用场景来说，显得并不是那么合适了。&lt;/p>
&lt;h3 id="gpu-operator-架构原理">GPU-Operator 架构原理&lt;/h3>
&lt;p>前文提到，NVIDIA GPU Operator 管理 GPU 节点就像管理 CPU 节点一样方便，那么它是如何实现这一能力呢？&lt;/p>
&lt;p>我们一起来看看 GPU-Operator 运行时的架构图：&lt;/p></description></item><item><title>使用 KubeEdge 和 EdgeMesh 实现边缘复杂网络场景下的节点通信</title><link>https://openksc.github.io/zh/blogs/kubesphere-kubeedge-edgemesh/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-kubeedge-edgemesh/</guid><description>&lt;blockquote>
&lt;p>作者：吴波，来自北京的运维工程师，专注于云原生，KubeSphere 狂热爱好者。&lt;/p>&lt;/blockquote>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>KubeEdge 是面向边缘计算场景、专为边云协同设计的业界首个云原生边缘计算框架，在 K8s 原生的容器编排调度能力之上实现了边云之间的应用协同、资源协同、数据协同和设备协同等能力，完整打通了边缘计算中云、边、设备协同的场景。其中 KubeEdge 架构主要包含云边端三部分：&lt;/p>
&lt;ul>
&lt;li>云上是统一的控制面，包含原生的 K8s 管理组件，以及 KubeEdge 自研的 CloudCore 组件，负责监听云端资源的变化，提供可靠和高效的云边消息同步。&lt;/li>
&lt;li>边侧主要是 EdgeCore 组件，包含 Edged、MetaManager、EdgeHub 等模块，通过接收云端的消息，负责容器的生命周期管理。&lt;/li>
&lt;li>端侧主要是 device mapper 和 eventBus，负责端侧设备的接入。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/fa9d0a79-b3fa-49d4-917d-18ad80db4e6d.png" alt="">&lt;/p>
&lt;h3 id="底层逻辑">底层逻辑&lt;/h3>
&lt;p>KubeEdge 是 K8s 在边缘场景下的延伸。目标是将 K8s 对容器编排的能力延伸到边缘上；
KubeEdge 主要包含两个组件，云端的 CloudCore 和边缘节点上 EdgeCore，同时还有一个 Device 模块，用于管理海量的边缘设备。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/dd3d2f20-66e5-4a46-89dc-a34794124673.png" alt="">&lt;/p>
&lt;h3 id="kubeedge-功能组件">KubeEdge 功能组件&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>&lt;a href="https://kubeedge.io/zh/docs/architecture/edge/edged" title="Edged" target="_blank" rel="noopener noreferrer">Edged&lt;/a>:&lt;/strong> 在边缘节点上运行并管理容器化应用程序的代理。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://kubeedge.io/zh/docs/architecture/edge/edgehub" title="EdgeHub" target="_blank" rel="noopener noreferrer">EdgeHub&lt;/a>:&lt;/strong> Web 套接字客户端，负责与 Cloud Service 进行交互以进行边缘计算（例如 KubeEdge 体系结构中的 Edge Controller）。这包括将云侧资源更新同步到边缘，并将边缘侧主机和设备状态变更报告给云。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://kubeedge.io/zh/docs/architecture/cloud/cloudhub" title="CloudHub" target="_blank" rel="noopener noreferrer">CloudHub&lt;/a>:&lt;/strong> Web 套接字服务器，负责在云端缓存信息、监视变更，并向 EdgeHub 端发送消息。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://kubeedge.io/zh/docs/architecture/cloud/edge_controller" title="EdgeController" target="_blank" rel="noopener noreferrer">EdgeController&lt;/a>:&lt;/strong> kubernetes 的扩展控制器，用于管理边缘节点和 pod 的元数据，以便可以将数据定位到对应的边缘节点。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://kubeedge.io/zh/docs/architecture/edge/eventbus" title="EventBus" target="_blank" rel="noopener noreferrer">EventBus&lt;/a>:&lt;/strong> 一个与 MQTT 服务器（mosquitto）进行交互的 MQTT 客户端，为其他组件提供发布和订阅功能。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://kubeedge.io/zh/docs/architecture/edge/devicetwin" title="DeviceTwin" target="_blank" rel="noopener noreferrer">DeviceTwin&lt;/a>:&lt;/strong> 负责存储设备状态并将设备状态同步到云端。它还为应用程序提供查询接口。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://kubeedge.io/zh/docs/architecture/edge/metamanager" title="MetaManager" target="_blank" rel="noopener noreferrer">MetaManager&lt;/a>:&lt;/strong> Edged 端和 EdgeHub 端之间的消息处理器。它还负责将元数据存储到轻量级数据库（SQLite）或从轻量级数据库（SQLite）检索元数据。&lt;/li>
&lt;/ul>
&lt;h2 id="kubeedge">KubeEdge&lt;/h2>
&lt;p>为了更好的支持 KubeEdge 并提供可视化界面管理边缘节点，本文档使用 KubeSphere 平台用来管理边缘节点，&lt;a href="https://kubesphere.com.cn/docs/v3.3/" title="KubeSphere 官方文档" target="_blank" rel="noopener noreferrer">KubeSphere 官方文档&lt;/a>。&lt;/p></description></item><item><title>使用 KubeKey v3.1.1 离线部署原生 Kubernetes v1.28.8 实战</title><link>https://openksc.github.io/zh/blogs/using-kubekey-v3.1.1-deploy-k8s-v1.28.8-offline/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/using-kubekey-v3.1.1-deploy-k8s-v1.28.8-offline/</guid><description>&lt;p>本文我将为大家实战演示，如何基于操作系统 &lt;strong>openEuler 22.03 LTS SP3&lt;/strong>，利用 KubeKey 制作 Kubernetes 离线安装包，并实战离线部署 &lt;strong>Kubernetes v1.28.8&lt;/strong> 集群。&lt;/p>
&lt;p>&lt;strong>实战服务器配置 (架构 1:1 复刻小规模生产环境，配置略有不同)&lt;/strong>&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">离线环境 k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">离线环境 k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">离线环境 k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-registry&lt;/td>
 &lt;td style="text-align: center">192.168.9.90&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">离线环境部署节点和镜像仓库节点&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-deploy&lt;/td>
 &lt;td style="text-align: center">192.168.9.89&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">联网主机用于制作离线包&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">5&lt;/td>
 &lt;td style="text-align: center">32&lt;/td>
 &lt;td style="text-align: center">64&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">500&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>实战环境涉及软件版本信息&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>操作系统：&lt;strong>openEuler 22.03 LTS SP3&lt;/strong>&lt;/li>
&lt;li>Kubernetes：&lt;strong>v1.28.8&lt;/strong>&lt;/li>
&lt;li>KubeKey: &lt;strong>v3.1.1&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="1-制作离线部署资源">1. 制作离线部署资源&lt;/h2>
&lt;p>本文增加了一台能联网的 &lt;strong>ksp-deploy&lt;/strong> 节点，在该节点下载 KubeKey 最新版（&lt;strong>v3.1.1&lt;/strong>），用来制作离线部署资源包。&lt;/p></description></item><item><title>使用 KubeKey 安装部署 Kubernetes 与 Kube-OVN</title><link>https://openksc.github.io/zh/blogs/use-kubekey-to-install-and-deploy-kubernetes-and-kubeovn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/use-kubekey-to-install-and-deploy-kubernetes-and-kubeovn/</guid><description>&lt;blockquote>
&lt;p>作者简介：林瑞超，锐捷网络开发工程师， KubeSphere 社区 contributor， 关注 Kube-OVN, Cilium 等容器网络相关技术&lt;/p>&lt;/blockquote>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>KubeKey 是 KubeSphere v3.0 新增的安装方式，用户可以一键部署 Kubernetes 和 KubeSphere。Kube-OVN 是一款基于 OVS/OVN 的 Kubernetes 网络编排系统。本文将为大家介绍如何使用 KubeKey 来安装部署 Kubernetes 和 Kube-OVN。&lt;/p>
&lt;h3 id="kubekey-简介">KubeKey 简介&lt;/h3>
&lt;p>KubeKey 是 Kubernetes 和 KubeSphere 的新一代 Installer（安装程序），旨在更方便、快速、高效和灵活地安装 Kubernetes 与 KubeSphere。KubeKey 摒弃了原来 Ansible 带来的依赖问题，用 Go 重写，支持单独 Kubernetes 或整体安装 KubeSphere。它也是扩展和升级集群的有效工具。&lt;/p>
&lt;h3 id="kube-ovn-简介">Kube-OVN 简介&lt;/h3>
&lt;p>Kube-OVN 是一款开源企业级云原生 Kubernetes 容器网络编排系统，它通过将 OpenStack 领域成熟的网络功能平移到 Kubernetes，极大增强了 Kubernetes 容器网络的安全性、可运维性、管理性和性能。在上个月 Kube-OVN 加入了 CNCF Sandbox。&lt;/p>
&lt;h2 id="准备工作">准备工作&lt;/h2>
&lt;ol>
&lt;li>满足 KubeKey 的安装条件&lt;/li>
&lt;li>满足 Kube-OVN 的&lt;a href="https://github.com/alauda/kube-ovn/wiki/%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C" target="_blank" rel="noopener noreferrer">安装条件&lt;/a>(主要是内核版本需要满足要求)&lt;/li>
&lt;/ol>
&lt;h2 id="安装步骤">安装步骤&lt;/h2>
&lt;ol>
&lt;li>下载 KubeKey&lt;/li>
&lt;/ol>
&lt;p>如果能正常访问 GitHub/Googleapis，可以从 GitHub &lt;a href="https://github.com/kubesphere/kubekey/releases" target="_blank" rel="noopener noreferrer">发布页面&lt;/a>下载 KubeKey 或直接使用以下命令。&lt;/p></description></item><item><title>使用 KubeKey 快速离线部署 KubeSphere 集群</title><link>https://openksc.github.io/zh/blogs/deploying-kubesphere-clusters-offline-with-kubekey/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deploying-kubesphere-clusters-offline-with-kubekey/</guid><description>&lt;h2 id="一kubekey-介绍">一、KubeKey 介绍&lt;/h2>
&lt;p>KubeKey（以下简称 KK）是一个用于部署 Kubernetes 集群的开源轻量级工具。它提供了一种灵活、快速、便捷的方式来仅安装 Kubernetes/K3s，或同时安装 Kubernetes/K3s 和 KubeSphere，以及其他云原生插件。除此之外，它也是扩展和升级集群的有效工具。&lt;/p>
&lt;p>KubeKey v2.0.0 版本新增了清单（manifest）和制品（artifact）的概念，为用户离线部署 Kubernetes 集群提供了一种解决方案。在过去，用户需要准备部署工具，镜像 tar 包和其他相关的二进制文件，每位用户需要部署的 Kubernetes 版本和需要部署的镜像都是不同的。现在使用 KK，用户只需使用清单 manifest 文件来定义将要离线部署的集群环境需要的内容，再通过该 manifest 来导出制品 artifact 文件即可完成准备工作。离线部署时只需要 KK 和 artifact 就可快速、简单的在环境中部署镜像仓库和 Kubernetes 集群。&lt;/p>
&lt;h2 id="二部署准备">二、部署准备&lt;/h2>
&lt;h3 id="1-资源清单">1. 资源清单&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>名称&lt;/th>
 &lt;th>数量&lt;/th>
 &lt;th>用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>KubeSphere 3.2.1&lt;/td>
 &lt;td>1&lt;/td>
 &lt;td>源集群打包使用&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>服务器&lt;/td>
 &lt;td>2&lt;/td>
 &lt;td>离线环境部署使用&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h3 id="2-源集群中下载解压-kk-200-rc-3">2. 源集群中下载解压 KK 2.0.0-rc-3&lt;/h3>
&lt;p>说明：由于 KK 版本不断更新请按照 GitHub 上最新 Releases 版本为准&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ wget https://github.com/kubesphere/kubekey/releases/download/v2.0.0-rc.3/kubekey-v2.0.0-rc.3-linux-amd64.tar.gz
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ tar -zxvf kubekey-v2.0.0-rc.3-linux-amd64.tar.gz 
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="3-源集群中使用-kk-创建-manifest">3. 源集群中使用 KK 创建 manifest&lt;/h3>
&lt;p>说明：manifest 就是一个描述当前 Kubernetes 集群信息和定义 artifact 制品中需要包含哪些内容的文本文件。目前有两种方式来生成该文件：&lt;/p></description></item><item><title>使用 Kubernetes 部署高可用 RocketMQ 集群</title><link>https://openksc.github.io/zh/blogs/rocketmq-k8s/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/rocketmq-k8s/</guid><description>&lt;blockquote>
&lt;p>作者：老Z，云原生爱好者，目前专注于云原生运维，KubeSphere Ambassador。&lt;/p>&lt;/blockquote>
&lt;p>Spring Cloud Alibaba 全家桶之 RocketMQ 是一款典型的分布式架构下的消息中间件产品，使用异步通信方式和发布订阅的消息传输模型。&lt;/p>
&lt;p>很多基于 Spring Cloud 开发的项目都喜欢采用 RocketMQ 作为消息中间件。&lt;/p>
&lt;p>RocketMQ 常用的部署模式如下：&lt;/p>
&lt;ul>
&lt;li>单 Master 模式&lt;/li>
&lt;li>多 Master 无 Slave 模式&lt;/li>
&lt;li>多 Master 多 Slave 模式-异步复制&lt;/li>
&lt;li>多 Master 多 Slave 模式-同步双写&lt;/li>
&lt;/ul>
&lt;p>更多的部署方案详细信息可以参考&lt;a href="https://gitee.com/apache/rocketmq/blob/master/docs/cn/operation.md#%e8%bf%90%e7%bb%b4%e7%ae%a1%e7%90%86" title="官方文档" target="_blank" rel="noopener noreferrer">官方文档&lt;/a>。&lt;/p>
&lt;p>本文重点介绍 单 Master 模式和多 Master 多 Slave-异步复制模式在 K8s 集群上的部署方案。&lt;/p>
&lt;h3 id="单-master-模式">单 Master 模式&lt;/h3>
&lt;p>这种部署方式风险较大，仅部署一个 NameServer 和一个 Broker，一旦 Broker 重启或者宕机时，会导致整个服务不可用，不建议线上生产环境使用，仅可以用于开发和测试环境。&lt;/p>
&lt;p>部署方案参考官方&lt;a href="https://github.com/apache/rocketmq-docker" title="rocketmq-docker" target="_blank" rel="noopener noreferrer">rocketmq-docker&lt;/a>项目中使用的容器化部署方案涉及的镜像、启动方式、定制化配置。&lt;/p>
&lt;h3 id="多-master-多-slave-异步复制模式">多 Master 多 Slave-异步复制模式&lt;/h3>
&lt;p>每个 Master 配置一个 Slave，有多对 Master-Slave，HA 采用异步复制方式，主备有短暂消息延迟（毫秒级），这种模式的优缺点如下：&lt;/p>
&lt;ul>
&lt;li>优点：即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，同时 Master 宕机后，消费者仍然可以从 Slave 消费，而且此过程对应用透明，不需要人工干预，性能同多 Master 模式几乎一样；&lt;/li>
&lt;li>缺点：Master 宕机，磁盘损坏情况下会丢失少量消息。&lt;/li>
&lt;/ul>
&lt;p>多 Master 多 Slave-异步复制模式适用于生产环境，部署方案采用官方提供的 &lt;a href="https://github.com/apache/rocketmq-operator#rocketmq-operator" title="RocketMQ Operator" target="_blank" rel="noopener noreferrer">RocketMQ Operator&lt;/a>。&lt;/p></description></item><item><title>使用 KubeSphere DevOps 搭建自动化测试系统</title><link>https://openksc.github.io/zh/blogs/devops-automatic-testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/devops-automatic-testing/</guid><description>&lt;h2 id="测试分层">测试分层&lt;/h2>
&lt;p>测试的目的是为了验证预期的功能，发现潜在的缺陷。测试增强了交付合格产品的信心，也给敏捷迭代带来了可能。可以说，测试决定了产品的开发进度。&lt;/p>
&lt;p>网络模型有七层的 OSI 、四层的 TCP，而开发模式有 MTV、MVC、MVP、MVVM 等。高内聚、低耦合，划分职责、分模块、分层。然后结构化、标准化，技术逐步走向成熟。&lt;/p>
&lt;p>测试也分为，UI 测试、API 测试、单元测试。测试并不是一项新技术，更多是产出与成本的一种平衡。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200428175258.png" alt="">&lt;/p>
&lt;p>如上图，是一个测试金字塔。越往上，需要的成本越高，对环境要求越高，执行时间越长，维护越麻烦，但更贴近终端用户的场景。在 《Google软件测试之道》中，按照谷歌的经验，各层测试用例比例是 70：20：10，也就是 70% 的单元测试，20% 的 API 测试，10% 的 UI 测试。&lt;/p>
&lt;p>本篇主要讲的是如何在 KubeSphere 平台上使用 &lt;strong>KubeSphere DevOps 系统&lt;/strong> 运行自动化测试。&lt;/p>
&lt;h2 id="什么是-kubesphere-devops">什么是 KubeSphere DevOps&lt;/h2>
&lt;p>KubeSphere 针对容器与 Kubernetes 的应用场景，基于 Jenkins 提供了一站式 DevOps 系统，包括丰富的 CI/CD 流水线构建与插件管理功能，还提供 Binary-to-Image（B2I）、Source-to-Image（S2I），为流水线、S2I、B2I 提供代码依赖缓存支持，以及代码质量管理与流水线日志等功能。&lt;/p>
&lt;p>KubeSphere 内置的 DevOps 系统将应用的开发和自动发布与容器平台进行了很好的结合，还支持对接第三方的私有镜像仓库和代码仓库形成完善的私有场景下的 CI/CD，提供了端到端的用户体验。&lt;/p>
&lt;p>但是，很少有用户知道，KubeSphere DevOps 还可以用来搭建自动化测试系统，为自动化的&lt;strong>单元测试、API 测试和 UI 测试&lt;/strong>带来极大的便利性，提高测试人员的工作效率。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200428215435.png" alt="">&lt;/p>
&lt;h2 id="单元测试">单元测试&lt;/h2>
&lt;p>单元测试的运行频率非常高，每次提交代码都应该触发一次。单元测试的依赖少，通常只需要一个容器运行环境即可。&lt;/p>
&lt;p>下面是一个使用 golang:latest 跑单元测试的例子。&lt;/p>
&lt;pre tabindex="0">&lt;code>pipeline {
 agent {
 node {
 label &amp;#39;go&amp;#39;
 }
 }
 stages {
 stage(&amp;#39;testing&amp;#39;) {
 steps {
 container(&amp;#39;go&amp;#39;) {
 sh &amp;#39;&amp;#39;&amp;#39;
 git clone https://github.com/etcd-io/etcd.git
 cd etcd
 make test
 &amp;#39;&amp;#39;&amp;#39;
 }

 }
 }
 }
}
&lt;/code>&lt;/pre>&lt;p>执行日志：&lt;/p></description></item><item><title>使用 KubeSphere 和极狐GitLab 打造云原生持续交付系统</title><link>https://openksc.github.io/zh/blogs/kubesphere-jh-gitlab/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-jh-gitlab/</guid><description>&lt;h2 id="kubesphere-简介">KubeSphere 简介&lt;/h2>
&lt;p>Kubernetes 是一个非常复杂的容器编排平台，学习成本非常高，KubeSphere 所做的事情就是高度产品化和抽象了底层 Kubernetes，是一个面向云原生的操作系统。讲得再通俗一点，&lt;strong>Kubernetes 屏蔽了底层容器运行时的差异，而 KubeSphere 则屏蔽了底层 Kubernetes 集群的差异，它解决了 K8s 使用门槛高和云原生生态工具庞杂的痛点&lt;/strong>。你可以在可视化界面上点几下鼠标即可将 Pod 调度到集群的不同节点中，无需编写 YAML。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202205201209062.png" alt="">&lt;/p>
&lt;p>上面是 KubeSphere 的功能架构，可以看到 KubeSphere 包含了非常多的应用场景，比如微服务、DevOps、应用管理、可观测性和安全等，每一个场景生态下面都包含了很多对研发和运维人员比较友好的组件，而且所有的组件都是可插拔的，用户可以根据自己的意愿自由选择启用哪个组件。&lt;/p>
&lt;p>从 v4.0 开始，KubeSphere 会提供前后端可插拔的架构和框架，&lt;strong>任何第三方合作伙办和 ISV 都可以基于 KubeSphere 4.0 开放框架，开发扩展自己想要的功能插件，这些功能插件与 KubeSphere 有完全一致的 UI 体验，形成更强大的应用生态&lt;/strong>。这就好比 Mac OS 和 App Store 之间的关系一样，任何企业或团队都可以发布自己开发的插件到应用商店，灵活满足各类用户的需求，在社区与 KubeSphere 合作互利共赢。&lt;/p>
&lt;h2 id="极狐gitlab-简介">极狐GitLab 简介&lt;/h2>
&lt;p>极狐GitLab 是一个一体化的 DevOps 平台，可以简单理解为 GitLab 在国内的“发行版”。是由极狐(GitLab)公司推出的产品（极狐(GitLab)公司是以“中外合资3.0”模式成立的公司，在国内独立运营，为国内用户提供适合本土化的 DevOps 平台以及支持服务）。&lt;/p>
&lt;p>极狐GitLab 是开源的，任何人都可以参与开源共建，代码托管在极狐GitLab SaaS 上：https://jihulab.com/gitlab-cn/gitlab。其提供的一体化 DevOps 能力覆盖软件开发全生命周期（从计划到运维），同时内置了安全功能，能够利用开箱即用的安全能力构建 DevSecOps 体系。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202205201727215.png" alt="">&lt;/p>
&lt;p>更重要的一点是，极狐GitLab 支持自建（私有部署）和 SaaS 两种服务。在私有部署的时候，支持多种安装方式，其中就包括云原生的安装方式，因此，结合 KubeSphere 和极狐GitLab，可以打造出一个适应云原生时代的持续交付系统。&lt;/p>
&lt;h2 id="在-kubesphere-上安装极狐gitlab-和-runner">在 KubeSphere 上安装极狐GitLab 和 Runner&lt;/h2>
&lt;p>目前在 KubeSphere 上部署极狐GitLab 非常便利，只需要利用 KubeSphere 的&lt;strong>应用商店&lt;/strong>即可一键部署。&lt;/p></description></item><item><title>使用 KubeSphere 实现微服务的灰度发布</title><link>https://openksc.github.io/zh/blogs/kubesphere--microservice-grayrelease/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere--microservice-grayrelease/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>今天来说一说，在 KubeSphere 中两个 &amp;quot; 小姐姐 &amp;quot; 如何来回切换，这是什么意思哩？其实就是互联网产品中常用的灰度发布方式。&lt;/p>
&lt;p>互联网产品需要快速迭代上线，既要保证新功能运行正常，又要保证质量，一旦出现问题可以很快控制局面，就需要设计一套灰度发布系统。用大白话讲就是某个 APP 的新版本已经开发完成了，而老版本用户正在正常使用着，这个时候要是直接上线新版本，那么所有的用户都会用新版本，但是这种情况下，一旦出现问题，将导致所有的用户都不可用，所以会有策略的挑选一部分用户先用新版本，即使出现问题，也只是一小部分用户，方便回滚到旧版本，提升用户良好的体验性。&lt;/p>
&lt;h2 id="概述">概述&lt;/h2>
&lt;p>灰度发布（又名金丝雀发布）是指在黑与白之间，能够平滑过渡的一种发布方式。在其上可以进行 A/B testing，即让一部分用户继续用产品特性 A，一部分用户开始用产品特性 B，如果用户对 B 没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到 B 上面来。灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度。&lt;/p>
&lt;p>我们假设这个 A/B，就是 A 小姐姐和 B 小姐姐。&lt;/p>
&lt;h2 id="kubesphere-的微服务治理功能">KubeSphere 的微服务治理功能&lt;/h2>
&lt;p>KubeSphere 基于 Istio 微服务框架提供可视化的微服务治理功能，如果您在 Kubernetes 上运行和伸缩微服务，您可以为您的分布式系统配置基于 Istio 的微服务治理功能。KubeSphere 提供统一的操作界面，便于您集成并管理各类工具，包括 Istio、Envoy 和 Jaeger 等。&lt;/p>
&lt;p>&lt;strong>流量治理&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>金丝雀发布&lt;/strong>提供灵活的灰度策略，将流量按照所配置的比例转发至当前不同的灰度版本&lt;/li>
&lt;li>&lt;strong>蓝绿部署&lt;/strong>支持零宕机部署，让应用程序可以在独立的环境中测试新版本的功能和特性&lt;/li>
&lt;li>&lt;strong>流量镜像&lt;/strong>模拟生产环境，将实时流量的副本发送给被镜像的服务&lt;/li>
&lt;li>&lt;strong>熔断机制&lt;/strong>支持为服务设置对单个主机的调用限制&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>在 KubeSphere 中应用治理可以以可插拔式方式开启。开启后如下：&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/abdf6330-d75d-4fc2-9039-179af7030b9e.png" alt="">&lt;/p>
&lt;h2 id="准备工作">准备工作&lt;/h2>
&lt;p>&lt;strong>创建一个 SpringBoot 的项目用于测试，如下 pom.xml 文件：&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt;

&amp;lt;project xmlns=&amp;#34;http://maven.apache.org/POM/4.0.0&amp;#34; xmlns:xsi=&amp;#34;http://www.w3.org/2001/XMLSchema-instance&amp;#34;
 xsi:schemaLocation=&amp;#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&amp;#34;&amp;gt;
 &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;

 &amp;lt;groupId&amp;gt;com.pkulaw&amp;lt;/groupId&amp;gt;
 &amp;lt;artifactId&amp;gt;ServiceA&amp;lt;/artifactId&amp;gt;
 &amp;lt;version&amp;gt;1.0-SNAPSHOT&amp;lt;/version&amp;gt;

 &amp;lt;name&amp;gt;ServiceA&amp;lt;/name&amp;gt;

 &amp;lt;parent&amp;gt;
 &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
 &amp;lt;artifactId&amp;gt;spring-boot-starter-parent&amp;lt;/artifactId&amp;gt;
 &amp;lt;version&amp;gt;2.7.0&amp;lt;/version&amp;gt;
 &amp;lt;relativePath/&amp;gt; &amp;lt;!-- lookup parent from repository --&amp;gt;
 &amp;lt;/parent&amp;gt;

 &amp;lt;properties&amp;gt;
 &amp;lt;project.build.sourceEncoding&amp;gt;UTF-8&amp;lt;/project.build.sourceEncoding&amp;gt;
 &amp;lt;java.version&amp;gt;1.8&amp;lt;/java.version&amp;gt;
 &amp;lt;docker.image.prefix&amp;gt;springboot&amp;lt;/docker.image.prefix&amp;gt;
 &amp;lt;/properties&amp;gt;

 &amp;lt;dependencies&amp;gt;
 &amp;lt;dependency&amp;gt;
 &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
 &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt;
 &amp;lt;/dependency&amp;gt;
 &amp;lt;dependency&amp;gt;
 &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
 &amp;lt;artifactId&amp;gt;spring-boot-starter-test&amp;lt;/artifactId&amp;gt;
 &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
 &amp;lt;/dependency&amp;gt;

 &amp;lt;!-- 引入Actuator监控依赖 --&amp;gt;
 &amp;lt;dependency&amp;gt;
 &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
 &amp;lt;artifactId&amp;gt;spring-boot-starter-actuator&amp;lt;/artifactId&amp;gt;
 &amp;lt;/dependency&amp;gt;

 &amp;lt;dependency&amp;gt;
 &amp;lt;groupId&amp;gt;org.projectlombok&amp;lt;/groupId&amp;gt;
 &amp;lt;artifactId&amp;gt;lombok&amp;lt;/artifactId&amp;gt;
 &amp;lt;/dependency&amp;gt;

 &amp;lt;dependency&amp;gt;
 &amp;lt;groupId&amp;gt;cn.hutool&amp;lt;/groupId&amp;gt;
 &amp;lt;artifactId&amp;gt;hutool-all&amp;lt;/artifactId&amp;gt;
 &amp;lt;version&amp;gt;5.8.0&amp;lt;/version&amp;gt;
 &amp;lt;/dependency&amp;gt;
 &amp;lt;/dependencies&amp;gt;

 &amp;lt;build&amp;gt;
 &amp;lt;finalName&amp;gt;ServiceA&amp;lt;/finalName&amp;gt;
 &amp;lt;plugins&amp;gt;
 &amp;lt;plugin&amp;gt;
 &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
 &amp;lt;artifactId&amp;gt;spring-boot-maven-plugin&amp;lt;/artifactId&amp;gt;
 &amp;lt;/plugin&amp;gt;
 &amp;lt;/plugins&amp;gt;
 &amp;lt;/build&amp;gt;
&amp;lt;/project&amp;gt;
&lt;/code>&lt;/pre>&lt;p>&lt;strong>controller 代码：&lt;/strong>&lt;/p></description></item><item><title>使用 KubeSphere 在 Kubernetes 安装 cert-manager 为网站启用 HTTPS</title><link>https://openksc.github.io/zh/blogs/install-cert-managner-on-k8s/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/install-cert-managner-on-k8s/</guid><description>&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200428230558.png" alt="cert-manager">&lt;/p>
&lt;h2 id="什么是-cert-manager">什么是 cert-manager&lt;/h2>
&lt;p>cert-manager（&lt;code>https://cert-manager.io/&lt;/code>）是 Kubernetes 原生的证书管理控制器。它可以帮助从各种来源颁发证书，例如 Let's Encrypt，HashiCorp Vault，Venafi，简单的签名密钥对或自签名。它将确保证书有效并且是最新的，并在证书到期前尝试在配置的时间续订证书。它大致基于 kube-lego 的原理，并从其他类似项目（例如 kube-cert-manager）中借鉴了一些智慧。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200428230220.png" alt="">&lt;/p>
&lt;h3 id="准备工作">准备工作&lt;/h3>
&lt;ol>
&lt;li>需要一个公网可访问的 IP，例如 &lt;code>139.198.121.121&lt;/code>&lt;/li>
&lt;li>需要一个域名，并且已经解析到到对应的IP，例如 &lt;code> A kubesphere.io 139.198.121.121&lt;/code>，我们将 &lt;code>staging.kubesphere.io&lt;/code> 域名解析到了 &lt;code>139.198.121.121&lt;/code>&lt;/li>
&lt;li>在KubeSphere上已经运行网站对应的服务，例如本例中的&lt;code>ks-console&lt;/code>&lt;/li>
&lt;/ol>
&lt;h3 id="启用项目网关">启用项目网关&lt;/h3>
&lt;p>登录 KubeSphere，进入任意一个企业空间下的项目中。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200428224301.png" alt="">&lt;/p>
&lt;p>在 KubeSphere 中启用对应项目下的网关。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200428223939.png" alt="">&lt;/p>
&lt;blockquote>
&lt;p>我们开启的是一个NodePort类型的网关，需要在集群外部使用 LoadBalancer 转发到网关的端口，将 &lt;code>139.198.121.121&lt;/code> 绑定到 LoadBalancer 上，这样我们就可以通过公网IP直接访问我们的服务了；
如果 Kubernetes 集群是在物理机上，可以安装 Porter（&lt;code>https://porter.kubesphere.io&lt;/code>）负载均衡器对外暴露集群服务；
如果在公有云上，可以安装和配置公有云支持的负载均衡器插件，然后创建 LoadBalancer 类型的网关，填入公网IP对应的 eip，会自动创建好负载均衡器，并将端口转发到网关。&lt;/p>&lt;/blockquote>
&lt;h3 id="安装-cert-manager">安装 cert-manager&lt;/h3>
&lt;p>详细安装文档可以参考 &lt;a href="https://docs.cert-manager.io/en/latest/getting-started/install/kubernetes.html" target="_blank" rel="noopener noreferrer">cert-manager&lt;/a>。&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>cert-manager&lt;/code> 部署时会创建一个 &lt;code>webhook&lt;/code> 来校验 &lt;code>cert-manager&lt;/code> 相关对象是否符合格式，不过也会增加部署的复杂性。这里我们使用官方提供的一个 &lt;code>no-webhook&lt;/code> 版本安装。&lt;/p>&lt;/blockquote>
&lt;p>可以在 KubeSphere 右下角的工具箱中，打开 Web Kubectl。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200428225544.png" alt="">&lt;/p>
&lt;p>在 Web Kubectl 执行下列命令安装 cert-manager：&lt;/p></description></item><item><title>使用 Notification Manager 构建云原生通知系统</title><link>https://openksc.github.io/zh/blogs/notification-manager-system/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/notification-manager-system/</guid><description>&lt;h2 id="云原生下通知系统的特点">云原生下通知系统的特点&lt;/h2>
&lt;p>众所周知，在云原生领域，K8s 已经成为了事实上的标准。那么狭义上讲，云原生通知系统就可以理解为 K8s 下的通知系统，基于此，云原生下的通知系统拥有以下特点：&lt;/p>
&lt;ul>
&lt;li>为 Kubernetes 而生&lt;/li>
&lt;li>支持对接多种的事件源，告警、事件、审计等&lt;/li>
&lt;li>多租户设计&lt;/li>
&lt;li>高并发，可以同时处理大量的通知&lt;/li>
&lt;/ul>
&lt;h2 id="notification-manager-是什么">Notification Manager 是什么&lt;/h2>
&lt;p>&lt;a href="https://github.com/kubesphere/notification-manager" target="_blank" rel="noopener noreferrer">Notification Manager&lt;/a> 是 KubeSphere 开源的一款云原生多租户通知管理系统，支持从 Kubernetes 接收告警、事件、审计，根据用户设置的模板生成通知消息并推送给用户，支持将通知消息推送到 DingTalk，Email，Feishu，Pushover，Slack，WeCom，Webhook，短信平台（阿里、腾讯、华为）等。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/notification-manager-101.png" alt="">&lt;/p>
&lt;p>Notification Manager 会接收来自 Promethus、Alertmanager 发出的警告消息、K8s 产生的审计消息以及 K8s 本身的事件。在规划中我们还会实现接入 Promethus 的告警消息和接入 Cloud Event 。消息在进入缓存之后，会经过静默、抑制、路由、过滤、聚合，最后进行实际的通知，并记录在历史中。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/notification-manager-architecture.jpeg" alt="">&lt;/p>
&lt;p>下面是对每个步骤的解析：&lt;/p>
&lt;h3 id="缓存cache">缓存（Cache）&lt;/h3>
&lt;p>缓存的作用有两个，一个是应对告警风暴，同时产生大量告警时减少对系统的影响；另一个是对告警事件做聚合处理，飞书、钉钉和企业微信，对访问频次有限制。&lt;/p>
&lt;p>目前采用松耦合设计，支持内存、NATS Streamming（后续）。&lt;/p>
&lt;h3 id="静默silence">静默（Silence）&lt;/h3>
&lt;p>我们可以根据设定的规则对告警、事件、审计进行静默处理 ，比如支持设置静默时间，让晚上十点到早上八点不进行通知。同时我们可以支持设置全局静默规则和租户静默规则，全局的规则的会被全部租户启用，而租户规则只对当前租户起作用。&lt;/p>
&lt;h3 id="抑制inhibit">抑制（Inhibit）&lt;/h3>
&lt;p>一个节点宕机之后会发送大量告警，而这些告警不利于我们排查原因，我们可以通过设置抑制规则不再发送这部分告警给用户。&lt;/p>
&lt;h3 id="路由route">路由（Route）&lt;/h3>
&lt;p>告警、事件、审计都是由一个个标签组成的，路由的本质就是根据标签寻找需要接收标签的接收器。&lt;/p>
&lt;p>下图就是一个路由，他定义了一个路由，他有以下几部分组成：&lt;/p>
&lt;ul>
&lt;li>标签选择器 alertSelector&lt;/li>
&lt;li>接收器 Receiver&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/notification-manager-102.png" alt="">&lt;/p>
&lt;p>接收器定义了我们的通知往哪里发、如何发，有如下特点：&lt;/p>
&lt;ul>
&lt;li>可复用。尽可能复用配置，减少用户运维成本。&lt;/li>
&lt;li>多租户。支持定义全局接收器用于接收所有的通知，同时用户可以定义自己的通知接收器用于接收租户相关的通知。&lt;/li>
&lt;li>动态加载。为了实现动态加载，我们采用 CRD 的模式定义了 Config 和 Receiver。&lt;/li>
&lt;/ul>
&lt;p>Config CRD 定义了发送通知需要的信息，比如钉钉的 APP secret，它来定义我们如何发送信息，还可以通过标签区分全局 Config 和租户 Config 。&lt;/p></description></item><item><title>使用 Prometheus 在 KubeSphere 上监控 KubeEdge 边缘节点（Jetson） CPU、GPU 状态</title><link>https://openksc.github.io/zh/blogs/using-prometheus-monitor-jetson-edge/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/using-prometheus-monitor-jetson-edge/</guid><description>&lt;blockquote>
&lt;p>作者：朱亚光，之江实验室工程师，云原生/开源爱好者。&lt;/p>&lt;/blockquote>
&lt;h2 id="kubesphere-边缘节点的可观测性">KubeSphere 边缘节点的可观测性&lt;/h2>
&lt;p>在边缘计算场景下，KubeSphere 基于 KubeEdge 实现应用与工作负载在云端与边缘节点的统一分发与管理，解决在海量边、端设备上完成应用交付、运维、管控的需求。&lt;/p>
&lt;p>根据 KubeSphere 的&lt;a href="https://kubesphere.io/zh/docs/v3.3/installing-on-linux/introduction/kubekey/#%e6%94%af%e6%8c%81%e7%9f%a9%e9%98%b5" target="_blank" rel="noopener noreferrer">支持矩阵&lt;/a>，只有 1.23.x 版本的 K8s 支持边缘计算，而且 KubeSphere 界面也没有边缘节点资源使用率等监控信息的显示。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/image-20240407141223475.png" alt="">&lt;/p>
&lt;p>本文基于 KubeSphere 和 KubeEdge 构建云边一体化计算平台，通过 Prometheus 来监控 Nvidia Jetson 边缘设备状态，实现 KubeSphere 在边缘节点的可观测性。&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>组件&lt;/th>
 &lt;th>版本&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>KubeSphere&lt;/td>
 &lt;td>3.4.1&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>containerd&lt;/td>
 &lt;td>1.7.2&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>K8s&lt;/td>
 &lt;td>1.26.0&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>KubeEdge&lt;/td>
 &lt;td>1.15.1&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>Jetson 型号&lt;/td>
 &lt;td>NVIDIA Jetson Xavier NX (16GB ram)&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>Jtop&lt;/td>
 &lt;td>4.2.7&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>JetPack&lt;/td>
 &lt;td>5.1.3-b29&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>Docker&lt;/td>
 &lt;td>24.0.5&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h2 id="部署-k8s-环境">部署 K8s 环境&lt;/h2>
&lt;p>参考 &lt;a href="https://kubesphere.io/zh/docs/v3.4/quick-start/all-in-one-on-linux/" target="_blank" rel="noopener noreferrer">KubeSphere 部署文档&lt;/a>。通过 KubeKey 可以快速部署一套 K8s 集群。&lt;/p>
&lt;pre tabindex="0">&lt;code>// all in one 方式部署一台 单 master 的 k8s 集群

./kk create cluster --with-kubernetes v1.26.0 --with-kubesphere v3.4.1 --container-manager containerd
&lt;/code>&lt;/pre>&lt;h2 id="部署-kubeedge-环境">部署 KubeEdge 环境&lt;/h2>
&lt;p>参考 &lt;a href="https://zhuyaguang.github.io/kubeedge-install/" target="_blank" rel="noopener noreferrer">在 KubeSphere 上部署最新版的 KubeEdge&lt;/a>，部署 KubeEdge。&lt;/p></description></item><item><title>使用 x509-certificate-exporter 监控 Kubernetes 集群组件的证书</title><link>https://openksc.github.io/zh/blogs/x509-certificate-exporter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/x509-certificate-exporter/</guid><description>&lt;p>KubeSphere 虽然提供了运维友好的向导式操作界面，简化了 Kubernetes 的运维操作，但它还是建立在底层 Kubernetes 之上的，Kubernetes 默认的证书有效期都是一年，即使使用 &lt;a href="https://github.com/kubesphere/kubekey" target="_blank" rel="noopener noreferrer">KubeKey&lt;/a> 这样的集群安装利器也不能改变这个结果。如果不想办法对 Kubernetes 各个组件的证书有效期进行监控，说不定哪天就会掉进坑里。&lt;/p>
&lt;p>有部分读者可能听说过 &lt;a href="https://github.com/ribbybibby/ssl_exporter" target="_blank" rel="noopener noreferrer">ssl-exporter&lt;/a> 这个项目，它能提供多种针对 SSL 的检测手段，包括：HTTPS 证书、文件证书、Kubernetes Secret、Kubeconfig 文件。从功能上来看，它基本可以满足上述需求，但它的指标还不够丰富，本文将介绍一个更为强大的 Prometheus Exporter：&lt;a href="https://github.com/enix/x509-certificate-exporter" target="_blank" rel="noopener noreferrer">x509-certificate-exporter&lt;/a>。&lt;/p>
&lt;p>与 ssl-exporter 不同，x509-certificate-exporter 只专注于监控 Kubernetes 集群相关的证书，包括各个组件的文件证书、Kubernetes TLS Secret、Kubeconfig 文件，而且指标更加丰富。我们来看看在 KubeSphere 中如何部署 x509-certificate-exporter 以监控集群的所有证书。&lt;/p>
&lt;h2 id="准备-kubesphere-应用模板">准备 KubeSphere 应用模板&lt;/h2>
&lt;p>&lt;a href="https://kubesphere.com.cn" target="_blank" rel="noopener noreferrer">KubeSphere&lt;/a> 集成了 &lt;a href="https://github.com/openpitrix/openpitrix" target="_blank" rel="noopener noreferrer">OpenPitrix&lt;/a> 来提供应用程序全生命周期管理，OpenPitrix 是一个多云应用管理平台，KubeSphere 利用它实现了应用商店和应用模板，以可视化的方式部署并管理应用。对于应用商店中不存在的应用，用户可以将 Helm Chart 交付至 KubeSphere 的公共仓库，或者导入私有应用仓库来提供应用模板。&lt;/p>
&lt;p>本教程将使用 KubeSphere 的应用模板来部署 x509-certificate-exporter。&lt;/p>
&lt;p>要想从应用模板部署应用，需要创建一个企业空间、一个项目和两个用户帐户（&lt;code>ws-admin&lt;/code> 和 &lt;code>project-regular&lt;/code>）。&lt;code>ws-admin&lt;/code> 必须被授予企业空间中的 &lt;code>workspace-admin&lt;/code> 角色， &lt;code>project-regular&lt;/code> 必须被授予项目中的 &lt;code>operator&lt;/code> 角色。在创建之前，我们先来回顾一下 KubeSphere 的多租户架构。&lt;/p>
&lt;h3 id="多租户架构">多租户架构&lt;/h3>
&lt;p>KubeSphere 的多租户系统分&lt;strong>三个&lt;/strong>层级，即集群、企业空间和项目。KubeSphere 中的项目等同于 Kubernetes 的&lt;a href="https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/namespaces/" target="_blank" rel="noopener noreferrer">命名空间&lt;/a>。&lt;/p></description></item><item><title>手把手从零部署与运营生产级的 Kubernetes 集群与 KubeSphere</title><link>https://openksc.github.io/zh/blogs/kubernetes-kubesphere-ha/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubernetes-kubesphere-ha/</guid><description>&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200327191533.png" alt="">&lt;/p>
&lt;blockquote>
&lt;p>本文来自 KubeSphere 社区用户 &lt;strong>Liu_wt&lt;/strong> 投稿，欢迎所有社区用户参与投稿或分享经验案例。&lt;/p>&lt;/blockquote>
&lt;p>本文将从零开始，在干净的机器上安装 Docker、Kubernetes (使用 kubeadm)、Calico、Helm 与 KubeSphere，通过手把手的教程演示如何搭建一个高可用生产级的 Kubernetes，并在 Kubernetes 集群之上安装 KubeSphere 容器平台可视化运营集群环境。&lt;/p>
&lt;h2 id="一准备环境">一、准备环境&lt;/h2>
&lt;p>开始部署之前，请先确定当前满足如下条件，本次集群搭建，所有机器处于同一内网网段，并且可以互相通信。&lt;/p>
&lt;p>⚠️⚠️⚠️：&lt;strong>请详细阅读第一部分，后面的所有操作都是基于这个环境的，为了避免后面部署集群出现各种各样的问题，强烈建议你完全满足第一部分的环境要求&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;ul>
&lt;li>两台以上主机&lt;/li>
&lt;li>每台主机的主机名、Mac 地址、UUID 不相同&lt;/li>
&lt;li>CentOS 7（本文用 7.6/7.7）&lt;/li>
&lt;li>每台机器最好有 2G 内存或以上&lt;/li>
&lt;li>Control-plane/Master至少 2U 或以上&lt;/li>
&lt;li>各个主机之间网络相通&lt;/li>
&lt;li>禁用交换分区&lt;/li>
&lt;li>禁用 SELINUX&lt;/li>
&lt;li>关闭防火墙（我自己的选择，你也可以设置相关防火墙规则）&lt;/li>
&lt;li>Control-plane/Master和Worker节点分别开放如下端口&lt;/li>
&lt;/ul>&lt;/blockquote>
&lt;p>&lt;strong>Master节点&lt;/strong>&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>协议&lt;/th>
 &lt;th>方向&lt;/th>
 &lt;th>端口范围&lt;/th>
 &lt;th>作用&lt;/th>
 &lt;th>使用者&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>TCP&lt;/td>
 &lt;td>入站&lt;/td>
 &lt;td>6443*&lt;/td>
 &lt;td>Kubernetes API 服务器&lt;/td>
 &lt;td>所有组件&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>TCP&lt;/td>
 &lt;td>入站&lt;/td>
 &lt;td>2379-2380&lt;/td>
 &lt;td>etcd server client API&lt;/td>
 &lt;td>kube-apiserver, etcd&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>TCP&lt;/td>
 &lt;td>入站&lt;/td>
 &lt;td>10250&lt;/td>
 &lt;td>Kubelet API&lt;/td>
 &lt;td>kubelet 自身、控制平面组件&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>TCP&lt;/td>
 &lt;td>入站&lt;/td>
 &lt;td>10251&lt;/td>
 &lt;td>kube-scheduler&lt;/td>
 &lt;td>kube-scheduler 自身&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>TCP&lt;/td>
 &lt;td>入站&lt;/td>
 &lt;td>10252&lt;/td>
 &lt;td>kube-controller-manager&lt;/td>
 &lt;td>kube-controller-manager 自身&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Worker节点&lt;/strong>&lt;/p></description></item><item><title>探索 Kubernetes 持久化存储之 Longhorn 初窥门径</title><link>https://openksc.github.io/zh/blogs/kubernetes-longhorn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubernetes-longhorn/</guid><description>&lt;p>在 Kubernetes 生态系统中，持久化存储扮演着至关重要的角色，它是支撑业务应用稳定运行的基石。对于那些选择自建 Kubernetes 集群的运维架构师而言，选择合适的后端持久化存储解决方案是一项至关重要的选型决策。目前 Ceph、GlusterFS、NFS、openEBS 等解决方案已被广泛采用。&lt;/p>
&lt;p>为了丰富我们的技术栈，并为未来的容器云平台设计持久化存储提供更多灵活性和选择性。今天，我将跟大家一起探索，如何将 Longhorn 集成至 KubeSphere 管理的 Kubernetes 集群。&lt;/p>
&lt;p>本文核心内容概览：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Longhorn 持久化存储选型说明：&lt;/strong> 聊一聊 Longhorn 初体验的感想&lt;/li>
&lt;li>&lt;strong>Longhorn 存储服务如何部署：&lt;/strong> 如果利用 Helm 安装 Longhorn&lt;/li>
&lt;li>&lt;strong>实战演示&lt;/strong>：创建测试资源，体验 Longhorn 的效果。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>实战服务器配置(架构1:1复刻小规模生产环境，配置略有不同)&lt;/strong>&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ksp-registry&lt;/td>
 &lt;td style="text-align: center">192.168.9.90&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Harbor 镜像仓库&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.94&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker/CI&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">400+&lt;/td>
 &lt;td style="text-align: center">Containerd、OpenEBS、ElasticSearch/Longhorn/Ceph/NFS&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.98&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">300+&lt;/td>
 &lt;td style="text-align: center">Containerd、OpenEBS、ElasticSearch/Longhorn/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.99&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">300+&lt;/td>
 &lt;td style="text-align: center">Containerd、OpenEBS、ElasticSearch/Longhorn/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gpu-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.101&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker(GPU NVIDIA Tesla M40 24G)&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gpu-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.102&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker(GPU NVIDIA Tesla P100 16G)&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gateway-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.103&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;td style="text-align: center">自建应用服务代理网关/VIP：192.168.9.100&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gateway-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.104&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;td style="text-align: center">自建应用服务代理网关/VIP：192.168.9.100&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-mid&lt;/td>
 &lt;td style="text-align: center">192.168.9.105&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">部署在 k8s 集群之外的服务节点（Gitlab 等）&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">15&lt;/td>
 &lt;td style="text-align: center">56&lt;/td>
 &lt;td style="text-align: center">152&lt;/td>
 &lt;td style="text-align: center">600&lt;/td>
 &lt;td style="text-align: center">2100+&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>实战环境涉及软件版本信息&lt;/strong>&lt;/p></description></item><item><title>网络隔离的最小配置</title><link>https://openksc.github.io/zh/blogs/minimum-configuration-for-network-isolation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/minimum-configuration-for-network-isolation/</guid><description>&lt;blockquote>
&lt;p>作者：任云康，青云科技研发工程师&lt;/p>&lt;/blockquote>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>对于项目下的网络隔离，有用户提出了以下疑问：&lt;/p>
&lt;ul>
&lt;li>网络隔离是针对 Pod 的吗？&lt;/li>
&lt;li>网络隔离的最小配置是什么？
&lt;ul>
&lt;li>配置后，哪些是可以访问的，哪些是不可以访问的？&lt;/li>
&lt;li>通过 Ingress 暴露、LB 类型的 Service 暴露、NodePort 类型的 Service 暴露的流量的具体链路是什么样的？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="kubesphere-网络策略的实现思路">KubeSphere 网络策略的实现思路&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/20240425-1.png" alt="">&lt;/p>
&lt;p>KubeSphere 对于 NetworkPolicy 的集成中主要包含：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>集群网络策略管理：主要提供一个原生的 NetworkPolicy 资源的管理，当 KubeSphere 租户网络隔离无法满足用户的全部需求时，可以在此通过 yaml 管理原生的 NetworkPolicy。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>租户网络隔离管理：分为企业空间网络隔离和项目空间的网络隔离。&lt;/p>
&lt;ul>
&lt;li>当开启企业空间网络隔离时，会自动为该企业空间的所有项目创建一条只允许本企业空间访问的入站规则的网络策略，默认不限制出站流量；&lt;/li>
&lt;li>当开启项目网络隔离时，会自动为该项目创建一条只允许项目访问的入站规则的网络策略；&lt;/li>
&lt;li>项目网络隔离下可以配置白名单列表，内部白名单允许当前项目中的容器组与当前企业空间其他项目中的服务进行通信，外部白名单允许当前项目中的容器组与企业空间外部的特定网段和端口进行通信；&lt;/li>
&lt;li>默认不限制出站流量；如果配置出站白名单，那只会放行白名单上的出站项。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>出站：即对本项目下的 pod 是否可以访问本项目外的 pod/ip/port 的限制。&lt;/p>
&lt;p>入站：即对本项目外的 pod/ip 是否可以访问本项目下的 pod 所提供的服务的限制。&lt;/p>
&lt;p>注意：NetworkPolicy 是由具体的 CNI 来实现，KubeSphere 做了 UI 化的管理，同时封装了一个项目级别的 NetworkPolicy，用作项目空间的网络隔离。&lt;/p>&lt;/blockquote>
&lt;p>因此，对前文的问题，我们有了答案：&lt;/p>
&lt;ol>
&lt;li>网络隔离是针对与 pod 的，而项目网络隔离会匹配本项目下的所有 pod；也可以认为此处的网络隔离是针对项目的。&lt;/li>
&lt;li>服务通过 Ingress、NodePort、LoadBalancer 暴露，表明 service 要给集群外提供服务，如果使用 KubeSphere 项目网络隔离进行管理的话，需要配置外部白名单。&lt;/li>
&lt;/ol>
&lt;h2 id="配置">配置&lt;/h2>
&lt;p>当我们通过 NodePort、LoadBalancer 暴露 Kubernetes 的 service 时，kube-proxy 会创建相应的 ipvs 或 iptables 规则来转发流量。然而，当外部流量进入集群并根据这些规则被转发时，如果目标 pod 不在本地节点上，就会进行一次源网络地址转换（SNAT），这将导致 TCP 包中的源 IP 地址被替换为节点 IP 或 overlay 封装接口的 IP，从而丢失了客户端的原始 IP。如果目标 pod 在本地节点上，可能会直接转发到本机 pod，此时会保留客户端 IP。&lt;/p></description></item><item><title>微服务进阶之路 容器落地避坑指南</title><link>https://openksc.github.io/zh/blogs/microservice-blog/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/microservice-blog/</guid><description>&lt;p>微服务架构相对于单体架构有很大的变化，也产生了一些新的设计模式，比如 sidecar，如何开发一个微服务应用是一件有很大挑战性的事情，我们经常会听到有人讨论如何划分微服务，多细的颗粒度才是微服务等问题。初学者经常会处于一个“忐忑不安”的状态，所以我们急需要知道如何才能走上正确的微服务道路，或者需要一些最佳实践指导我们如何设计、开发一个微服务应用。&lt;/p>
&lt;h2 id="不骄不躁不跟风-知己知彼方可百战不殆">不骄不躁不跟风 知己知彼方可百战不殆&lt;/h2>
&lt;p>虽然现在已经进入到一个不谈微服务就落伍的时代，但作为 IT 从业者，我们一定要站在切身利益出发，多思考几个“为什么”，不要急于跟风。原因很简单，不管外面如何风吹雨打，只要你的房子足够结实、安全、舒服，那一般情况下就不需要拆除重建，所以在决定继续沿用单体架构还是转向微服务架构之前，我们一定要做两件事情：&lt;/p>
&lt;h3 id="第一件事从外部了解两种架构各自的优劣">第一件事，从外部了解两种架构各自的优劣：&lt;/h3>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20190930125405.png" alt="">&lt;/p>
&lt;p>可以看到，单体应用并不是一无是处。&lt;/p>
&lt;h3 id="第二件事审视我们自己的业务">第二件事，审视我们自己的业务：&lt;/h3>
&lt;ul>
&lt;li>上述单体架构列出的一些问题是否已经严重影响了我们的业务？&lt;/li>
&lt;li>企业新的业务系统是否要满足快速迭代、弹性等需求？&lt;/li>
&lt;li>团队内是否有 DevOps 氛围？&lt;/li>
&lt;li>企业内是否有足够的动力和技术储备去接触新的技术？&lt;/li>
&lt;/ul>
&lt;p>了解了单体应用和微服务应用的优劣特点，分析了企业自身的业务诉求和实际情况，最终还是决定转型微服务架构，那么我们也要清楚这不是一朝一夕的事情，需要分阶段逐步推进。&lt;/p>
&lt;h2 id="蒙眼狂奔不可取-循序渐进方可顺利进阶">蒙眼狂奔不可取 循序渐进方可顺利进阶&lt;/h2>
&lt;h3 id="第一阶段试炼-开发新应用">第一阶段试炼—— 开发新应用&lt;/h3>
&lt;p>对于初次接触微服务的企业，选择新应用入手是正确的方式。&lt;/p>
&lt;p>第一步可以选择 web-scale、无状态类型的新应用上手，比如基于 nginx 的网站、文档等，这类应用非常简单且容易实现，而且能体验到微服务在容器平台上的各种功能。&lt;/p>
&lt;p>有了一定的经验之后，第二步就可以开发有状态类型的新应用，有状态服务的最大挑战就是数据管理。&lt;/p>
&lt;p>敲重点，跟以往单体应用的共享数据库不同，微服务应用中的每一个服务“独享”自己的数据库，服务之间需要通过 API、事件或消息传递的方式来相互访问对方的数据，而不是通过直接访问对方数据库的方式。&lt;/p>
&lt;p>换句话说，理想中的微服务是封装自己的数据，通过API暴露数据出去，从而避免数据耦合，这样每个微服务的数据格式发生变化也不影响其它微服务的数据调用。开发过和升级过大型企业单体应用的人对此会深有体会，一旦有人改变了数据库 schema，整个应用都有可能启动不起来，团队开发效率会大大降低。&lt;/p>
&lt;p>&lt;strong>微服务架构并不尽善尽美，适合自己的方案才是王道。&lt;/strong>&lt;/p>
&lt;p>不难理解，微服务数据是牺牲强一致性而通过最终一致性的方式来管理，这对数据的划分带来很大难度，比如不能再用 join 的方式访问不同服务之间的数据表，实际当中也比较难做到或者做起来很麻烦，现在也没有成熟且好用的库或框架提供微服务的数据管理，而且某些应用确实需要强一致性。&lt;/p>
&lt;p>&lt;strong>而此时，我们不能通盘否定此类应用微服务化的可行性，应该适当折中或“妥协”，采用 miniservice。&lt;/strong>&lt;/p>
&lt;p>Miniservice 在开发与部署的独立性和敏捷性方面类似于微服务(microservice)，但没有微服务那么强的约束。通常情况下，一个 miniservcie 可以提供多个功能，这些功能之间可以共享数据库。这个时候千万不要害怕混合架构，不要害怕自己的微服务应用是否“正统”，“think big，start small，move fast“才是我们应该遵循的哲学。&lt;/p>
&lt;p>因此，一个企业应用里既有 microservice 也有 miniservice，甚至有单体部分（可以称之为 macroservice）都是可以接受的。&lt;/p>
&lt;p>以一个电商平台举例，在整个场景里面，业务开发人员面对的主要压力来自前端频繁的变动，因为要应对频繁的促销、推广、降价等活动，所以面对消费者最前端的业务需要快速迭代。消费者会不停的浏览商品，最终产生交易的请求数量要远低于获取商品信息的请求数量，因此将前端业务无状态化，进行微服务拆分、解耦，便可以快速应对市场变化，灵活做出改变。&lt;/p>
&lt;p>那是不是把整个平台都做到微服务级别会变得更好？答案是“不确定”，因为当微服务量级到达一定程度，由此产生的管理和运维压力是指数级增长的。而实际上，对于有些业务来讲也没有必要微服务化，比如很多电商平台都有 2B 的业务，其业务变化的频度和压力没有 2C 那么大，那以 macroservices 或者 miniservices 的方式去交付也是可以的。&lt;/p>
&lt;p>&lt;strong>开发人员应该分析在整个应用架构体系中，哪些适合微服务化，哪些亟需微服务化。&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20190930125610.png" alt="">&lt;/p>
&lt;h2 id="实践出真知">实践出真知&lt;/h2>
&lt;p>在上面的电商案例中，我们提到了服务无状态化，之所以期望服务无状态化，是因为无状态应用可以做到快速的扩缩容，可以应对井喷流量，可以最大效率的利用计算资源。&lt;/p>
&lt;p>我们经常听到，以无状态为荣，以有状态为耻，说的就是对于一个服务要尽量无状态化它，比如用户 session 管理，以前我们在业务逻辑模块进行管理，导致这些模块不能按照无状态方式任意伸缩。我们可以把这些 session 的管理抽取出来放到一个高可用或分布式的缓存中管理，业务模块通过调用API的方式去获取 session，这样就实现了这些模块的无状态化。&lt;/p></description></item><item><title>修复 K8s SSL/TLS 漏洞（CVE-2016-2183）指南</title><link>https://openksc.github.io/zh/blogs/kubesphere-ssl-tls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-ssl-tls/</guid><description>&lt;blockquote>
&lt;p>作者：老 Z，运维架构师，云原生爱好者，目前专注于云原生运维，云原生领域技术栈涉及 Kubernetes、KubeSphere、DevOps、OpenStack、Ansible 等。&lt;/p>&lt;/blockquote>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>&lt;img src="https://znotes-1258881081.cos.ap-beijing.myqcloud.com/k8s-on-kubesphere/image-20230215145009515.png" alt="内容导图">&lt;/p>
&lt;h3 id="测试服务器配置">&lt;strong>测试服务器配置&lt;/strong>&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">zdeops-master&lt;/td>
 &lt;td style="text-align: center">192.168.9.9&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Ansible 运维控制节点&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">storage-node-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">storage-node-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">storage-node-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">harbor&lt;/td>
 &lt;td style="text-align: center">192.168.9.89&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Harbor&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">22&lt;/td>
 &lt;td style="text-align: center">84&lt;/td>
 &lt;td style="text-align: center">320&lt;/td>
 &lt;td style="text-align: center">2800&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h3 id="测试环境涉及软件版本信息">&lt;strong>测试环境涉及软件版本信息&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>操作系统：&lt;strong>CentOS-7.9-x86_64&lt;/strong>&lt;/li>
&lt;li>Ansible：&lt;strong>2.8.20&lt;/strong>&lt;/li>
&lt;li>KubeSphere：&lt;strong>3.3.0&lt;/strong>&lt;/li>
&lt;li>Kubernetes：&lt;strong>v1.24.1&lt;/strong>&lt;/li>
&lt;li>GlusterFS：&lt;strong>9.5.1&lt;/strong>&lt;/li>
&lt;li>ElasticSearch：&lt;strong>7.17.5&lt;/strong>&lt;/li>
&lt;li>Harbor：&lt;strong>2.5.1&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>生产环境 KubeSphere 3.3.0 部署的 Kubernetes 集群在安全评估的时候发现安全漏洞，其中一项漏洞提示 &lt;strong>SSL/TLS 协议信息泄露漏洞 (CVE-2016-2183)&lt;/strong>。&lt;/p></description></item><item><title>修复 KubeSphere 内置 Jenkins 的 Apache Log4j2 漏洞</title><link>https://openksc.github.io/zh/blogs/kubesphere-jenkins-log4j2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-jenkins-log4j2/</guid><description>&lt;blockquote>
&lt;p>作者：老 Z，运维架构师，云原生爱好者，目前专注于云原生运维，云原生领域技术栈涉及 Kubernetes、KubeSphere、DevOps、OpenStack、Ansible 等。&lt;/p>&lt;/blockquote>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/image-20230210141327867.png" alt="">&lt;/p>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>生产环境 KubeSphere 3.3.0 部署的 Kubernetes 集群在安全评估的时候发现安全漏洞，其中一项漏洞提示&lt;strong>目标可能存在 Apache Log4j2 远程代码执行漏洞 (CVE-2021-44228)&lt;/strong>。&lt;/p>
&lt;p>本文记录了该漏洞修复的全部过程，文中介绍了修复该漏洞的两种解决方案，其中涉及自定义构建 KubeSphere 适用的 Jenkins Image 的详细操作。&lt;/p>
&lt;h2 id="漏洞修复方案">漏洞修复方案&lt;/h2>
&lt;h3 id="漏洞详细信息">漏洞详细信息&lt;/h3>
&lt;p>漏洞报告中涉及漏洞 &lt;strong>目标可能存在 Apache Log4j2 远程代码执行漏洞 (CVE-2021-44228)&lt;/strong> 的具体信息如下：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/image-20230208151708221.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/image-20230208110325840.png" alt="">&lt;/p>
&lt;h3 id="漏洞分析">漏洞分析&lt;/h3>
&lt;ol>
&lt;li>分析漏洞报告信息，发现对应的服务端口为 &lt;strong>30180&lt;/strong>，对应的服务为 &lt;strong>Jenkins&lt;/strong>。使用 Curl 访问服务端口，查看返回头信息。&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">[&lt;/span>root@ks-k8s-master-0 ~&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e"># curl -I http://192.168.9.91:30180&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>HTTP/1.1 &lt;span style="color:#ae81ff">403&lt;/span> Forbidden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Date: Thu, &lt;span style="color:#ae81ff">09&lt;/span> Feb &lt;span style="color:#ae81ff">2023&lt;/span> 00:36:45 GMT
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X-Content-Type-Options: nosniff
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Set-Cookie: JSESSIONID.b1c3bc24&lt;span style="color:#f92672">=&lt;/span>node084x6l5z2ss0ghsb2t9tde2gl16558.node0; Path&lt;span style="color:#f92672">=&lt;/span>/; HttpOnly
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Expires: Thu, &lt;span style="color:#ae81ff">01&lt;/span> Jan &lt;span style="color:#ae81ff">1970&lt;/span> 00:00:00 GMT
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Content-Type: text/html;charset&lt;span style="color:#f92672">=&lt;/span>utf-8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X-Hudson: 1.395
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X-Jenkins: 2.319.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X-Jenkins-Session: 1fde6067
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Content-Length: &lt;span style="color:#ae81ff">541&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Server: Jetty&lt;span style="color:#f92672">(&lt;/span>9.4.43.v20210629&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>说明：&lt;/strong> 从结果中可以看到 KubeSphere 3.3.0 采用的 Jenkins 使用的 Jetty 版本为 &lt;strong>9.4.43.v20210629&lt;/strong>，跟漏扫报告中的结果一致。&lt;/p></description></item><item><title>一文搞定 KubeKey 3.1.1 离线部署 KubeSphere 3.4.1 和 Kubernetes v1.28</title><link>https://openksc.github.io/zh/blogs/deploy-kubesphere-3.4.1-and-k8s-v1.28-with-kubekey-3.1.1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deploy-kubesphere-3.4.1-and-k8s-v1.28-with-kubekey-3.1.1/</guid><description>&lt;p>本文将详细介绍，如何基于操作系统 &lt;strong>openEuler 22.03 LTS SP3&lt;/strong>，利用 KubeKey 制作 KubeSphere 和 Kubernetes 离线安装包，并实战部署 &lt;strong>KubeSphere 3.4.1&lt;/strong> 和 &lt;strong>Kubernetes 1.28.8&lt;/strong> 集群。&lt;/p>
&lt;p>&lt;strong>实战服务器配置 (架构 1:1 复刻小规模生产环境，配置略有不同)&lt;/strong>&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ksp-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">离线环境 KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">离线环境 KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-master-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">离线环境 KubeSphere/k8s-master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-registry&lt;/td>
 &lt;td style="text-align: center">192.168.9.90&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">离线环境部署节点和镜像仓库节点（Harbor）&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-deploy&lt;/td>
 &lt;td style="text-align: center">192.168.9.89&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">联网主机用于制作离线包&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">32&lt;/td>
 &lt;td style="text-align: center">64&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">500&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>实战环境涉及软件版本信息&lt;/strong>&lt;/p></description></item><item><title>一文说清 KubeSphere 容器平台的价值</title><link>https://openksc.github.io/zh/blogs/kubesphere-values/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-values/</guid><description>&lt;p>KubeSphere 作为云原生家族 &lt;strong>后起之秀&lt;/strong>，开源近两年的时间以来收获了诸多用户与开发者的认可。本文通过大白话从零诠释 KubeSphere 的定位与价值，以及不同团队为什么会选择 KubeSphere。&lt;/p>
&lt;h2 id="对于企业-kubesphere-是什么">对于企业 KubeSphere 是什么&lt;/h2>
&lt;p>KubeSphere 是在 Kubernetes 之上构建的 &lt;strong>多租户&lt;/strong> 容器平台，以应用为中心，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。使用 KubeSphere 不仅能够帮助企业在公有云或私有化数据中心快速搭建 Kubernetes 集群，还提供了一套功能丰富的向导式操作界面。&lt;/p>
&lt;video controls="controls" style="width: 100% !important; height: auto !important;">
 &lt;source type="video/mp4" src="https://kubesphere-docs.pek3b.qingstor.com/website/%E4%BA%A7%E5%93%81%E4%BB%8B%E7%BB%8D/KubeSphere-2.1.1-demo.mp4">
&lt;/video>
&lt;p>KubeSphere 能够帮助企业快速构建一个功能丰富的容器云平台，让企业在享受 Kubernetes 的弹性伸缩与敏捷部署的同时，还可以在容器平台拥有 IaaS 平台的存储与网络能力，获得与 IaaS 一样稳定的用户体验。比如在 KubeSphere 2.1.1 新增了对阿里云与腾讯云块存储插件的集成，支持为 Pod 挂载公有云的存储，为有状态应用提供更稳定的持久化存储的能力。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200410133408.png" alt="对于企业 KubeSphere 是什么">&lt;/p>
&lt;p>在日常的运维开发中，我们可能需要使用与管理大量的开源工具，频繁地在不同工具的 GUI 和 CLI 窗口操作，每一个工具的单独安装、使用与运维都会带来一定的学习成本，而 KubeSphere 容器平台能够统一纳管与对接这些工具，提供一致性的用户体验。这意味着，我们不需要再去多线程频繁地在各种开源组件的控制面板窗口和命令行终端切换，极大赋能企业中的开发和运维团队，提高生产效率。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200410133506.png" alt="统一纳管工具">&lt;/p>
&lt;h2 id="对于开发者-kubesphere-是什么">对于开发者 KubeSphere 是什么&lt;/h2>
&lt;p>有很多用户习惯把 KubeSphere 定义为 “云原生全家桶”。不难理解，KubeSphere 就像是一个一揽子解决方案，我们设计了一套完整的管理界面，开发与运维在一个统一的平台中，可以非常方便地安装与管理用户最常用的云原生工具，从业务视角提供了一致的用户体验来降低复杂性。为了不影响底层 Kubernetes 本身的灵活性，也为了让用户能够按需安装，KubeSphere 所有功能组件都是可插拔的。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200410133832.png" alt="对于开发者 KubeSphere 是什么">&lt;/p>
&lt;p>KubeSphere 基于 &lt;a href="https://github.com/openpitrix/openpitrix" target="_blank" rel="noopener noreferrer">OpenPitrix&lt;/a> 和 Helm 提供了应用商店，对内可作为团队间共享企业内部的中间件、大数据、APM 和业务应用等，方便开发者一键部署应用至 Kubernetes 中；对外可作为根据行业特性构建行业交付标准、交付流程和应用生命周期管理的基础，作为行业通用的应用商店，可根据不同需求应对不同的业务场景。在 3.0 版本还将支持计量 (Metering)，方便企业对应用与集群资源消耗的成本进行管理。&lt;/p></description></item><item><title>应用现代化中的弹性伸缩</title><link>https://openksc.github.io/zh/blogs/building-resilient-and-scalable-systems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/building-resilient-and-scalable-systems/</guid><description>&lt;blockquote>
&lt;p>作者：马伟，青云科技容器顾问，云原生爱好者，目前专注于云原生技术，云原生领域技术栈涉及 Kubernetes、KubeSphere、KubeKey 等。&lt;/p>&lt;/blockquote>
&lt;p>2019 年，我在给很多企业部署虚拟化，介绍虚拟网络和虚拟存储。&lt;/p>
&lt;p>2023 年，这些企业都已经上了云原生了。对于高流量的 Web 应用程序，实时数据分析，大规模数据处理、移动应用程序等业务，容器比虚拟机更适合，因为它轻量级，快速响应，可轻松移植，并具有很强的弹性伸缩能力。&lt;/p>
&lt;p>为什么需要弹性伸缩呢？&lt;/p>
&lt;ul>
&lt;li>峰值负载应对：促销活动、节假日购物季或突发事件根据需求快速扩展资源，保证应用可用性和性能。&lt;/li>
&lt;li>提高资源利用率：根据实际资源负载动态调整资源规模，避免基础设施资源浪费，降低 TCO。&lt;/li>
&lt;li>应对故障和容错：多实例部署和快速替换，提高业务连续性和可用性。&lt;/li>
&lt;li>跟随需求变化：匹配前端的业务需求及压力，快速调整规模，提高事件应对能力，满足需求和期望。&lt;/li>
&lt;/ul>
&lt;h2 id="horizontal-pod-autoscaling">Horizontal Pod Autoscaling&lt;/h2>
&lt;p>Kubernetes 自身提供一种弹性伸缩的机制，包括 Vertical Pod Autoscaler (VPA)和 Horizontal Pod Autoscaler (HPA)。HPA 根据 CPU 、内存利用率增加或减少副本控制器的 pod 数量，它是一个扩缩资源规模的功能特性。&lt;/p>
&lt;p>HPA 依赖 Metrics-Server 捕获 CPU、内存数据来提供资源使用测量数据，也可以根据自定义指标（如 Prometheus）进行扩缩。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/1684252964357.png" alt="">&lt;/p>
&lt;p>由上图看出，HPA 持续监控 Metrics-Server 的指标情况，然后计算所需的副本数动态调整资源副本，实现设置目标资源值的水平伸缩。&lt;/p>
&lt;p>&lt;strong>但也有一定局限性：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>无外部指标支持。如不同的事件源，不同的中间件/应用程序等，业务端的应用程序变化及依赖是多样的，不只是基于 CPU 和内存扩展。&lt;/li>
&lt;li>无法 1-&amp;gt;0。应用程序总有 0 负载的时候，此时不能不运行工作负载吗？&lt;/li>
&lt;/ul>
&lt;p>所以就有了&lt;strong>Kubernetes-based Event-Driven Autoscaling（KEDA）！&lt;/strong>&lt;/p>
&lt;h2 id="keda">KEDA&lt;/h2>
&lt;p>KEDA 基于事件驱动进行自动伸缩。什么是事件驱动？我理解是对系统上的各种事件做出反应并采取相应行动（伸缩）。那么 KEDA 就是一个 HPA+多种触发器。只要触发器收到某个事件被触发，KEDA 就可以使用 HPA 进行自动伸缩了，并且，KEDA 可以 1-0，0-1!&lt;/p>
&lt;h3 id="架构">架构&lt;/h3>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/1683791027319.png" alt="">&lt;/p>
&lt;p>KEDA 自身有几个组件：&lt;/p></description></item><item><title>云原生安全产品 NeuVector 简介</title><link>https://openksc.github.io/zh/blogs/neuvector-cloud-native-security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/neuvector-cloud-native-security/</guid><description>&lt;p>近日一则《SUSE 发布 NeuVector：业内首个开源容器安全平台》的文章被转载于各大 IT 新闻网站。作为 SUSE 家族的新进成员，在 3 个月后便履行了开源承诺，着实让人赞叹。那么 NeuVector 究竟有哪些过人之处能得到 SUSE 的青睐？而对比各安全厂商的开源安全产品又有哪些突破？接下来，我会以一个 SecDevOps 的视角对 NeuVector 进行简要分析。&lt;/p>
&lt;h2 id="开源云原生安全产品现状">开源云原生安全产品现状&lt;/h2>
&lt;p>NeuVector 此次开源的并非某个组件或者安全工具，而是一套完整的容器安全平台。这与其他各大云原生安全厂商的开源策略有很大的区别。目前，云原生领域活跃的开源厂商包括：Aqua Security, Falco(sysdig), Anchore, Fairwinds, Portshift 等，以及被红帽收购的 Stackrox，除此还有像 Clair 这样来自大厂的安全工具。而传统的安全厂商虽然都有面向原生安全的产品，然而鲜有软件开源。云原生安全产品成为了创新型安全厂商突破传统厂商重围的一条重要赛道。而开源则更像他们检验其产品的试金石。&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>项目&lt;/th>
 &lt;th>厂商&lt;/th>
 &lt;th>链接&lt;/th>
 &lt;th>Star&lt;/th>
 &lt;th>类型&lt;/th>
 &lt;th>开源时间&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>clair&lt;/td>
 &lt;td>Quay&lt;/td>
 &lt;td>&lt;a href="https://github.com/quay/clair" target="_blank" rel="noopener noreferrer">https://github.com/quay/clair&lt;/a>&lt;/td>
 &lt;td>8.4k&lt;/td>
 &lt;td>镜像扫描&lt;/td>
 &lt;td>2015-11-13&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>trivy&lt;/td>
 &lt;td>Aqua&lt;/td>
 &lt;td>&lt;a href="https://github.com/aquasecurity/trivy" target="_blank" rel="noopener noreferrer">https://github.com/aquasecurity/trivy&lt;/a>&lt;/td>
 &lt;td>10.1k&lt;/td>
 &lt;td>镜像扫描&lt;/td>
 &lt;td>2019-04-11&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>kube-hunter&lt;/td>
 &lt;td>Aqua&lt;/td>
 &lt;td>&lt;a href="https://github.com/aquasecurity/kube-hunter/" target="_blank" rel="noopener noreferrer">https://github.com/aquasecurity/kube-hunter/&lt;/a>&lt;/td>
 &lt;td>3.4k&lt;/td>
 &lt;td>漏洞扫描&lt;/td>
 &lt;td>2018-07-18&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>kube-bench&lt;/td>
 &lt;td>Aqua&lt;/td>
 &lt;td>&lt;a href="https://github.com/aquasecurity/kube-bench" target="_blank" rel="noopener noreferrer">https://github.com/aquasecurity/kube-bench&lt;/a>&lt;/td>
 &lt;td>4.5k&lt;/td>
 &lt;td>CIS 安全基线&lt;/td>
 &lt;td>2017-06-19&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>starboard&lt;/td>
 &lt;td>Aqua&lt;/td>
 &lt;td>&lt;a href="https://github.com/aquasecurity/starboard" target="_blank" rel="noopener noreferrer">https://github.com/aquasecurity/starboard&lt;/a>&lt;/td>
 &lt;td>968&lt;/td>
 &lt;td>Dashboard&lt;/td>
 &lt;td>2020-03-17&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>tracee&lt;/td>
 &lt;td>Aqua&lt;/td>
 &lt;td>&lt;a href="https://github.com/aquasecurity/tracee" target="_blank" rel="noopener noreferrer">https://github.com/aquasecurity/tracee&lt;/a>&lt;/td>
 &lt;td>1.5k&lt;/td>
 &lt;td>基于 eBPF 的系统事件追踪&lt;/td>
 &lt;td>2019-09-18&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>anchore-engine&lt;/td>
 &lt;td>anchore&lt;/td>
 &lt;td>&lt;a href="https://github.com/anchore/anchore-engine" target="_blank" rel="noopener noreferrer">https://github.com/anchore/anchore-engine&lt;/a>&lt;/td>
 &lt;td>1.4k&lt;/td>
 &lt;td>漏洞扫描&lt;/td>
 &lt;td>2017-09-06&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>kyverno&lt;/td>
 &lt;td>kyverno.io&lt;/td>
 &lt;td>&lt;a href="https://github.com/kyverno/kyverno" target="_blank" rel="noopener noreferrer">https://github.com/kyverno/kyverno&lt;/a>&lt;/td>
 &lt;td>1.8k&lt;/td>
 &lt;td>Kubernetes 策略与审计&lt;/td>
 &lt;td>2019-02-04&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>GateKeeper&lt;/td>
 &lt;td>OPA (sysdig)&lt;/td>
 &lt;td>&lt;a href="https://github.com/open-policy-agent/gatekeeper" target="_blank" rel="noopener noreferrer">https://github.com/open-policy-agent/gatekeeper&lt;/a>&lt;/td>
 &lt;td>1.3k&lt;/td>
 &lt;td>Kubernetes 策略与审计&lt;/td>
 &lt;td>2018-10-26&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>falco&lt;/td>
 &lt;td>falcosecurity(sysdig)&lt;/td>
 &lt;td>&lt;a href="https://github.com/falcosecurity/falco" target="_blank" rel="noopener noreferrer">https://github.com/falcosecurity/falco&lt;/a>&lt;/td>
 &lt;td>4.4k&lt;/td>
 &lt;td>基于内核模块的系统事件追踪、警告&lt;/td>
 &lt;td>2016-01-19&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>terrascan&lt;/td>
 &lt;td>accurics.com&lt;/td>
 &lt;td>&lt;a href="https://github.com/accurics/terrascan" target="_blank" rel="noopener noreferrer">https://github.com/accurics/terrascan&lt;/a>&lt;/td>
 &lt;td>2.7k&lt;/td>
 &lt;td>通用的 IaS 配置扫描&lt;/td>
 &lt;td>2017-09-11&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>Kubei&lt;/td>
 &lt;td>portshift&lt;/td>
 &lt;td>&lt;a href="https://github.com/cisco-open/kubei" target="_blank" rel="noopener noreferrer">https://github.com/cisco-open/kubei&lt;/a>&lt;/td>
 &lt;td>489&lt;/td>
 &lt;td>镜像扫描(带面板)&lt;/td>
 &lt;td>2020-03-22&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>Polaris&lt;/td>
 &lt;td>Fairwinds&lt;/td>
 &lt;td>&lt;a href="https://github.com/FairwindsOps/polaris" target="_blank" rel="noopener noreferrer">https://github.com/FairwindsOps/polaris&lt;/a>&lt;/td>
 &lt;td>2.4k&lt;/td>
 &lt;td>配置扫描与策略&lt;/td>
 &lt;td>2018-11-15&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>kubesec&lt;/td>
 &lt;td>controlplaneio&lt;/td>
 &lt;td>&lt;a href="https://github.com/controlplaneio/kubesec" target="_blank" rel="noopener noreferrer">https://github.com/controlplaneio/kubesec&lt;/a>&lt;/td>
 &lt;td>667&lt;/td>
 &lt;td>Kubernetes 配置扫描&lt;/td>
 &lt;td>2017-10-10&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>KubeEye&lt;/td>
 &lt;td>KubeSphere&lt;/td>
 &lt;td>&lt;a href="https://github.com/kubesphere/kubeeye" target="_blank" rel="noopener noreferrer">https://github.com/kubesphere/kubeeye&lt;/a>&lt;/td>
 &lt;td>424&lt;/td>
 &lt;td>基于策略的 Kubernetes 集群配置扫描&lt;/td>
 &lt;td>2020-11-07&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>kube-linter&lt;/td>
 &lt;td>Stackrox(RedHat)&lt;/td>
 &lt;td>&lt;a href="https://github.com/stackrox/kube-linter" target="_blank" rel="noopener noreferrer">https://github.com/stackrox/kube-linter&lt;/a>&lt;/td>
 &lt;td>1.8k&lt;/td>
 &lt;td>Kubernetes 配置扫描&lt;/td>
 &lt;td>2020-08-13&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>上表中，我们列举出了来自各个安全厂商的主要开源项目。从上面的表格中我们可以看出,目前开源安全软件集中在四大类别：&lt;/p></description></item><item><title>云原生的 WebAssembly 能取代 Docker 吗？</title><link>https://openksc.github.io/zh/blogs/can-webassembly-replace-docker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/can-webassembly-replace-docker/</guid><description>&lt;blockquote>
&lt;p>WebAssembly 是一个可移植、体积小、加载快并且兼容 Web 的全新格式。由于 WebAssembly 具有很高的安全性，可移植性，效率和轻量级功能，因此它是应用程序安全沙箱方案的理想选择。现如今 WebAssembly 已受到容器，功能计算以及物联网和边缘计算社区的广泛关注。究竟 WebAssembly 是怎样的一种技术，能否取代 Docker，就请阅读本文。&lt;/p>&lt;/blockquote>
&lt;blockquote>
&lt;p>本文是整理自 KubeSphere 2020 年度 Meetup 中 Second State CEO Michael Yuan 的分享。&lt;/p>&lt;/blockquote>
&lt;p>大家下午好，我是 Second State 的 CEO Michael Yuan，我们公司的主要研发在台北和美国，然后在北京望京有个办公室。今天非常开心来到 KubeSphere 2020 Meetup，我给大家分享的主题是云原生的 WebAssembly 能取代 Docker 吗？&lt;/p>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/WebAssembly-2.png" alt="">&lt;/p>
&lt;p>这是一个著名的 Twitter，是 Docker 的创始人 Solomon Hykes 在 2019 年 3 月份发布的。他说如果2008年的时候，WASM (WebAssembly) 和 WASI (WebAssembly System Interface, WASM 系统接口) 这两个东西已经存在了的话，他就没有必要创立 Docker了。他认为 WebAssembly 是计算的未来。这条推特在社区里造成很大影响，引发了很多人的的疑问。因为很多人认为，WebAssembly 可以在浏览器里取代 JavaScript，是用来玩游戏的。为什么突然成为在服务端能够取代 Docker 的东西呢？也就在这一年多后，包括我们公司在内，很多人在这里面做了很多 research。&lt;/p>
&lt;h2 id="webassembly-在服务端的位置">WebAssembly 在服务端的位置&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/WebAssembly-3.png" alt="">&lt;/p></description></item><item><title>云原生时代，如何通过 KubeSphere x 极狐GitLab 构建安全应用？</title><link>https://openksc.github.io/zh/blogs/kubesphere-gitlab-secure/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-gitlab-secure/</guid><description>&lt;blockquote>
&lt;p>本文整理自云原生 Meetup 杭州站上，极狐(GitLab) DevOps 技术布道师马景贺的演讲。&lt;/p>&lt;/blockquote>
&lt;p>当听到云原生的时候，你会想起什么？&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubesphere-gitlab-20230628-1.jpg" alt="">&lt;/p>
&lt;p>可能很多人很自然地就会想到 Kubernetes、容器、微服务、开源等等，这些关键词是我们接触云原生绕不开的话题。但是以上还少了一个关键词：安全。&lt;/p>
&lt;p>云原生从 2013 年出现，2015 年发展起来以后，安全也逐渐被关注和重视。&lt;/p>
&lt;p>以云原生中常用的镜像安全为例，下图是通过拉取常用镜像，用 Trivy 进行扫描的结果，不同颜色对应不同等级的漏洞。例如 node.js 这个开源项目，高危漏洞有 544 个，中危级漏洞有 921 个；还有 Jenkins，漏洞数量也不少。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubesphere-gitlab-20230628-2.jpg" alt="">&lt;/p>
&lt;p>可以看到，容器镜像安全问题比较严重，关于容器镜像安全的更多分析报告，感兴趣的朋友可以参阅 Anchore 发布的 2021 年、2022 年软件供应链安全报告。&lt;/p>
&lt;p>一个完整的软件开发生命周期包括源代码开发、构建、测试、部署等环节，每一个步骤都可能存在潜在安全风险。我们应该把安全嵌入到每一个环节中去，也就是将 DevSecOps 应用到云原生应用程序开发的每一个环节中去，再加上 K8s 容器镜像安全扫描，才能打造一个完整的云原生安全生态。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubesphere-gitlab-20230628-3.jpg" alt="">&lt;/p>
&lt;h2 id="devsecops-是什么如何帮助我们打造云原生安全生态">DevSecOps 是什么？如何帮助我们打造云原生安全生态？&lt;/h2>
&lt;p>&lt;strong>DevSecOps 是一个兼具深度和广度的纵深安全防御系统&lt;/strong>，从 Source code、 Build 、Test 到 Deploy，任何一个阶段，都有对应的安全手段。其次，DevSecOps 是流程、工具、文化的深度结合，传统的研发团队里，开发人员只负责代码的开发，不会关注后续的运维等流程，但在 DevSecOps 标准要求下，安全是团队中每个人的责任，需要贯穿从开发到运维全生命周期的每一个环节。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubesphere-gitlab-20230628-4.jpg" alt="">&lt;/p>
&lt;p>DevSecOps 强调的&lt;strong>安全测试左移&lt;/strong>，其实就是更早地让研发加入进来。我们软件研发过程中的大部分安全问题，都是在开发阶段引入的，因此，如果从 Source code 阶段就尽早将安全考虑在内，从源头处提升安全能力，就能更有效、更低成本地发现和解决安全问题。&lt;/p>
&lt;p>&lt;strong>安全持续自动化&lt;/strong>也是很重要的一个部分，因为一个完整的安全流程，涉及到很多安全工具，如果每个工具都手动配置，手动触发安全测试，那么工作量就会大大提升。所以 DevSecOps 期望的是安全持续自动化，开发写完代码提交变更以后（MR 或者 PR），就可以进行自动扫描，产出报告并建议你如何修复。&lt;/p>
&lt;blockquote>
&lt;p>当然了现在也进入 ChatGPT 时代，我前段时间做了一个 Demo，ChatGPT 在 Code Review 阶段就告诉你哪些是有安全风险的，甚至还会给出推荐的修复代码，感兴趣的朋友也可以去尝试一下&lt;/p>&lt;/blockquote>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/kubesphere-gitlab-20230628-5.jpg" alt="">&lt;/p></description></item><item><title>运营商业务系统基于 KubeSphere 的容器化实践</title><link>https://openksc.github.io/zh/blogs/kubesphere-container-2020/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-container-2020/</guid><description>&lt;blockquote>
&lt;p>本篇文章是 KubeSphere 2020 年度 Meetup 上讲师宋磊分享内容整理而成。&lt;/p>&lt;/blockquote>
&lt;p>大家好，我是宋磊，在运营商的一个科技子公司任职，主要做大数据业务。我主要负责公司的 IaaS 层和 PaaS 层的建设和运营的工作，涉及到两个层面。因为 Kubernetes 是一个非常全面的技术体系，并不是我们部署了一个集群把业务放上去就能开箱即用，涉及到很多方面，比如服务器、网络、存储，还有一系列的工具链的支持，我们才能真正的去投产，所以我们团队是比较适合做这件事的。&lt;/p>
&lt;h2 id="业务类型和实践架构">业务类型和实践架构&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/ToS-operator.png" alt="">&lt;/p>
&lt;p>我们目前有三种类型的业务：
1.接口的服务，容量占比是比较大的一块
2.APP 的应用
3.外部的应用系统，主要做智慧政务、智慧生态、智慧城市、智慧旅游等业务&lt;/p>
&lt;p>这三个类型的业务，整体的 TPS 的峰值大约在 2500，平均在 1500 左右。&lt;/p>
&lt;p>我们整体的集群规模：我们所有的集群都是以物理服务器进行部署的，生产集群有 50 个物理节点，测试的集群有 20——30 个节点，整体的 Kubernetes 集群的规模不到 100 个物理节点。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/architecture-operator.png" alt="">&lt;/p>
&lt;p>上面这张图是我们 Kubernetes 的实践。&lt;/p>
&lt;p>IaaS 层：
数据中心物理层的网络是 SDN 加 VXLAN 的架构，后续对于网络插件的选型是有考虑的。&lt;/p>
&lt;p>存储这一块我们主要是对接 Ceph，我们有一个比较大的 Ceph 集群，大概有 50 个物理节点，其中对接层不单单跑了 KubeSphere 的这些业务，还跑了一些 OpenStack 的虚拟机。我们在 Ceph 上面做了一些数据的分层，闪存盘(存放集群元数据)和 SATA 盘(存放真正的数据)，也做了一些数据的热度分层，然后以 KubeSphere 为中心的容器集群周边做了很多对接的工具链。这其中的一些工具链不是容器化的，而是外链的，比如说 CMDB 配置管理，Prometheus 的监控，Skywalking 主要做微服务的全链路监控，还有一些日志的采集分析，主要还是以 ELK 的工具链为主，也是在 KubeSphere 集群之外的，DevOps 这层是基于 Jenkins 的 pipeline 去做的。&lt;/p></description></item><item><title>在 Azure CNI 中启用 Calico WireGuard</title><link>https://openksc.github.io/zh/blogs/calico-wireguard-support-with-azure-cni/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/calico-wireguard-support-with-azure-cni/</guid><description>&lt;blockquote>
&lt;p>原文链接：&lt;a href="https://thenewstack.io/calico-wireguard-support-with-azure-cni/" target="_blank" rel="noopener noreferrer">https://thenewstack.io/calico-wireguard-support-with-azure-cni/&lt;/a>&lt;/p>&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>作者：Peter Kelly&lt;/strong>&lt;br />
&lt;strong>译者：田璧州&lt;/strong>&lt;br />
&lt;strong>注：本文已取得作者本人的翻译授权&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;p>去年 6 月，&lt;a href="https://www.tigera.io/?utm_content=inline-mention" target="_blank" rel="noopener noreferrer">Tigera&lt;/a> 宣布首次在 k8s 上支持用于集群内加密传输的开源 VPN，&lt;a href="https://www.wireguard.com/" target="_blank" rel="noopener noreferrer">WireGuard&lt;/a> 。我们从来不喜欢坐以待毙，所以我们一直在努力为这项技术开发一些令人兴奋的新功能，其中第一个功能是使用 &lt;a href="https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md" target="_blank" rel="noopener noreferrer">Azure 容器网络接口&lt;/a> (CNI) 在 &lt;a href="https://azure.microsoft.com/en-us/services/kubernetes-service/" target="_blank" rel="noopener noreferrer">Azure Kubernetes 服务&lt;/a> (AKS) 上支持 WireGuard。&lt;/p>
&lt;p>首先，这里简单回顾一下什么是 WireGuard 以及我们如何在 Calico 中使用它。&lt;/p>
&lt;p>WireGuard 是一种 VPN 技术，从 linux 5.6 内核开始默认包含在内核中，它被定位为 IPsec 和 OpenVPN 的替代品。它的目标是更加快速、安全、易于部署和管理。正如不断涌现的 SSL/TLS 的漏洞显示，密码的敏捷性会极大增加复杂性，这与 WireGuard 的目标不符，为此，WireGuard 故意将密码和算法的配置灵活性降低，以减少该技术的可攻击面和可审计性。它的目标是更加简单快速，所以使用标准的 Linux 网络命令便可以很容易的对它进行配置，并且只有约 4000 行代码，使得它的代码可读性高，容易理解和接受审查。&lt;/p>
&lt;p>WireGuard 是一种 VPN 技术，通常被认为是 C/S 架构。它同样能在端对端的网格网络架构中配置使用，这就是 Tigera 设计的 WireGuard 可以在 Kubernetes 中启用的解决方案。使用 Calico，所有启用 WireGuard 的节点将端对端形成一个加密的网格。Calico 甚至支持在同一集群内同时包含启用 WireGuard 的节点与未启用 WireGuard 的节点，并且可以相互通信。&lt;/p></description></item><item><title>在 Debian 12 上安装 KubeSphere 实战入门</title><link>https://openksc.github.io/zh/blogs/deploy-kubesphere-on-debian12/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deploy-kubesphere-on-debian12/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;h3 id="知识点">&lt;strong>知识点&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>KubeKey 安装部署 KubeSphere 和 Kubernetes&lt;/li>
&lt;li>Debian 操作系统的基本配置&lt;/li>
&lt;li>Kubernetes 常用工作负载的创建&lt;/li>
&lt;li>KubeSphere 控制台操作入门&lt;/li>
&lt;/ul>
&lt;h3 id="演示服务器配置">&lt;strong>演示服务器配置&lt;/strong>&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">3&lt;/td>
 &lt;td style="text-align: center">12&lt;/td>
 &lt;td style="text-align: center">48&lt;/td>
 &lt;td style="text-align: center">120&lt;/td>
 &lt;td style="text-align: center">600&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h3 id="演示环境涉及软件版本信息">&lt;strong>演示环境涉及软件版本信息&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>操作系统：&lt;strong>Debian 12&lt;/strong>&lt;/li>
&lt;li>KubeSphere：&lt;strong>3.3.2&lt;/strong>&lt;/li>
&lt;li>Kubernetes：&lt;strong>v1.26.0&lt;/strong>&lt;/li>
&lt;li>KubeKey: &lt;strong>v3.0.7&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>Debian 曾经的开源 Linux 发行版的王者，唯一的电信级开源操作系统，我曾经在 IDC 工作时，所有的核心业务、自建防火墙都运行在 Debian 之上，稳如磐石。&lt;/p>
&lt;p>基于多种原因 Debian 的用户不断减少，尤其是在国内，已经很少有人谈及 Debian了。在当今寻求 CentOS 替代品的背景下，Debian 也是一种选择。&lt;/p></description></item><item><title>在 Kubernetes 上部署 RabbitMQ</title><link>https://openksc.github.io/zh/blogs/rabbitmq-k8s/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/rabbitmq-k8s/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>知识点&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>RabbitMQ 单节点安装部署&lt;/li>
&lt;li>RabbitMQ 集群安装部署&lt;/li>
&lt;li>GitOps 运维思想&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>&lt;strong>演示服务器配置&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">zdeops-master&lt;/td>
 &lt;td style="text-align: center">192.168.9.9&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Ansible 运维控制节点&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">storage-node-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">storage-node-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">storage-node-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/GlusterFS&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">harbor&lt;/td>
 &lt;td style="text-align: center">192.168.9.89&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Harbor&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">22&lt;/td>
 &lt;td style="text-align: center">84&lt;/td>
 &lt;td style="text-align: center">320&lt;/td>
 &lt;td style="text-align: center">2800&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>&lt;strong>演示环境涉及软件版本信息&lt;/strong>&lt;/p></description></item><item><title>在 Kubernetes 中安装和使用 Apache APISIX Ingress 网关</title><link>https://openksc.github.io/zh/blogs/use-apache-apisix-ingress-in-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/use-apache-apisix-ingress-in-kubesphere/</guid><description>&lt;p>&lt;a href="https://kubesphere.com.cn/blogs/kubesphere-3.2.0-ga-announcement/" target="_blank" rel="noopener noreferrer">KubeSphere 3.2.0 发布了！&lt;/a>为项目网关增配了整套监控及管理页面，同时引入了集群网关来提供集群层面全局的 Ingress 网关能力。当然，我们还是可以部署使用第三方 Ingress Controller，本文将以 &lt;a href="https://apisix.apache.org/docs/ingress-controller/getting-started/" target="_blank" rel="noopener noreferrer">Apache APISIX Ingress Controller&lt;/a> 为例介绍如何通过 KubeSphere 快速为 Kubernetes 集群使用两种不同类型的网关，同时对它们的使用状态进行监控。&lt;/p>
&lt;p>本文将分为一下几部分展开：&lt;/p>
&lt;ul>
&lt;li>KubeSphere 项目网关的新管理界面的应用展示&lt;/li>
&lt;li>通过 KubeSphere 的应用管理能力快速使用 Apache APISIX Ingress Controller&lt;/li>
&lt;li>利用 KubeSphere 的自定义监控能力获取 Apache APISIX 网关的运行指标&lt;/li>
&lt;/ul>
&lt;h2 id="准备工作">准备工作&lt;/h2>
&lt;h3 id="安装-kubesphere">安装 KubeSphere&lt;/h3>
&lt;p>安装 KubeSphere 有两种方法。一是在 Linux 上直接安装，可以参考文档：&lt;a href="https://kubesphere.com.cn/docs/quick-start/all-in-one-on-linux/" target="_blank" rel="noopener noreferrer">在 Linux 安装 KubeSphere&lt;/a>； 二是在已有 Kubernetes 中安装，可以参考文档：&lt;a href="https://kubesphere.com.cn/docs/quick-start/minimal-kubesphere-on-k8s/" target="_blank" rel="noopener noreferrer">在 Kubernetes 安装 KubeSphere&lt;/a>。&lt;/p>
&lt;p>KubeSphere 最小化安装版本已经包含了监控模块，因此不需要额外启用，可以通过「系统组件」页面中的「监控」标签页确认安装状态。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202111251132484.png" alt="">&lt;/p>
&lt;h3 id="部署-httpbin-演示应用">部署 httpbin 演示应用&lt;/h3>
&lt;p>由于需要演示网关的访问控制能力，我们必须要先有一个可以访问的应用作为网关的后台服务。这里我们使用 &lt;a href="https://httpbin.org/" target="_blank" rel="noopener noreferrer">httpbin.org&lt;/a> 提供的 &lt;a href="https://hub.docker.com/r/kennethreitz/httpbin/" target="_blank" rel="noopener noreferrer">kennethreitz/httpbin&lt;/a> 容器应用作为演示应用。&lt;/p>
&lt;p>在 KubeSphere 中，我们可以先创建新的项目或使用已有的项目，进入项目页面后，选择「应用负载」下的「服务」直接创建无状态工作负载并生成配套的服务。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202111251132702.png" alt="">&lt;/p>
&lt;p>使用 &lt;a href="https://hub.docker.com/r/kennethreitz/httpbin/" target="_blank" rel="noopener noreferrer">kennethreitz/httpbin&lt;/a> 容器默认的 &lt;code>80&lt;/code> 端口作为服务端口，创建完成后确保在「工作负载」和「服务」页面下都可以看到 &lt;code>httpbin&lt;/code> 的对应条目，如下图所示。&lt;/p></description></item><item><title>在 Kubernetes 中安装和使用 JuiceFS 存储</title><link>https://openksc.github.io/zh/blogs/kubesphere-juicefs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-juicefs/</guid><description>&lt;h2 id="juicefs-简介">JuiceFS 简介&lt;/h2>
&lt;p>JuiceFS 是为海量数据设计的分布式文件系统，使用对象存储来做数据持久化，避免重复造轮子，还能大大降低工程复杂度，让用户专注解决元数据和访问协议部分的难题。&lt;/p>
&lt;p>使用 JuiceFS 存储数据，数据本身会被持久化在对象存储（例如，Amazon S3），而数据所对应的元数据可以根据场景需要被持久化在 Redis、MySQL、SQLite 等多种数据库中。&lt;/p>
&lt;h2 id="kubesphere-平台介绍">KubeSphere 平台介绍&lt;/h2>
&lt;p>KubeSphere 是在 Kubernetes 之上构建的以应用为中心的多租户容器平台，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。&lt;/p>
&lt;p>KubeSphere 提供了运维友好的向导式操作界面，即便是 Kubernetes 经验并不丰富的用户，也能相对轻松的上手开始管理和使用。它提供了基于 Helm 的应用市场，可以在可视化界面下非常轻松地安装各种 Kubernetes 应用。&lt;/p>
&lt;hr>
&lt;p>本教程将介绍如何在 KubeSphere 中一键部署 JuiceFS CSI Driver，为集群上的各种应用提供数据持久化。&lt;/p>
&lt;h2 id="前提条件">前提条件&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://v3-1.docs.kubesphere.io/zh/docs/installing-on-linux/public-cloud/install-kubesphere-on-huaweicloud-ecs/" target="_blank" rel="noopener noreferrer">安装 KubeSphere&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubesphere.com.cn/docs/pluggable-components/app-store/" target="_blank" rel="noopener noreferrer">在 KubeSphere 中启用应用商店&lt;/a>&lt;/li>
&lt;li>准备对象存储
&lt;ul>
&lt;li>&lt;a href="https://support.huaweicloud.com/function-obs/index.html" target="_blank" rel="noopener noreferrer">创建华为云 OBS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://support.huaweicloud.com/usermanual-ca/zh-cn_topic_0046606340.html" target="_blank" rel="noopener noreferrer">创建秘钥&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="部署-redis">部署 Redis&lt;/h2>
&lt;p>Redis 是 JuiceFS 架构中的关键组件，它负责存储所有元数据并响应客户端对元数据的操作。所以在部署 JuiceFS CSI Driver 之前，需要先部署一个 Redis 数据库，部署详细步骤可参考 &lt;a href="https://kubesphere.com.cn/docs/application-store/built-in-apps/redis-app/" target="_blank" rel="noopener noreferrer">KubeSphere 官方文档&lt;/a>。&lt;/p>
&lt;h2 id="部署-juicefs-csi-driver">部署 JuiceFS CSI Driver&lt;/h2>
&lt;p>KubeSphere 从 3.2.0 开始新增了 “&lt;strong>动态加载应用商店&lt;/strong>” 的功能，合作伙伴可通过提交 PR 申请将应用的 Helm Chart 集成到 KubeSphere 应用商店，这样 KubeSphere 应用商店即可动态加载应用。目前 JuiceFS CSI Driver 的 Helm Chart 已经通过这种方式集成到了 KubeSphere 的应用商店，用户可以一键将 JuiceFS CSI Driver 部署至 Kubernetes。&lt;/p></description></item><item><title>在 Kubernetes 中部署并使用 KubeEdge</title><link>https://openksc.github.io/zh/blogs/kubesphere-integrate-kubeedge/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-integrate-kubeedge/</guid><description>&lt;p>边缘计算在广泛制造业、工业、零售和金融等行业，随着云原生应用的兴起，不可变基础设施和快速的应用交付等特性很适用于边缘计算场景。因此在 Kubernetes 上使用边缘计算框架是近年很火热的一个方向。本篇会介绍下边缘计算的场景和架构，并以一个 Demo 示例展示如何运行一个边缘应用到边缘节点上。&lt;/p>
&lt;h2 id="边缘计算痛点和场景">边缘计算痛点和场景&lt;/h2>
&lt;p>首先，边缘计算是云计算的延伸，云计算按需和资源池化的特性可以满足资源利用率的提升和计算资源的集中供给，但边缘测的应用场景决定不可能什么应用都丢到数据中心里。比如&lt;/p>
&lt;ul>
&lt;li>低延迟处理。车联网场景如果要进行数据共享和指令下发需要极低延迟的通信保障和计算处理速度。&lt;/li>
&lt;li>本地处理数据。有些数据较敏感不能什么都传到云端（如用户照片、密码）&lt;/li>
&lt;li>离线自治。很多边端设备不一定有可靠的连接保持和云端的通信。如农业、地理科学的传感器。&lt;/li>
&lt;/ul>
&lt;p>因此对于边缘节点和边缘设备来说，需要统一管理和本地计算的能力，来实现云端负责总体决策，边端负责本地执行的效果。这样处理的好处就是数据处理效率高、减少云边带宽、降低运营成本，却又不缺少云端的资产管理、调度监控、安全管控。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202301111706561.png" alt="">&lt;/p>
&lt;p>了解边缘计算含义后，就有提问了，既然边缘计算和云计算都是一种概念，势必市场上有众多边缘计算的产品和标准，为什么要在 Kubernetes 上使用边缘计算呢？&lt;/p>
&lt;p>这个我个人理解是双向奔赴的，kubernetes 本身比较适合基础架构标准化和应用交付，可以帮助云端统一管理边缘节点和边缘设备，也适合于标准应用的云端下发，生态丰富且开源开放的可观测体系也适合于不同的企业用管数据中心的方法实现边端可观测性。而 Kubernetes 也需要边缘计算将自己的能力得到更多的延伸，去实现更多的平台能力和平台标准，毕竟云-网-边-端是当代云计算需要涵盖的每个方向~&lt;/p>
&lt;h2 id="常见边缘计算框架">常见边缘计算框架&lt;/h2>
&lt;p>对于产品选型来说，云原生领域最方便的就是打开 landscape 了。我们来看看 Automation&amp;amp;Configuration 这栏：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202301111706007.png" alt="">&lt;/p>
&lt;p>我们常见的有三种，如 KubeEdge、OpenYurt 和 SuperEdge。本篇先拿 KubeEdge 这个孵化项目分享下。&lt;/p>
&lt;h2 id="kubeedge-架构">KubeEdge 架构&lt;/h2>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202301111712875.png" alt="">&lt;/p>
&lt;p>KubeEdge 分为云端和边端。云端核心组件 CloudCore 和边端核心组件 EdgeCore 联合实现边缘计算框架的众多功能，如云边通信、设备管理、离线自治等。还有一些辅助组件如 EdgeMesh 实现边端通信和服务治理，Sedna 提供边端 AI 框架等。&lt;/p>
&lt;p>而到具体的 CloudCore 和 EdgeCore 的组成，可从下图详细学习架构设计：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/202301111715701.png" alt="">&lt;/p>
&lt;h3 id="云端">云端&lt;/h3>
&lt;p>CloudCore 由 CloudHub 和 EdgeController、DeviceController 组成。&lt;/p>
&lt;ul>
&lt;li>CloudHub。主要观察云边变化，读写 Edge 消息，缓存数据后通过 WebSocket/QUIC（K8s 的 listwatch 机制太耗资源）发送给 EdgeHub，还要把和 Edge 通信得到的一些消息发送给 Controller。&lt;/li>
&lt;li>EdgeController。作为 ApiServer 和 EdgeCore 的桥梁，管理常用的配置、Pod、缓存等事件，把 EdgeCore 订阅到的 Pod 的众多事件信息同步状态到 ApiServer。也把 ApiServer 的 ADD/UPDATE/DELETE 等事件同步到 EdgeCore。&lt;/li>
&lt;li>DeviceController。通过 EdgeCore DeviceTwin 同步设备更新，总体过程是	Mapper—&amp;gt;MQTT—&amp;gt;EventBus—&amp;gt;DeviceTwin-&amp;gt;EdgeHub-&amp;gt;CloudHub—&amp;gt;Deviceontroller-&amp;gt;APIServer。另一方面就是云端创建的 Device，下发到边端得到元数据进行设备端更新。&lt;/li>
&lt;/ul>
&lt;h3 id="边端">边端&lt;/h3>
&lt;ul>
&lt;li>EdgeHub。云边通信边端，同步资源更新。&lt;/li>
&lt;li>EventBus。发送 / 接收 MQTT 消息&lt;/li>
&lt;li>MetaManager。在 SQLlite 存储数据，是 Edged 和 EdgeHub 的消息处理器。&lt;/li>
&lt;li>Edged。边端裁剪版 kubelet。管理 Pod、configmap、volume 等资源的生命周期。还包含一个 StatusManager 和 MetaClient 组件。前者每 10s 将本地数据库存储的状态信息上传至云，后者作为 client 和本地迷你 Etcd（MetaManager）交互，如读取云端下发的 ConfigMap、Secret，写 Node、Pod Status。&lt;/li>
&lt;li>DeviceTwin。存储设备属性和状态，创建 Edge 设备和节点关系，同步设备属性到云。&lt;/li>
&lt;li>ServiceBus: 接收云上服务请求和边缘应用进行 http 交互&lt;/li>
&lt;/ul>
&lt;h2 id="安装部署">安装部署&lt;/h2>
&lt;h3 id="安装-cloudcore">安装 Cloudcore&lt;/h3>
&lt;p>KubeSphere 已经集成了 KubeEdge，可提供边缘节点纳管、应用下发、日志监控等功能。接下来将在 KubeSphere 上演示边缘计算 Demo。&lt;/p></description></item><item><title>在 Kubernetes 中部署云原生开发工具 Nocalhost</title><link>https://openksc.github.io/zh/blogs/kubesphere-nocalhost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-nocalhost/</guid><description>&lt;h2 id="kubesphere-简介">KubeSphere 简介&lt;/h2>
&lt;p>KubeSphere 是在 Kubernetes 之上构建的以应用为中心的多租户容器平台，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。&lt;/p>
&lt;p>KubeSphere 提供了运维友好的向导式操作界面，即便是 Kubernetes 经验并不丰富的用户，也能相对轻松的上手开始管理和使用。它提供了基于 Helm 的应用市场，可以在图形化界面下非常轻松地安装各种 Kubernetes 应用。&lt;/p>
&lt;h2 id="nocalhost-简介">Nocalhost 简介&lt;/h2>
&lt;p>&lt;a href="https://nocalhost.dev/" target="_blank" rel="noopener noreferrer">Nocalhost&lt;/a> 是一个允许开发者直接在 Kubernetes 集群内开发应用的工具。&lt;/p>
&lt;p>Nocalhost 的核心功能是：提供 Nocalhost IDE 插件（包括 VSCode 和 Jetbrains 插件），将远端的工作负载更改为开发模式。在开发模式下，容器的镜像将被替换为包含开发工具（例如 JDK、Go、Python 环境等）的开发镜像。当开发者在本地编写代码时，任何修改都会实时被同步到远端开发容器中，应用程序会立即更新（取决于应用的热加载机制或重新运行应用），开发容器将继承原始工作负载所有的声明式配置（configmap、secret、volume、env 等）。&lt;/p>
&lt;p>Nocalhost 还提供：&lt;/p>
&lt;ul>
&lt;li>VSCode 和 Jetbrains IDE 一键 Debug 和 HotReload&lt;/li>
&lt;li>在 IDE 内直接提供开发容器的终端，获得和本地开发一致的体验&lt;/li>
&lt;li>提供基于 Namespace 隔离的开发空间和 Mesh 开发空间&lt;/li>
&lt;/ul>
&lt;p>在使用 Nocalhost 开发 Kubernetes 的应用过程中，免去了镜像构建，更新镜像版本，等待集群调度 Pod 的过程，把编码/测试/调试反馈循环(code/test/debug cycle)从分钟级别降低到了秒级别，大幅提升开发效率&lt;/p>
&lt;p>此外，Nocalhost 还提供了 Server 端帮助企业管理 Kubernetes 应用、开发者和开发空间，方便企业统一管理各类开发和测试环境。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/nocalhost-server.png" alt="">&lt;/p></description></item><item><title>在 Kubernetes 中基于 StatefulSet 部署 MySQL（上）</title><link>https://openksc.github.io/zh/blogs/mysql-on-k8s-statefulset-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/mysql-on-k8s-statefulset-1/</guid><description>&lt;p>&lt;strong>大家好，我是老 Z！&lt;/strong>&lt;/p>
&lt;p>本文实现了 MySQL 数据库在基于 KubeSphere 部署的 K8s 集群上的安装部署，部署方式采用了图形化这种形式。下一篇文章将会涉及 GitOps 的基础操作，部署过程涉及的所有 YAML 文件都会使用 Git 进行版本管理，并存放在 Git 仓库中，敬请期待！&lt;/p>
&lt;p>本文部署的 MySQL 选择了比较保守的 5.7 系列，其他版本可能会有不同。本文的操作仅适用于小规模数据量且对可靠性和性能要求不高的数据库使用场景，例如开发测试环境、例如我生产环境的 Nacos 服务。生产环境或是重要的数据库个人不建议将数据放到 K8s 上，优先采用云服务商提供的 RDS，其次自己利用虚拟机搭建 MySQL 主从或是 Galera Cluster，且一定做好备份方案。&lt;/p>
&lt;p>&lt;strong>数据库的可靠性、可用性是运维的重中之重，不容忽视，切记！！！&lt;/strong>&lt;/p>
&lt;h3 id="本文知识点">本文知识点&lt;/h3>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>单节点 MySQL 在 K8s 上的安装配置&lt;/li>
&lt;li>KubeSphere 图形化部署工作负载&lt;/li>
&lt;li>GitOps 入门&lt;/li>
&lt;li>Git 常用操作&lt;/li>
&lt;li>配置代码如何实现在 GitHub 和 Gitee 保持同步&lt;/li>
&lt;li>MySQL 性能测试基础&lt;/li>
&lt;li>运维思想、思路&lt;/li>
&lt;/ul>
&lt;h3 id="演示服务器配置">演示服务器配置&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">操作系统&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">zdeops-master&lt;/td>
 &lt;td style="text-align: center">CentOS-7.9-x86_64&lt;/td>
 &lt;td style="text-align: center">192.168.9.9&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Ansible 运维控制节点&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-0&lt;/td>
 &lt;td style="text-align: center">CentOS-7.9-x86_64&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">32&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-1&lt;/td>
 &lt;td style="text-align: center">CentOS-7.9-x86_64&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">32&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-2&lt;/td>
 &lt;td style="text-align: center">CentOS-7.9-x86_64&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">32&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">glusterfs-node-0&lt;/td>
 &lt;td style="text-align: center">CentOS-7.9-x86_64&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">GlusterFS/Elasticsearch&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">glusterfs-node-1&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">GlusterFS/Elasticsearch&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">glusterfs-node-2&lt;/td>
 &lt;td style="text-align: center">CentOS-7.9-x86_64&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">GlusterFS/Elasticsearch&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;hr>
&lt;h2 id="mysql-安装之旅">MySQL 安装之旅&lt;/h2>
&lt;h3 id="寻找参考文档">寻找参考文档&lt;/h3>
&lt;h4 id="我个人查找参考文档习惯的的寻找路径">我个人查找参考文档习惯的的寻找路径&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>官方网站&lt;/strong>-精准定位
&lt;ul>
&lt;li>官网有时没有相关文档、或是文档不够详细&lt;/li>
&lt;li>英文文档、阅读困难&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>搜索关键字&lt;/strong>-大海捞针
&lt;ul>
&lt;li>CSDN&lt;/li>
&lt;li>博客园&lt;/li>
&lt;li>某个人博客&lt;/li>
&lt;li>问答网站&lt;/li>
&lt;li>其他&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="打开-mysql-官方网站">打开 &lt;a href="https://dev.mysql.com/doc/" title="MySQL 官方网站" target="_blank" rel="noopener noreferrer">MySQL 官方网站&lt;/a>&lt;/h4>
&lt;p>选择 &lt;strong>MySQL5.7&lt;/strong> 版本的 Reference Manual。&lt;/p></description></item><item><title>在 Kubernetes 中基于 StatefulSet 部署 MySQL（下）</title><link>https://openksc.github.io/zh/blogs/mysql-on-k8s-statefulset-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/mysql-on-k8s-statefulset-2/</guid><description>&lt;p>&lt;strong>大家好，我是老Z！&lt;/strong>&lt;/p>
&lt;p>上篇文章实现了 MySQL 数据库在基于 KubeSphere 部署的 K8s 集群上的安装部署，部署方式采用了图形化界面这种形式。本文将会介绍如何使用 GitOps 来部署 MySQL，部署过程涉及的所有 YAML 文件都会使用 Git 进行版本管理，并存放在 Git 仓库中。因此，本文还会涉及 GitOps 的基础操作。&lt;/p>
&lt;h2 id="原生-k8s-使用-gitops-部署-mysql">原生 K8s 使用 GitOps 部署 MySQL&lt;/h2>
&lt;p>上篇文章我们完成了通过 KubeSphere 部署单实例 MySQL，那么原生的 K8s 又该如何操作？GitOps 又是什么、又该如何实现？&lt;/p>
&lt;h3 id="什么是-gitops">什么是 GitOps&lt;/h3>
&lt;ul>
&lt;li>GitOps 是一套使用 Git 来管理基础架构和应用配置的实践，而 Git 指的是一个开源版控制系统。&lt;/li>
&lt;li>GitOps 在运行过程中以 Git 为声明性基础架构和应用的单一事实来源。&lt;/li>
&lt;li>GitOps 使用 Git 拉取请求来自动管理基础架构的置备和部署。&lt;/li>
&lt;li>Git 存储库包含系统的全部状态，因此系统状态的修改痕迹既可查看也可审计。&lt;/li>
&lt;li>GitOps 经常被用作 K8s 和云原生应用开发的运维模式，并且可以实现对 K8s 的持续部署。&lt;/li>
&lt;li>GitOps 是一种持续交付的方式。它的核心思想是将应用系统的声明性基础架构和应用程序存放在 Git 版本库中。&lt;/li>
&lt;/ul>
&lt;h3 id="准备资源配置清单-思路梳理">准备资源配置清单-思路梳理&lt;/h3>
&lt;p>我们知道玩 K8s 的必备技能就是要手写资源配置清单，一般使用 YAML 格式的文件来创建我们预期的资源配置。&lt;/p>
&lt;p>此时我们也要手写 MySQL 的资源配置清单？我很慌，参数我记不全啊。&lt;/p>
&lt;p>NO！NO！NO！投机取巧的时刻到了，前面卖的关子在这揭开了。&lt;/p></description></item><item><title>在 Kubernetes 中实现微服务应用监控</title><link>https://openksc.github.io/zh/blogs/kubernetes--microservice--monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubernetes--microservice--monitoring/</guid><description>&lt;blockquote>
&lt;p>张坚，科大讯飞开发工程师，云原生爱好者。&lt;/p>&lt;/blockquote>
&lt;p>本篇文章我们基于 Prometheus 和 Grafana 实现微服务应用监控。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/20220927141938.png" alt="">&lt;/p>
&lt;blockquote>
&lt;p>KubeSphere 平台本身提供了监控功能，包括节点状态、集群资源使用率、Etcd、API Server 等监控，不过缺少了应用级别的监控。&lt;/p>&lt;/blockquote>
&lt;h2 id="引入依赖包">引入依赖包&lt;/h2>
&lt;p>在应用中引入监控所需要的 jar 包，包含 Prometheus 和 Actuator。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-xml" data-lang="xml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;lt;dependency&amp;gt;&lt;/span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;lt;groupId&amp;gt;&lt;/span>org.springframework.boot&lt;span style="color:#f92672">&amp;lt;/groupId&amp;gt;&lt;/span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;lt;artifactId&amp;gt;&lt;/span>spring-boot-starter-actuator&lt;span style="color:#f92672">&amp;lt;/artifactId&amp;gt;&lt;/span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;lt;/dependency&amp;gt;&lt;/span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;lt;dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;lt;groupId&amp;gt;&lt;/span>io.micrometer&lt;span style="color:#f92672">&amp;lt;/groupId&amp;gt;&lt;/span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;lt;artifactId&amp;gt;&lt;/span>micrometer-registry-prometheus&lt;span style="color:#f92672">&amp;lt;/artifactId&amp;gt;&lt;/span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;lt;/dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>引入这 2 个包以后就通过 Prometheus 抓取到应用的监控信息。&lt;/p>
&lt;h2 id="修改应用配置暴露监控端口">修改应用配置，暴露监控端口&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">management&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">endpoints&lt;/span>: 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">web&lt;/span>: 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">exposure&lt;/span>: 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">include&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;*&amp;#34;&lt;/span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">base-path&lt;/span>: &lt;span style="color:#ae81ff">/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">metrics&lt;/span>: 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tags&lt;/span>: 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">application&lt;/span>: &lt;span style="color:#ae81ff">${spring.application.name}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>management.server.port：启用独立的端口来提供监控，未配置的情况下共用应用端口；&lt;/li>
&lt;li>management.metrics.tags.xxx：在统计信息中添加自定义的标签；&lt;/li>
&lt;li>management.endpoints.web.exposure.include：用于包含我们要公开的端点列表 , 我们这里设置为* 代表所有。&lt;/li>
&lt;li>management.endpoints.web.base-path：用于设置 Promethues 的监控路径，默认是通过 &lt;code>/actuator/prometheus&lt;/code> 访问，这样配置以后只需要通过 &lt;code>/prometheus&lt;/code> 访问&lt;/li>
&lt;/ul>
&lt;p>配置完成后重启服务，通过浏览器访问 &lt;code>localhost:8080/prometheus&lt;/code> 即可抓取到 Prometheus 的监控数据，效果如下：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/20220928154733.png" alt="">&lt;/p>
&lt;h2 id="修改-service-配置">修改 Service 配置&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">spring-cloud-provider-service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">cloud-demo&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">labels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">spring-cloud-provider-service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">micrometer-prometheus-discovery&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;true&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">metrics&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">protocol&lt;/span>: &lt;span style="color:#ae81ff">TCP&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">port&lt;/span>: &lt;span style="color:#ae81ff">8080&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">targetPort&lt;/span>: &lt;span style="color:#ae81ff">8080&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">selector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">spring-cloud-provider&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>修改后端服务端的 Service：&lt;/p></description></item><item><title>在 Kubernetes 中使用 Rook 构建云原生存储环境</title><link>https://openksc.github.io/zh/blogs/rook-on-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/rook-on-kubesphere/</guid><description>&lt;h2 id="rook-介绍">Rook 介绍&lt;/h2>
&lt;p>Rook 是一个开源的云原生存储编排器，为各种存储解决方案提供平台、框架和支持，以便与云原生环境进行原生集成。&lt;/p>
&lt;p>Rook 将分布式存储系统转变为自管理、自扩展、自修复的存储服务。它使存储管理员的部署、引导、配置、配置、扩展、升级、迁移、灾难恢复、监控和资源管理等任务自动化。&lt;/p>
&lt;p>简而言之，Rook 就是一组 Kubernetes 的 Operator，它可以完全控制多种数据存储解决方案（例如 Ceph、EdgeFS、Minio、Cassandra）的部署，管理以及自动恢复。&lt;/p>
&lt;p>到目前为止，Rook 支持的最稳定的存储仍然是 Ceph，本文将介绍如何使用 Rook 来创建维护 Ceph 集群，并作为 Kubernetes 的持久化存储。&lt;/p>
&lt;h2 id="环境准备">环境准备&lt;/h2>
&lt;p>K8s 环境可以通过安装 KubeSphere 进行部署,我使用的是高可用方案。&lt;/p>
&lt;p>在公有云上安装 KubeSphere 参考文档：&lt;a href="https://v3-1.docs.kubesphere.io/zh/docs/installing-on-linux/public-cloud/install-kubesphere-on-huaweicloud-ecs/" title="多节点安装" target="_blank" rel="noopener noreferrer">多节点安装&lt;/a>&lt;/p>
&lt;p>⚠️ 注意：kube-node(5,6,7)的节点上分别有两块数据盘。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kube-master1 Ready master 118d v1.17.9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-master2 Ready master 118d v1.17.9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-master3 Ready master 118d v1.17.9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-node1 Ready worker 118d v1.17.9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-node2 Ready worker 118d v1.17.9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-node3 Ready worker 111d v1.17.9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-node4 Ready worker 111d v1.17.9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-node5 Ready worker 11d v1.17.9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-node6 Ready worker 11d v1.17.9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-node7 Ready worker 11d v1.17.9
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>安装前请确保 node 节点都安装上了 lvm2，否则会报错。&lt;/p></description></item><item><title>在 KubeSphere 安装 Orion vGPU 使用 TensorFlow 运行深度学习训练</title><link>https://openksc.github.io/zh/blogs/kubesphere-orion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-orion/</guid><description>&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200116193908.png" alt="">&lt;/p>
&lt;h2 id="概览">概览&lt;/h2>
&lt;p>本文将使用 &lt;a href="https://github.com/kubesphere/kubesphere" target="_blank" rel="noopener noreferrer">KubeSphere 容器平台&lt;/a>，在 Kubernetes 上部署 &lt;a href="https://github.com/virtaitech/orion" target="_blank" rel="noopener noreferrer">Orion vGPU 软件&lt;/a> 进行深度学习加速，并基于 Orion vGPU 软件使用经典的 Jupyter Notebook 进行模型训练与推理。&lt;/p>
&lt;p>在开始安装 Orion vGPU 和演示深度学习训练之前，先简单了解一下，什么是 &lt;strong>vGPU&lt;/strong> 以及什么是 &lt;strong>Orion vGPU&lt;/strong>。&lt;/p>
&lt;h2 id="什么是-vgpu">什么是 vGPU&lt;/h2>
&lt;p>vGPU 又称 &lt;strong>虚拟 GPU&lt;/strong>，早在几年前就由 &lt;a href="https://www.nvidia.cn/data-center/virtual-gpu-technology/" target="_blank" rel="noopener noreferrer">NVIDIA&lt;/a> 推出了这个概念以及相关的产品。&lt;strong>vGPU 是通过对数据中心（物理机）的 GPU 进行虚拟化&lt;/strong>，用户可在多个虚拟机或容器中 &lt;strong>共享该数据中心的物理 GPU 资源&lt;/strong>，有效地提高性能并降低成本。vGPU 使得 GPU 与用户之间的关系不再是一对一，而是 &lt;strong>一对多&lt;/strong>。&lt;/p>
&lt;h2 id="为什么需要-vgpu">为什么需要 vGPU&lt;/h2>
&lt;p>随着 AI 技术的快速发展，越来越多的企业开始将 AI 技术应用到自身业务之中。目前，云端 AI 算力主要由三类 AI 加速器来提供：GPU，FPGA 和 AI ASIC 芯片。这些加速器的优点是性能非常高，缺点是 &lt;strong>成本高昂，缺少异构加速管理和调度&lt;/strong>。大部分企业因无法构建高效的加速器资源池，而不得不独占式地使用这些昂贵的加速器资源，导致 &lt;strong>资源利用率低，成本高&lt;/strong>。&lt;/p>
&lt;p>以 GPU 为例，通过创新的 vGPU 虚拟化技术，能够帮助用户无需任务修改就能透明地共享和使用数据中心内任何服务器之上的 AI 加速器，不但能够帮助用户提高资源利用率，而且可以 &lt;strong>极大便利 AI 应用的部署，构建数据中心级的 AI 加速器资源池&lt;/strong>。&lt;/p></description></item><item><title>在 KubeSphere 部署 Wiki 系统 wiki.js 并启用中文全文检索</title><link>https://openksc.github.io/zh/blogs/kubesphere-wiki/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-wiki/</guid><description>&lt;h2 id="背景">背景&lt;/h2>
&lt;p>wiki.js 是优秀的开源 Wiki 系统，相较于 xwiki ，功能目前性上比 xwiki 不够完善，但也在不断进步。 Wiki 写作、分享、权限管理功能还是有的，胜在 UI 设计很漂亮，能满足小团队的基本知识管理需求。&lt;/p>
&lt;p>以下工作是在 KubeSphere 3.2.1 + Helm 3 已经部署好的情况下进行的。&lt;/p>
&lt;p>部署 KubeSphere 的方法官网有很详细的文档介绍，这里不再赘叙。
&lt;a href="https://kubesphere.com.cn/docs/installing-on-linux/introduction/multioverview/" target="_blank" rel="noopener noreferrer">https://kubesphere.com.cn/docs/installing-on-linux/introduction/multioverview/&lt;/a>&lt;/p>
&lt;h2 id="准备-storageclass">准备 storageclass&lt;/h2>
&lt;p>我们使用 OpenEBS 作为存储，OpenEBS 默认安装的 Local StorageSlass 在 Pod 销毁后自动删除，不适合用于我的 MySQL 存储，我们在 Local StorageClass 基础上稍作修改，创建新的 StorageClass，允许 Pod 销毁后，PV 内容继续保留，手动决定怎么处理。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">items&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">storage.k8s.io/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">StorageClass&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">annotations&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cas.openebs.io/config&lt;/span>: |&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> - name: StorageType
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> value: &amp;#34;hostpath&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> - name: BasePath
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> value: &amp;#34;/var/openebs/localretain/&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">openebs.io/cas-type&lt;/span>: &lt;span style="color:#ae81ff">local&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">storageclass.beta.kubernetes.io/is-default-class&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;false&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">storageclass.kubesphere.io/supported-access-modes&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;[&amp;#34;ReadWriteOnce&amp;#34;]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">localretain&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">provisioner&lt;/span>: &lt;span style="color:#ae81ff">openebs.io/local&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">reclaimPolicy&lt;/span>: &lt;span style="color:#ae81ff">Retain&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">volumeBindingMode&lt;/span>: &lt;span style="color:#ae81ff">WaitForFirstConsumer&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">List&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">resourceVersion&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">selfLink&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="部署-postgresql-数据库">部署 PostgreSQL 数据库&lt;/h2>
&lt;p>我们团队其他项目中也需要使用 PostgreSQL, 为了提高 PostgreSQL 数据库的利用率和统一管理，我们独立部署 PostgreSQL，并在安装 wiki.js 时，配置为使用外部数据库。&lt;/p></description></item><item><title>在 KubeSphere 上部署 AI 大模型 Ollama</title><link>https://openksc.github.io/zh/blogs/deploy-ollama-on-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deploy-ollama-on-kubesphere/</guid><description>&lt;p>随着人工智能、机器学习、AI 大模型技术的迅猛发展，我们对计算资源的需求也在不断攀升。特别是对于需要处理大规模数据和复杂算法的 AI 大模型，GPU 资源的使用变得至关重要。对于运维工程师而言，掌握如何在 Kubernetes 集群上管理和配置 GPU 资源，以及如何高效部署依赖这些资源的应用，已成为一项不可或缺的技能。&lt;/p>
&lt;p>今天，我将带领大家深入了解如何在 KubeSphere 平台上，利用 Kubernetes 强大的生态和工具，实现 GPU 资源的管理和应用部署。以下是本文将要探讨的三个核心主题：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>集群扩容与 GPU 节点集成&lt;/strong>：我们将通过 KubeKey 工具，扩展 Kubernetes 集群并增加具备 GPU 能力的 Worker 节点，为 AI 应用提供必要的硬件支持。&lt;/li>
&lt;li>&lt;strong>GPU 资源的 Kubernetes 集成&lt;/strong>：使用 Helm 安装和配置 NVIDIA GPU Operator，这是 NVIDIA 官方提供的一个解决方案，旨在简化 Kubernetes 集群中 GPU 资源的调用和管理。&lt;/li>
&lt;li>&lt;strong>实战部署：Ollama 大模型管理工具&lt;/strong>：我们将在 KubeSphere 上部署 Ollama，一个专为 AI 大模型设计的管理工具，以验证 GPU 资源是否能够被正确调度和高效使用。&lt;/li>
&lt;/ol>
&lt;p>通过阅读本文，您将获得 Kubernetes 上 管理 GPU 资源的知识和技巧，帮助您在云原生环境中，充分利用 GPU 资源，推动 AI 应用的快速发展。&lt;/p>
&lt;p>&lt;strong>KubeSphere 最佳实战「2024」&lt;/strong> 系列文档的实验环境硬件配置和软件信息如下：&lt;/p>
&lt;p>&lt;strong>实战服务器配置(架构1:1复刻小规模生产环境，配置略有不同)&lt;/strong>&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ksp-registry&lt;/td>
 &lt;td style="text-align: center">192.168.9.90&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Harbor 镜像仓库&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.94&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker/CI&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-worker-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">300+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch/Ceph/Longhorn/NFS/&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.98&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">300+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch//Ceph/Longhorn&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-storage-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.99&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">300+&lt;/td>
 &lt;td style="text-align: center">ElasticSearch//Ceph/Longhorn&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gpu-worker-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.101&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker(GPU NVIDIA Tesla M40 24G)&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gpu-worker-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.102&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">k8s-worker(GPU NVIDIA Tesla P100 16G)&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gateway-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.103&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;td style="text-align: center">自建应用服务代理网关/VIP：192.168.9.100&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-gateway-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.104&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;td style="text-align: center">自建应用服务代理网关/VIP：192.168.9.100&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-mid&lt;/td>
 &lt;td style="text-align: center">192.168.9.105&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">部署在 k8s 集群之外的服务节点（Gitlab 等）&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">15&lt;/td>
 &lt;td style="text-align: center">56&lt;/td>
 &lt;td style="text-align: center">152&lt;/td>
 &lt;td style="text-align: center">600&lt;/td>
 &lt;td style="text-align: center">2000&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>实战环境涉及软件版本信息&lt;/strong>&lt;/p></description></item><item><title>在 KubeSphere 上部署 Apache Pulsar</title><link>https://openksc.github.io/zh/blogs/pulsar-on-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/pulsar-on-kubesphere/</guid><description>&lt;blockquote>
&lt;p>作者介绍：徐文涛，StreamNative Content Strategist，热爱云原生与开源技术，活跃于本地化/文档/技术博客贡献，持有 K8s CKA/CKAD/CKS 认证。&lt;/p>&lt;/blockquote>
&lt;h2 id="apache-pulsar-介绍">Apache Pulsar 介绍&lt;/h2>
&lt;p>Apache Pulsar 作为 Apache 软件基金会顶级项目，是下一代云原生分布式消息流平台，集消息、存储、轻量化函数式计算为一体，采用计算与存储分离架构设计，支持多租户、持久化存储、跨地域复制、分层存储，具有强一致性、高吞吐、低延时及高可扩展性等流数据存储特性，是云原生时代解决实时消息流数据传输、存储和计算的最佳解决方案。&lt;/p>
&lt;p>Pulsar 的 Broker 没有状态，不存储数据。BookKeeper 负责存储数据，其中的 Bookie 支持水平扩容，元数据的信息存储在 ZooKeeper 上。这种架构也方便容器化的环境进行扩缩容，支持高可用等场景。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/pulsar-kubesphere-1.png" alt="">&lt;/p>
&lt;h2 id="kubesphere-介绍">KubeSphere 介绍&lt;/h2>
&lt;p>KubeSphere 是建立在 Kubernetes 之上的面向云原生应用的分布式操作系统，完全开源，支持多云与多集群管理、应用商店、可观测性（监控、告警及审计等）和多租户等功能，提供全栈的 IT 自动化运维能力，简化企业的 DevOps 工作流。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/pulsar-kubesphere-2.jpg" alt="">&lt;/p>
&lt;h2 id="在-kubesphere-上安装-apache-pulsar">在 KubeSphere 上安装 Apache Pulsar&lt;/h2>
&lt;p>作为一个以应用为中心的 K8s 容器平台，KubeSphere 为用户提供多种安装应用的方式。为快速部署应用，这里给大家推荐以下两种方法：&lt;/p>
&lt;ul>
&lt;li>直接从 KubeSphere 的应用商店中安装 Pulsar。KubeSphere 从 3.2.0 版本开始便新增了“动态加载应用商店”的功能。想要贡献应用的小伙伴可以直接向 KubeSphere 的 &lt;a href="https://github.com/kubesphere/helm-charts" target="_blank" rel="noopener noreferrer">Helm 仓库&lt;/a>贡献应用的 Helm Chart，待 PR 审核通过后应用商店上会加载最新的应用列表。&lt;/li>
&lt;li>添加 Apache Pulsar 的 Helm 仓库至 KubeSphere 的应用仓库，然后从应用仓库中安装 Pulsar 应用模板。此安装方式类似在命令行使用 &lt;code>helm&lt;/code> 相关命令添加仓库并安装。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>注：KubeSphere 的应用商店与应用的全生命周期管理功能基于开源项目 &lt;a href="https://github.com/openpitrix/openpitrix" target="_blank" rel="noopener noreferrer">OpenPitrix&lt;/a>。在安装 Pulsar 前，你需要先&lt;a href="https://kubesphere.com.cn/docs/pluggable-components/app-store/" target="_blank" rel="noopener noreferrer">在 KubeSphere 中启用 OpenPitrix&lt;/a>。&lt;/p></description></item><item><title>在 KubeSphere 上部署 OpenLDAP 并进行对接使用</title><link>https://openksc.github.io/zh/blogs/kubesphere-openldap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-openldap/</guid><description>&lt;blockquote>
&lt;p>作者：申红磊，青云科技容器解决方案架构师，开源项目爱好者，KubeSphere Member。&lt;/p>&lt;/blockquote>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>在实际使用中，会有一些用户，在不同场景中经常碰到 OpenLDAP 对接问题：&lt;/p>
&lt;ul>
&lt;li>能否对接 LDAP？&lt;/li>
&lt;li>对接方式都有什么，有界面吗？&lt;/li>
&lt;li>能否按自己要求来对接指定账户，而非全部账户都能访问？&lt;/li>
&lt;li>能否批量导入账户？&lt;/li>
&lt;li>默认角色如何绑定？&lt;/li>
&lt;/ul>
&lt;p>在这里可以简单操作一下，以便来抛砖引玉，主要思路为：在 KubeSphere 中直接运行一个 LDAP Server，用 ApacheDirectoryStudio 来验证，然后使用 KubeSphere 进行 LDAP 对接验证。&lt;/p>
&lt;h2 id="前置条件">前置条件&lt;/h2>
&lt;p>您需要部署一个 K8s 集群，并在集群中安装 KubeSphere。有关详细信息，请参阅&lt;a href="https://kubesphere.io/zh/docs/v3.3/installing-on-linux/" target="_blank" rel="noopener noreferrer">在 Linux 上安装&lt;/a>和&lt;a href="https://kubesphere.io/zh/docs/v3.3/installing-on-kubernetes/" target="_blank" rel="noopener noreferrer">在 Kubernetes 上安装&lt;/a>。&lt;/p>
&lt;h2 id="kubesphere-中部署-ldap">KubeSphere 中部署 LDAP&lt;/h2>
&lt;p>这里通过应用为用户提供完整的业务功能，由一个或多个特定功能的组件组成。来部署 OpenLDAP&lt;/p>
&lt;h3 id="部署-ldap-应用">部署 LDAP 应用&lt;/h3>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/4655e595576c40b2a7dcb70f71956931.png" alt="">&lt;/p>
&lt;p>创建无状态服务（演示使用）&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/6f3fb40f378841eaba3216135fb77049.png" alt="">&lt;/p>
&lt;p>这里使用的镜像为：&lt;a href="https://hub.docker.com/r/bitnami/openldap" target="_blank" rel="noopener noreferrer">bitnami/openldap:latest&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker pull bitnami/openldap:latest
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 参考 the OpenLDAP server instance 可以配置 env 在后面使用&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ docker run --detach --rm --name openldap &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --network my-network &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --env LDAP_ADMIN_USERNAME&lt;span style="color:#f92672">=&lt;/span>admin &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --env LDAP_ADMIN_PASSWORD&lt;span style="color:#f92672">=&lt;/span>adminpassword &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --env LDAP_USERS&lt;span style="color:#f92672">=&lt;/span>customuser &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --env LDAP_PASSWORDS&lt;span style="color:#f92672">=&lt;/span>custompassword &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> bitnami/openldap:latest
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/17385d847e014114b9a14dcf703c41c5.png" alt="">&lt;/p></description></item><item><title>在 KubeSphere 上快速安装和使用 KDP 云原生数据平台</title><link>https://openksc.github.io/zh/blogs/kdp-on-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kdp-on-kubesphere/</guid><description>&lt;blockquote>
&lt;p>作者简介：金津，智领云高级研发经理，华中科技大学计算机系硕士。加入智领云 8 余年，长期从事云原生、容器化编排领域研发工作，主导了智领云自研的 BDOS 应用云平台、云原生大数据平台 KDP 等产品的开发，并在多个大规模项目中成功实施落地，在大规模容器化编排系统方向有丰富的实践经验。&lt;/p>&lt;/blockquote>
&lt;h2 id="在-kubesphere-上部署-kdp">在 KubeSphere 上部署 KDP&lt;/h2>
&lt;p>GitHub 地址：https://github.com/linktimecloud/kubernetes-data-platform/blob/main/docs/zh/user-tutorials/install-kdp-on-kubesphere-101.md&lt;/p>
&lt;h3 id="技术简介">技术简介&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubesphere/kubekey" target="_blank" rel="noopener noreferrer">KubeKey&lt;/a>&lt;/p>
&lt;p>KubeKey 是一个开源的 Kubernetes 安装程序和生命周期管理工具。它支持安装 Kubernetes 集群、KubeSphere 以及其他相关组件。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubesphere/kubesphere" target="_blank" rel="noopener noreferrer">KubeSphere&lt;/a>&lt;/p>
&lt;p>KubeSphere 是一个用于云原生应用程序管理的分布式操作系统，使用 Kubernetes 作为其内核。它提供了即插即用架构，允许第三方应用程序无缝集成到其生态系统中。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.linktimecloud.com/kubernetes-data-platform" target="_blank" rel="noopener noreferrer">Kubernetes Data Platform&lt;/a>&lt;/p>
&lt;p>KDP（Kubernetes Data Platform）提供了一个基于 Kubernetes 的现代化混合云原生数据平台，能够利用 Kubernetes 的云原生能力来有效地管理数据平台。KDP 是构建在 Kubernetes 之上的，因此可以与任意的 Kubernetes 管理平台快速集成。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>总的来说，KubeSphere 和 KDP 组合在一起，能够为用户提供一套完善的、强大的、基于 Kubernetes 的现代化云原生应用数据平台。未来，通过 KubeSphere 的 LuBan 集成框架，可以将 KDP 开发成为 KubeSphere 的扩展组件，从而进一步深度融合进 KubeSphere。&lt;/p>
&lt;h3 id="先决条件">先决条件&lt;/h3>
&lt;p>在 Kubernetes 上已安装 KubeSphere（&lt;a href="https://kubesphere.io/zh/docs/v3.4/quick-start/minimal-kubesphere-on-k8s/" target="_blank" rel="noopener noreferrer">快速开始可参考在 Kubernetes 上最小化安装 KubeSphere&lt;/a>）：&lt;/p></description></item><item><title>在 KubeSphere 中部署高可用 Redis 集群</title><link>https://openksc.github.io/zh/blogs/kubesphere-redis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-redis/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>&lt;code>Redis&lt;/code> 是在开发过程中经常用到的&lt;strong>缓存中间件&lt;/strong>，在生产环境中为了考虑&lt;strong>稳定性&lt;/strong>和&lt;strong>高可用&lt;/strong>一般为集群模式的部署。&lt;/p>
&lt;p>常规部署在虚拟机上的方式配置繁琐并且需要手动重启节点，而使用 &lt;code>K8s&lt;/code> 进行 &lt;code>Redis&lt;/code> 集群的部署有以下优点：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>安装便捷&lt;/strong>：使用镜像或者 &lt;code>yaml&lt;/code> 配置文件即可一件安装&lt;/li>
&lt;li>&lt;strong>自动调度&lt;/strong>：容器挂掉后会自动调度重启和资源分配&lt;/li>
&lt;li>&lt;strong>缩扩容方便&lt;/strong>：在 &lt;code>扩容&lt;/code>、&lt;code>缩容&lt;/code> 方面的优点无需多说，一键伸缩&lt;/li>
&lt;li>&lt;strong>稳定高效&lt;/strong>：&lt;code>K8s&lt;/code> 在整个集群上进行调度，只要整个集群不挂掉总会调度到合适节点重启容器服务&lt;/li>
&lt;/ul>
&lt;h2 id="安装-redis-集群">安装 Redis 集群&lt;/h2>
&lt;p>我这里新建了一个 &lt;code>test-project&lt;/code> 的项目空间来做 &lt;code>Redis&lt;/code> 集群所有安装资源的放置，后续在 &lt;code>DNS&lt;/code> 上会用到项目空间名称，会标注这一部分，需要注意用自己的项目空间名。&lt;/p>
&lt;p>安装集群大概分为以下几步：&lt;/p>
&lt;ol>
&lt;li>配置 &lt;code>redis.conf&lt;/code> 字典；&lt;/li>
&lt;li>创建 &lt;code>redis&lt;/code> 服务；&lt;/li>
&lt;li>容器组配置；&lt;/li>
&lt;li>存储设置；&lt;/li>
&lt;li>高级设置。&lt;/li>
&lt;/ol>
&lt;p>现在从第一步开始。&lt;/p>
&lt;h3 id="配置-redisconf-字典">配置 redis.conf 字典&lt;/h3>
&lt;p>在项目空间的 &lt;code>配置&lt;/code> → &lt;code>配置字典&lt;/code> → &lt;code>创建&lt;/code> 进行配置字典的创建。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/8300ba2ab7ae4cb6aa7911b9e239326b.png" alt="">&lt;/p>
&lt;p>名称就叫 &lt;code>redis-conf&lt;/code> 然后下一步 添加键值对数据。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/d793fa7f942846228973287d86047edf.png" alt="">&lt;/p>
&lt;p>&lt;code>key&lt;/code> 值的内容为 &lt;code>redis.conf&lt;/code>，&lt;code>value&lt;/code> 值为：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-cpp" data-lang="cpp">&lt;span style="display:flex;">&lt;span>cluster&lt;span style="color:#f92672">-&lt;/span>enabled yes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cluster&lt;span style="color:#f92672">-&lt;/span>config&lt;span style="color:#f92672">-&lt;/span>file nodes.conf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cluster&lt;span style="color:#f92672">-&lt;/span>node&lt;span style="color:#f92672">-&lt;/span>timeout &lt;span style="color:#ae81ff">5000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cluster&lt;span style="color:#f92672">-&lt;/span>require&lt;span style="color:#f92672">-&lt;/span>full&lt;span style="color:#f92672">-&lt;/span>coverage no
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cluster&lt;span style="color:#f92672">-&lt;/span>migration&lt;span style="color:#f92672">-&lt;/span>barrier &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>appendonly yes
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="创建-redis-服务">创建 Redis 服务&lt;/h3>
&lt;p>在项目空间的 &lt;code>应用负载&lt;/code> → &lt;code>服务&lt;/code> → &lt;code>创建&lt;/code> 进行 &lt;strong>Redis&lt;/strong> 服务的创建。&lt;/p></description></item><item><title>在 KubeSphere 中监控集群外部 Etcd</title><link>https://openksc.github.io/zh/blogs/kubesphere-etcd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-etcd/</guid><description>&lt;h2 id="1-本文简介">1. 本文简介&lt;/h2>
&lt;p>本文源于 KubeSphere 开源社区 8 群里的一个小伙伴 @Jam 提到的 Ectd 监控没有数据，希望我帮忙看一下。本来我也是没有启用 Etcd 监控的，但是既然小伙伴如此信任我提了要求了，那必须安排。所以才有了本文。&lt;/p>
&lt;p>经研究发现，KubeSphere 自带的集群状态监控中有 Etcd 监控的页面展示，但是在 KubeSphere 3.2.1 版本中，默认配置开启 Etcd 监控后，集群状态中的 Etcd 监控页面确实没有任何数据。本文将记录里解决该问题的排障之旅。&lt;/p>
&lt;p>&lt;strong>本文知识点&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Prometheus-Operator&lt;/strong>&lt;/li>
&lt;li>KubeSphere 开启 Etcd 监控&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>演示服务器配置&lt;/strong>&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">zdeops-master&lt;/td>
 &lt;td style="text-align: center">192.168.9.9&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Ansible 运维控制节点&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">32&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">32&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">32&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">glusterfs-node-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">GlusterFS/ElasticSearch&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">glusterfs-node-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">GlusterFS/ElasticSearch&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">glusterfs-node-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">GlusterFS/ElasticSearch&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h2 id="2-kubesphere-crd-开启-etcd-监控">2. KubeSphere CRD 开启 Etcd 监控&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>编辑 &lt;strong>CRD&lt;/strong> 中的 &lt;strong>ks-installer&lt;/strong> 的 YAML 配置文件。&lt;/p></description></item><item><title>在 KubeSphere 中开启高度自动化的微服务可观测性</title><link>https://openksc.github.io/zh/blogs/kubesphere-deepflow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-deepflow/</guid><description>&lt;p>Kubernetes 为开发者们带来了巨大的微服务部署便利，但同时也将可观测性建设的重要性提升到了前所未有的程度：大量微服务之间错综复杂的调用关系难以梳理，应用性能瓶颈链路难以排查，应用异常难以定位。从现在开始，使用 KubeSphere 的所有用户可以从 KubeSphere 的应用商店中快速部署 DeepFlow，为微服务应用轻松开启&lt;strong>高度自动化&lt;/strong>的&lt;strong>全栈、全链路&lt;/strong>可观测性。&lt;/p>
&lt;h2 id="什么是-kubesphere">什么是 KubeSphere&lt;/h2>
&lt;p>KubeSphere 是在 Kubernetes 之上构建的面向云原生应用的分布式操作系统，完全开源，支持多云与多集群管理，提供全栈的 IT 自动化运维能力，简化企业的 DevOps 工作流。它的架构可以非常方便地使第三方应用与云原生生态组件进行即插即用 (plug-and-play) 的集成。&lt;/p>
&lt;p>作为全栈的多租户容器平台，KubeSphere 提供了运维友好的向导式操作界面，帮助企业快速构建一个强大和功能丰富的容器云平台。KubeSphere 为用户提供构建企业级 Kubernetes 环境所需的多项功能，例如多云与多集群管理、Kubernetes 资源管理、DevOps、应用生命周期管理、微服务治理（服务网格）、日志查询与收集、服务与网络、多租户管理、监控告警、事件与审计查询、存储管理、访问权限控制、GPU 支持、网络策略、镜像仓库管理以及安全管理等。&lt;/p>
&lt;p>GitHub 地址： &lt;a href="https://github.com/kubesphere" target="_blank" rel="noopener noreferrer">https://github.com/kubesphere&lt;/a>。&lt;/p>
&lt;h2 id="什么是-deepflow">什么是 DeepFlow&lt;/h2>
&lt;p>&lt;a href="https://github.com/deepflowys/deepflow" target="_blank" rel="noopener noreferrer">DeepFlow&lt;/a> 是一款开源的高度自动化的可观测性平台，是为云原生应用开发者建设可观测性能力而量身打造的全栈、全链路、高性能数据引擎。DeepFlow 使用 eBPF、WASM、OpenTelemetry 等新技术，创新的实现了 AutoTracing、AutoMetrics、AutoTagging、SmartEncoding 等核心机制，帮助开发者提升埋点插码的自动化水平，降低可观测性平台的运维复杂度。利用 DeepFlow 的可编程能力和开放接口，开发者可以快速将其融入到自己的可观测性技术栈中。&lt;/p>
&lt;p>GitHub 地址： &lt;a href="https://github.com/deepflowys/deepflow" target="_blank" rel="noopener noreferrer">https://github.com/deepflowys/deepflow&lt;/a>。&lt;/p>
&lt;h2 id="为什么选择-deepflow">为什么选择 DeepFlow&lt;/h2>
&lt;p>目前，社区已经拥有了非常丰富的 Metrics、Tracing、Logging 解决方案，比如著名的 Prometheus、Telegraf、SkyWalking、OpenTelemetry、Fluentd、Loki 等。随着 &lt;a href="https://ebpf.io/" target="_blank" rel="noopener noreferrer">eBPF&lt;/a> 技术的发展和 Linux 内核 4.X 版本的普及，可观测性迎来了更加&lt;strong>自动化&lt;/strong>、更加&lt;strong>零侵扰&lt;/strong>的玩法。在经过一番调研后，KubeSphere 选择将 DeepFlow 作为利用 eBPF 能力建设可观测性的开源解决方案，并将其集成到应用商店中。现在，KubeSphere 用户可使用应用模板，轻松将 DeepFlow 一键部署至 Kubernetes 环境中。&lt;/p>
&lt;h2 id="轻松部署-deepflow">轻松部署 DeepFlow&lt;/h2>
&lt;p>你可以在 KubeSphere 应用商店中找到 DeepFlow，选择它并依次点击&lt;code>安装&lt;/code> -&amp;gt; &lt;code>下一步&lt;/code> -&amp;gt; &lt;code>安装&lt;/code>，即可完成 DeepFlow 的部署。&lt;/p></description></item><item><title>在 KubeSphere 中开启新一代云原生数仓 Databend</title><link>https://openksc.github.io/zh/blogs/databend-on-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/databend-on-kubesphere/</guid><description>&lt;blockquote>
&lt;p>作者：尚卓燃（ &lt;a href="https://github.com/PsiACE" target="_blank" rel="noopener noreferrer">https://github.com/PsiACE&lt;/a> ），Databend 研发工程师，Apache OpenDAL (Incubating) PPMC。&lt;/p>&lt;/blockquote>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>Databend 是一款完全面向云对象存储的新一代云原生数据仓库，专为弹性和高效设计，为您的大规模分析需求保驾护航。Databend 同时是一款符合 Apache-2.0 协议的开源软件，除了访问云服务（ &lt;a href="https://app.databend.com/" target="_blank" rel="noopener noreferrer">https://app.databend.com/&lt;/a> ）之外，用户还可以自己部署 Databend 生产集群以满足工作负载需要。&lt;/p>
&lt;p>Databend 的典型使用场景包括：&lt;/p>
&lt;ul>
&lt;li>实时分析平台，日志的快速查询与可视化。&lt;/li>
&lt;li>云数据仓库，历史订单数据的多维度分析和报表生成。&lt;/li>
&lt;li>混合云架构，统一管理和处理不同来源和格式的数据。&lt;/li>
&lt;li>成本和性能敏感的 OLAP 场景，动态调整存储和计算资源。&lt;/li>
&lt;/ul>
&lt;p>KubeSphere 是在 Kubernetes 之上构建的以应用为中心的多租户容器平台，提供全栈的 IT 自动化运维的能力，可以管理多个节点上的容器化应用，提供高可用性、弹性扩缩容、服务发现、负载均衡等功能。&lt;/p>
&lt;p>利用 KubeSphere 部署和管理 Databend 具有以下优点：&lt;/p>
&lt;ul>
&lt;li>使用 Helm Charts 部署 Databend 集群，简化应用管理、部署过程和参数设置。&lt;/li>
&lt;li>利用 Kubernetes 的特性来实现 Databend 集群的自动恢复、水平扩展、负载均衡等。&lt;/li>
&lt;li>与 Kubernetes 上的其他服务或应用轻松集成和交互，如 MinIO、Prometheus、Grafana 等。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>本文将会介绍如何使用 KubeSphere 创建和部署 Databend 高可用集群，并使用 QingStor 作为底层存储服务。&lt;/strong>&lt;/p>
&lt;h2 id="配置对象存储">配置对象存储&lt;/h2>
&lt;p>对象存储是一种存储模型，它把数据作为对象来管理和访问，而不是文件或块。对象存储的优点包括：可扩展性、低成本、高可用性等。&lt;/p>
&lt;p>Databend 完全面向对象存储而设计，在减少复杂性和成本的同时提高灵活性和效率。Databend 支持多种对象存储服务，如 AWS S3、Azure Blob、Google Cloud Storage、HDFS、Alibaba Cloud OSS、Tencent Cloud COS 等。您可以根据业务的需求和偏好选择合适的服务来存放你的数据。&lt;/p></description></item><item><title>在 KubeSphere 中快速安装部署 Nacos</title><link>https://openksc.github.io/zh/blogs/nacos-on-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/nacos-on-kubesphere/</guid><description>&lt;blockquote>
&lt;p>作者：老 Z，运维架构师，云原生爱好者，目前专注于云原生运维，云原生领域技术栈涉及 Kubernetes、KubeSphere、DevOps、OpenStack、Ansible 等。&lt;/p>&lt;/blockquote>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>Nacos 集群如何在 K8s 集群上部署？Nacos 依赖的 MySQL 数据库如何在 K8S 集群上部署？GitOps 在 K8s 集群上是一种什么体验？本文将带你全面了解上述问题。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>演示服务器配置&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">zdeops-master&lt;/td>
 &lt;td style="text-align: center">192.168.9.9&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Ansible 运维控制节点&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-k8s-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200+200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker/Ceph&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">glusterfs-node-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.95&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">GlusterFS&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">glusterfs-node-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.96&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">GlusterFS&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">glusterfs-node-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.97&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">GlusterFS&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">harbor&lt;/td>
 &lt;td style="text-align: center">192.168.9.89&lt;/td>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">Harbor&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">22&lt;/td>
 &lt;td style="text-align: center">84&lt;/td>
 &lt;td style="text-align: center">320&lt;/td>
 &lt;td style="text-align: center">2200&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>&lt;strong>演示环境涉及软件版本信息&lt;/strong>&lt;/p></description></item><item><title>在 KubeSphere 中快速部署使用 GitLab 并构建 DevOps 项目</title><link>https://openksc.github.io/zh/blogs/kubesphere-gitlab-devops/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-gitlab-devops/</guid><description>&lt;p>新年伊始，“极狐(GitLab) 联合青云（QingCloud 公有云服务和 KubeSphere 容器平台）、上海云轴（ZStack Cloud 云平台和 ZStack Cube 超融合一体机)、宝德计算、上海恒岳等国内多家知名云厂商和服务器厂商，首发 GitNative 系列产品解决方案，针对不同部署环境和应用场景，推出支持公有云、私有云、本地数据中心部署的 ‘GitNative 一体化 DevOps 平台’ 和 ‘GitNative CI/CD流水线引擎’ 解决方案。”&lt;/p>
&lt;p>在社区看到上面&lt;a href="https://mp.weixin.qq.com/s/3MyvKzPd9GgVUnjkws8YHQ" target="_blank" rel="noopener noreferrer">这条新闻&lt;/a>的时候有种 “虎躯一震” 的感觉，确实很高兴能看到国内的云社区、云厂商能在 DevOps 领域有这样接地气的商业产品合作，相信更多这样跨界合作产品的出现也会推动我们国内的 DevOps 社区及产品有进一步发展。那么对于我们开源社区的小伙伴而言，通过 GitLab 社区版以及 KubeSphere 平台提供的 DevOps 能力，其实也可以自己尝试搭建一套类似的 DevOps 平台来一起感受一下 Kubernetes 时代下 GitOps 体系的魅力。&lt;/p>
&lt;p>所以我们本次分享将和大家一起动手来实践一下在 KubeSphere 部署 GitLab CE（Community Edition 社区版）并构建与之联动的 DevOps 项目。&lt;/p>
&lt;h2 id="前提条件">前提条件&lt;/h2>
&lt;h3 id="安装-kubesphere">安装 KubeSphere&lt;/h3>
&lt;p>安装 KubeSphere 有两种方法。一是在 Linux 上直接安装，可以参考文档：&lt;a href="https://kubesphere.com.cn/docs/quick-start/all-in-one-on-linux/" target="_blank" rel="noopener noreferrer">在 Linux 安装 KubeSphere&lt;/a>； 二是在已有 Kubernetes 中安装，可以参考文档：&lt;a href="https://kubesphere.com.cn/docs/quick-start/minimal-kubesphere-on-k8s/" target="_blank" rel="noopener noreferrer">在 Kubernetes 安装 KubeSphere&lt;/a>。&lt;/p>
&lt;h3 id="在-kubesphere-中启用-devops-套件">在 KubeSphere 中启用 DevOps 套件&lt;/h3>
&lt;p>在 KubeSphere 中启用 DevOps 套件可以参考文档：&lt;a href="https://kubesphere.com.cn/docs/pluggable-components/devops/" target="_blank" rel="noopener noreferrer">启用可插拔组建 · KubeSphere DevOps 系统&lt;/a>。安装完成后可以在「平台管理」页面的「系统组建」部分看到 Jenkins 头像图标。&lt;/p></description></item><item><title>在 KubeSphere 中使用 DevOps 部署 Java 微服务配置监控预警</title><link>https://openksc.github.io/zh/blogs/kubesphere-devops-java-microservice/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubesphere-devops-java-microservice/</guid><description>&lt;h2 id="开发-java-微服务并引入监控组件">开发 Java 微服务并引入监控组件&lt;/h2>
&lt;p>我们基于 Spring Cloud +Nacos 开发 Java 微服务，Java 服务开发不做过多的叙述。&lt;/p>
&lt;h3 id="项目中引入-actuator">项目中引入 Actuator&lt;/h3>
&lt;p>我们在项目的 bom 中引入 Spring Boot Actuator，它提供了多种特性来监控和管理应用程序，可以基于 HTTP，也可以基于 JMX。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-java" data-lang="java">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;lt;&lt;/span>dependency&lt;span style="color:#f92672">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;lt;&lt;/span>groupId&lt;span style="color:#f92672">&amp;gt;&lt;/span>org.&lt;span style="color:#a6e22e">springframework&lt;/span>.&lt;span style="color:#a6e22e">boot&lt;/span>&lt;span style="color:#f92672">&amp;lt;/&lt;/span>groupId&lt;span style="color:#f92672">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;lt;&lt;/span>artifactId&lt;span style="color:#f92672">&amp;gt;&lt;/span>spring&lt;span style="color:#f92672">-&lt;/span>boot&lt;span style="color:#f92672">-&lt;/span>starter&lt;span style="color:#f92672">-&lt;/span>actuator&lt;span style="color:#f92672">&amp;lt;/&lt;/span>artifactId&lt;span style="color:#f92672">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;lt;/&lt;/span>dependency&lt;span style="color:#f92672">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="配置-actuator">配置 Actuator&lt;/h3>
&lt;p>引入 Actuator 后，原则上我们无需做任何配置即可使用，在我们项目中我们结合实际需求及提升安全性做了如下配置：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-java" data-lang="java">&lt;span style="display:flex;">&lt;span>management.&lt;span style="color:#a6e22e">health&lt;/span>.&lt;span style="color:#a6e22e">elasticsearch&lt;/span>.&lt;span style="color:#a6e22e">enabled&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">false&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>management.&lt;span style="color:#a6e22e">endpoints&lt;/span>.&lt;span style="color:#a6e22e">web&lt;/span>.&lt;span style="color:#a6e22e">exposure&lt;/span>.&lt;span style="color:#a6e22e">include&lt;/span>&lt;span style="color:#f92672">=*&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>management.&lt;span style="color:#a6e22e">endpoints&lt;/span>.&lt;span style="color:#a6e22e">web&lt;/span>.&lt;span style="color:#a6e22e">base&lt;/span>&lt;span style="color:#f92672">-&lt;/span>path&lt;span style="color:#f92672">=/&lt;/span>api&lt;span style="color:#f92672">/&lt;/span>actuator
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>management.&lt;span style="color:#a6e22e">metrics&lt;/span>.&lt;span style="color:#a6e22e">tags&lt;/span>.&lt;span style="color:#a6e22e">application&lt;/span>&lt;span style="color:#f92672">=&lt;/span>${service.&lt;span style="color:#a6e22e">name&lt;/span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>management.&lt;span style="color:#a6e22e">metrics&lt;/span>.&lt;span style="color:#a6e22e">tags&lt;/span>.&lt;span style="color:#a6e22e">appid&lt;/span>&lt;span style="color:#f92672">=&lt;/span>${service.&lt;span style="color:#a6e22e">appid&lt;/span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>management.&lt;span style="color:#a6e22e">server&lt;/span>.&lt;span style="color:#a6e22e">port&lt;/span>&lt;span style="color:#f92672">=&lt;/span>8090
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>management.server.port：启用独立的端口来提供监控，避免监控相关 api 暴露在服务外；&lt;/li>
&lt;li>management.metrics.tags.xxx：在统计信息中添加自定义的标签；&lt;/li>
&lt;li>management.endpoints.web.exposure.include：用于包含我们要公开的端点列表 , 我们这里设置为* 代表所有。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>观察应用程序监控数据&lt;/strong>&lt;/p>
&lt;p>当我们运行编写好的程序后，通过访问 &lt;code>http://localhost:8090/api/actuator/prometheus&lt;/code> 可以看到类似如下数据，其中就有我们通过配置添加的 tag 数据，后续我们部署的 monitor 会通过如下地址将数据采集到 Prometheus 中。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/f5b8e605-f17c-4c3c-bbfa-c8477a7ca306.png" alt="">&lt;/p>
&lt;h3 id="应用部署配置">应用部署配置&lt;/h3>
&lt;p>&lt;strong>1. 编写 DevOps 文件&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-groovy" data-lang="groovy">&lt;span style="display:flex;">&lt;span>pipeline &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> agent &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> node &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> label &lt;span style="color:#e6db74">&amp;#39;maven&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> options&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> buildDiscarder&lt;span style="color:#f92672">(&lt;/span>logRotator&lt;span style="color:#f92672">(&lt;/span>numToKeepStr: &lt;span style="color:#e6db74">&amp;#39;10&amp;#39;&lt;/span>&lt;span style="color:#f92672">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> parameters &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> string&lt;span style="color:#f92672">(&lt;/span>name:&lt;span style="color:#e6db74">&amp;#39;APP_NAME&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span>defaultValue: &lt;span style="color:#e6db74">&amp;#39;accounts-service&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span>description:&lt;span style="color:#e6db74">&amp;#39;应用名称 必须使用小写 需跟maven构建中一致&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> string&lt;span style="color:#f92672">(&lt;/span>name:&lt;span style="color:#e6db74">&amp;#39;PROJECT_NAMESPACE&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span>defaultValue: &lt;span style="color:#e6db74">&amp;#39;basebiz&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span>description:&lt;span style="color:#e6db74">&amp;#39;部署项目集名称&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> string&lt;span style="color:#f92672">(&lt;/span>name:&lt;span style="color:#e6db74">&amp;#39;SERVICE_SRC_PATH&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span>defaultValue: &lt;span style="color:#e6db74">&amp;#39;accounts-service-webapp&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span>description:&lt;span style="color:#e6db74">&amp;#39;war包路径&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> string&lt;span style="color:#f92672">(&lt;/span>name:&lt;span style="color:#e6db74">&amp;#39;PROGECT_GIT_PATH&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span>defaultValue:&lt;span style="color:#e6db74">&amp;#39;basebiz/accounts-service.git&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span>description:&lt;span style="color:#e6db74">&amp;#39;项目gitlabpath &amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> string&lt;span style="color:#f92672">(&lt;/span>name:&lt;span style="color:#e6db74">&amp;#39;TAG_NAME&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span>defaultValue: &lt;span style="color:#e6db74">&amp;#39;&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span>description:&lt;span style="color:#e6db74">&amp;#39;tag 发布线上必须填写 格式v20210101(v+当前日期)&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> string&lt;span style="color:#f92672">(&lt;/span>name:&lt;span style="color:#e6db74">&amp;#39;PODCOUNT&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span>defaultValue: &lt;span style="color:#e6db74">&amp;#39;2&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span>description:&lt;span style="color:#e6db74">&amp;#39;部署pod数量&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> string&lt;span style="color:#f92672">(&lt;/span>name:&lt;span style="color:#e6db74">&amp;#39;HEALTH_CHECK_URI&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span>defaultValue: &lt;span style="color:#e6db74">&amp;#39;/api/actuator/health&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span>description:&lt;span style="color:#e6db74">&amp;#39;健康检测地址&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> environment &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//构建镜像
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> REGISTRY &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;hub.xxxx.cn&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> DOCKERHUB_NAMESPACE &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;app&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> DOCKER_CREDENTIAL_ID &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;dockerhub-account&amp;#39;&lt;/span> &lt;span style="color:#75715e">//hub账号密钥
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> GITHUB_CREDENTIAL_ID &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;gitlab-account&amp;#39;&lt;/span> &lt;span style="color:#75715e">//gitlab账号密钥
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">//环境部署凭证
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> KUBECONFIG_CREDENTIAL_ID_DEV &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;testing-kubeconfig&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> KUBECONFIG_CREDENTIAL_ID_VIEW &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;xxxxaliyun-testing&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> KUBECONFIG_CREDENTIAL_ID_PROD &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;xxx-prod&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> DOWNLOAD_BASEDOMAIN &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;gitlab.xxxx.cn&amp;#39;&lt;/span> &lt;span style="color:#75715e">//公共资源下载
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> COMMIT_ID&lt;span style="color:#f92672">=&lt;/span> sh&lt;span style="color:#f92672">(&lt;/span> returnStdout: &lt;span style="color:#66d9ef">true&lt;/span>&lt;span style="color:#f92672">,&lt;/span> script: &lt;span style="color:#e6db74">&amp;#39;git rev-parse --short HEAD&amp;#39;&lt;/span>&lt;span style="color:#f92672">).&lt;/span>&lt;span style="color:#a6e22e">trim&lt;/span>&lt;span style="color:#f92672">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> stages &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> stage &lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#39;迁出代码&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> steps &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> checkout&lt;span style="color:#f92672">(&lt;/span>scm&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> stage &lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#39;编译&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> steps &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> container &lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#39;maven&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//***************************************
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">//**************下载通用模版***************
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> sh &lt;span style="color:#e6db74">&amp;#39;curl -o `pwd`/start.sh https://${DOWNLOAD_BASEDOMAIN}/base/basicevn/-/raw/master/shell/springboot-start.sh&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sh &lt;span style="color:#e6db74">&amp;#39;curl -o `pwd`/settings.xml https://${DOWNLOAD_BASEDOMAIN}/base/basicevn/-/raw/master/setting.xml&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sh &lt;span style="color:#e6db74">&amp;#39;curl -o `pwd`/Dockerfile https://${DOWNLOAD_BASEDOMAIN}/base/basicevn/-/raw/master/dockerfile/javaservice/dockerfile&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//***************************************
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> sh &lt;span style="color:#e6db74">&amp;#39;mkdir `pwd`/yaml&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sh &lt;span style="color:#e6db74">&amp;#39;curl -o `pwd`/yaml/devops-java.yaml https://${DOWNLOAD_BASEDOMAIN}/base/basicevn/-/raw/master/yaml/java-service-v1.0.0.yaml&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sh &lt;span style="color:#e6db74">&amp;#39;mvn -Dmaven.test.skip=true -gs `pwd`/settings.xml clean package -U -Denv.trackerror=true&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> stage&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#39;构建并推送镜像&amp;#39;&lt;/span>&lt;span style="color:#f92672">){&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> steps&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> container &lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#39;maven&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sh &lt;span style="color:#e6db74">&amp;#39;docker build --build-arg SERVICE_SRC_PATH=$SERVICE_SRC_PATH \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> --build-arg GENERATE_PATH=generated-resources/appassembler/jsw/$APP_NAME \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> --build-arg RELEASE_NAME=$BRANCH_NAME-$BUILD_NUMBER \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> --build-arg APP_NAME=$APP_NAME \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> -f Dockerfile \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> -t $REGISTRY/$DOCKERHUB_NAMESPACE/$APP_NAME:$BRANCH_NAME-$TAG_NAME-$BUILD_NUMBER-$COMMIT_ID \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> --no-cache .&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> withCredentials&lt;span style="color:#f92672">([&lt;/span>usernamePassword&lt;span style="color:#f92672">(&lt;/span>passwordVariable &lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#39;DOCKER_PASSWORD&amp;#39;&lt;/span> &lt;span style="color:#f92672">,&lt;/span>usernameVariable &lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#39;DOCKER_USERNAME&amp;#39;&lt;/span> &lt;span style="color:#f92672">,&lt;/span>credentialsId &lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#e6db74">&amp;#34;$DOCKER_CREDENTIAL_ID&amp;#34;&lt;/span> &lt;span style="color:#f92672">,)])&lt;/span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sh &lt;span style="color:#e6db74">&amp;#39;echo &amp;#34;$DOCKER_PASSWORD&amp;#34; | docker login $REGISTRY -u &amp;#34;$DOCKER_USERNAME&amp;#34; --password-stdin&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sh &lt;span style="color:#e6db74">&amp;#39;docker push $REGISTRY/$DOCKERHUB_NAMESPACE/$APP_NAME:$BRANCH_NAME-$TAG_NAME-$BUILD_NUMBER-$COMMIT_ID&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sh &lt;span style="color:#e6db74">&amp;#39;docker tag $REGISTRY/$DOCKERHUB_NAMESPACE/$APP_NAME:$BRANCH_NAME-$TAG_NAME-$BUILD_NUMBER-$COMMIT_ID $REGISTRY/$DOCKERHUB_NAMESPACE/$APP_NAME:latest &amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sh &lt;span style="color:#e6db74">&amp;#39;docker push $REGISTRY/$DOCKERHUB_NAMESPACE/$APP_NAME:latest &amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> stage&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#34;gitlab 打 tag&amp;#34;&lt;/span>&lt;span style="color:#f92672">){&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> when&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expression&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> params&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#a6e22e">TAG_NAME&lt;/span> &lt;span style="color:#f92672">=~&lt;/span> &lt;span style="color:#e6db74">/v.*/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> steps &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> withCredentials&lt;span style="color:#f92672">([&lt;/span>usernamePassword&lt;span style="color:#f92672">(&lt;/span>credentialsId: &lt;span style="color:#e6db74">&amp;#34;$GITHUB_CREDENTIAL_ID&amp;#34;&lt;/span>&lt;span style="color:#f92672">,&lt;/span> passwordVariable: &lt;span style="color:#e6db74">&amp;#39;GIT_PASSWORD&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span> usernameVariable: &lt;span style="color:#e6db74">&amp;#39;GIT_USERNAME&amp;#39;&lt;/span>&lt;span style="color:#f92672">)])&lt;/span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sh &lt;span style="color:#e6db74">&amp;#39;git config --global user.email &amp;#34;xxxx@xxxx.cn&amp;#34; &amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sh &lt;span style="color:#e6db74">&amp;#39;git config --global user.name &amp;#34;xxxx&amp;#34; &amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sh &lt;span style="color:#e6db74">&amp;#39;git tag -a $TAG_NAME-$BUILD_NUMBER -m &amp;#34;$TAG_NAME&amp;#34; &amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sh &lt;span style="color:#e6db74">&amp;#39;git push https://$GIT_USERNAME:$GIT_PASSWORD@$DOWNLOAD_BASEDOMAIN/$PROGECT_GIT_PATH --tags --ipv4&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> stage&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#39;部署测试环境&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// when{
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// branch &amp;#39;master&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#75715e">// }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> steps &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">//input(id: &amp;#39;deploy-to-dev&amp;#39;, message: &amp;#39;deploy to dev?&amp;#39;)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> kubernetesDeploy&lt;span style="color:#f92672">(&lt;/span>configs: &lt;span style="color:#e6db74">&amp;#39;yaml/**&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span> enableConfigSubstitution: &lt;span style="color:#66d9ef">true&lt;/span>&lt;span style="color:#f92672">,&lt;/span> kubeconfigId: &lt;span style="color:#e6db74">&amp;#34;$KUBECONFIG_CREDENTIAL_ID_DEV&amp;#34;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> stage&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#39;部署生产环境&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> when&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expression&lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> params&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#a6e22e">TAG_NAME&lt;/span> &lt;span style="color:#f92672">=~&lt;/span> &lt;span style="color:#e6db74">/v.*/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> steps &lt;span style="color:#f92672">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> input&lt;span style="color:#f92672">(&lt;/span>id: &lt;span style="color:#e6db74">&amp;#39;deploy-to-prod&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span> message: &lt;span style="color:#e6db74">&amp;#39;是否允许发布生产?&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetesDeploy&lt;span style="color:#f92672">(&lt;/span>configs: &lt;span style="color:#e6db74">&amp;#39;yaml/**&amp;#39;&lt;/span>&lt;span style="color:#f92672">,&lt;/span> enableConfigSubstitution: &lt;span style="color:#66d9ef">true&lt;/span>&lt;span style="color:#f92672">,&lt;/span> kubeconfigId: &lt;span style="color:#e6db74">&amp;#34;$KUBECONFIG_CREDENTIAL_ID_PROD&amp;#34;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Jenkinsfile 文件描述了如下几个过程：&lt;/p></description></item><item><title>在 openEuler 22.03 上安装 KubeSphere 实战教程</title><link>https://openksc.github.io/zh/blogs/deploy-kubesphere-on-openeuler22.03/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deploy-kubesphere-on-openeuler22.03/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;h3 id="知识点">知识点&lt;/h3>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>KubeKey 安装部署 KubeSphere 和 Kubernetes&lt;/li>
&lt;li>openEuler 操作系统的基本配置&lt;/li>
&lt;li>Kubernetes 常用工作负载的创建&lt;/li>
&lt;/ul>
&lt;h3 id="演示服务器配置">演示服务器配置&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">3&lt;/td>
 &lt;td style="text-align: center">12&lt;/td>
 &lt;td style="text-align: center">48&lt;/td>
 &lt;td style="text-align: center">120&lt;/td>
 &lt;td style="text-align: center">600&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h3 id="演示环境涉及软件版本信息">演示环境涉及软件版本信息&lt;/h3>
&lt;ul>
&lt;li>操作系统：&lt;strong>openEuler 22.03 LTS SP1 x86_64&lt;/strong>&lt;/li>
&lt;li>KubeSphere：&lt;strong>3.3.2&lt;/strong>&lt;/li>
&lt;li>Kubernetes：&lt;strong>v1.25.5&lt;/strong>&lt;/li>
&lt;li>KubeKey: &lt;strong>v3.0.7&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>本文介绍了如何在 &lt;strong>openEuler 22.03 LTS SP1&lt;/strong> X86 架构服务器上部署 KubeSphere 和 Kubernetes 集群。我们将使用 KubeSphere 开发的 KubeKey 工具实现自动化部署，在三台服务器上实现高可用模式最小化部署 Kubernetes 集群和 KubeSphere。我们将提供详细的部署说明，以便读者轻松地完成部署过程。&lt;/p></description></item><item><title>在 Pod 中如何获取客户端的真实 IP</title><link>https://openksc.github.io/zh/blogs/how-to-get-real-ip-in-pod/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/how-to-get-real-ip-in-pod/</guid><description>&lt;blockquote>
&lt;p>Kubernetes 依靠 kube-proxy 组件实现 Service 的通信与负载均衡。在这个过程中，由于使用了 SNAT 对源地址进行了转换，导致 Pod 中的服务拿不到真实的客户端 IP 地址信息。本篇主要解答了在 Kubernetes 集群中负载如何获取客户端真实 IP 地址这个问题。&lt;/p>&lt;/blockquote>
&lt;h2 id="创建一个后端服务">创建一个后端服务&lt;/h2>
&lt;h3 id="服务选择">服务选择&lt;/h3>
&lt;p>这里选择 &lt;code>containous/whoami&lt;/code> 作为后端服务镜像。在 Dockerhub 的介绍页面，可以看到访问其 80 端口时，会返回客户端的相关信息。在代码中，我们可以在 Http 头部中拿到这些信息。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>Hostname : 6e0030e67d6a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IP : 127.0.0.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IP : ::1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IP : 172.17.0.27
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IP : fe80::42:acff:fe11:1b
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>GET / HTTP/1.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Host: 0.0.0.0:32769
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>User-Agent: curl/7.35.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Accept: */*
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="集群环境">集群环境&lt;/h3>
&lt;p>简单介绍一下集群的状况。集群有三个节点，一个 master ，两个 worker 节点。如下图:&lt;/p>
&lt;p>&lt;img src="../../../images/blogs/how-to-get-real-ip-in-pod/cluster-info.png" alt="">&lt;/p>
&lt;h3 id="创建服务">创建服务&lt;/h3>
&lt;ul>
&lt;li>创建企业空间、项目&lt;/li>
&lt;/ul>
&lt;p>如下图所示，这里将企业空间和项目命名为 realip&lt;/p>
&lt;p>&lt;img src="../../../images/blogs/how-to-get-real-ip-in-pod/create-ns.png" alt="">&lt;/p>
&lt;ul>
&lt;li>创建服务&lt;/li>
&lt;/ul>
&lt;p>这里创建无状态服务，选择 &lt;code>containous/whoami&lt;/code> 镜像，使用默认端口。&lt;/p>
&lt;p>&lt;img src="../../../images/blogs/how-to-get-real-ip-in-pod/create-svc.png" alt="">&lt;/p>
&lt;ul>
&lt;li>将服务改为 NodePort 模式&lt;/li>
&lt;/ul>
&lt;p>编辑服务的外网访问方式，修改为 NodePort 模式。&lt;/p></description></item><item><title>在 Ubuntu 22.04 上安装 KubeSphere 实战教程</title><link>https://openksc.github.io/zh/blogs/deploy-kubesphere-on-ubuntu22.04/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/deploy-kubesphere-on-ubuntu22.04/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;h3 id="知识点">&lt;strong>知识点&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>定级：&lt;strong>入门级&lt;/strong>&lt;/li>
&lt;li>KubeKey 安装部署 KubeSphere 和 Kubernetes&lt;/li>
&lt;li>Ubuntu 操作系统的基本配置&lt;/li>
&lt;li>Kubernetes 常用工作负载的创建&lt;/li>
&lt;/ul>
&lt;h3 id="演示服务器配置">&lt;strong>演示服务器配置&lt;/strong>&lt;/h3>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-0&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ks-master-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">4&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">200&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-master/k8s-worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">3&lt;/td>
 &lt;td style="text-align: center">12&lt;/td>
 &lt;td style="text-align: center">48&lt;/td>
 &lt;td style="text-align: center">120&lt;/td>
 &lt;td style="text-align: center">600&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;h3 id="演示环境涉及软件版本信息">&lt;strong>演示环境涉及软件版本信息&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>操作系统：&lt;strong>Ubuntu 22.04.2 LTS&lt;/strong>&lt;/li>
&lt;li>KubeSphere：&lt;strong>v3.3.2&lt;/strong>&lt;/li>
&lt;li>Kubernetes：&lt;strong>v1.25.5&lt;/strong>&lt;/li>
&lt;li>KubeKey: &lt;strong>v3.0.7&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>本文介绍了如何在 Ubuntu 22.04 LTS 服务器上部署 KubeSphere 和 Kubernetes 集群。我们将使用 KubeSphere 开发的 KubeKey 工具实现自动化部署，在三台服务器上实现高可用模式最小化部署 Kubernetes 集群和 KubeSphere。我们将提供详细的部署说明，以便读者轻松地完成部署过程。&lt;/p></description></item><item><title>在 VMware vSphere 中构建 Kubernetes 存储环境</title><link>https://openksc.github.io/zh/blogs/kubernetes-vmware-vsphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/kubernetes-vmware-vsphere/</guid><description>&lt;blockquote>
&lt;p>作者：马伟，青云科技容器顾问，云原生爱好者，目前专注于云原生技术，云原生领域技术栈涉及 Kubernetes、KubeSphere、kubekey等。&lt;/p>&lt;/blockquote>
&lt;p>相信很多小伙伴和企业在构建容器集群时都会考虑存储选型问题，不论是块存储 / 文件存储 / 对象存储的选择，亦或是一体机 / 裸机+外置存储 / 虚拟化+存储的纠结，都是在规划容器集群时的顾虑。对于原先就有虚拟化环境的用户来说，我能否&lt;strong>直接搭建容器集群在虚拟化环境中，并直接使用现有的存储用于容器&lt;/strong>呢？本文将以 VMware vSphere CNS+KubeSphere 为工具在虚拟化环境搭建容器及存储环境。&lt;/p>
&lt;h2 id="vsphere-cns">vSphere CNS&lt;/h2>
&lt;p>VMware vSphere Cloud Native Storage（CNS）是 VMware 结合 vSphere 和 K8s 提供容器卷管理的组件。K8s 存储有 in tree 和 out of tree 两种存储类型，in tree 存储如 AWS EBS，Ceph RBD 等。是 VMware in tree 存储 VCP 演进到 out of tree 存储提供的符合 CSI 插件规范的容器存储。它由两部分构成：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>vCenter 的 CNS 控制平台&lt;/strong>。vCenter 调度底层虚拟化结合的存储平台创建容器存储卷。&lt;/li>
&lt;li>&lt;strong>K8s 的 CSI 存储插件&lt;/strong>。对 vCenter 创建的卷进行附加 / 分离 / 挂载等操作。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/1660111355913-8b5fed67-a378-4b31-a418-a17f066c28d2.png" alt="">&lt;/p></description></item><item><title>长虹电器基于 Kubernetes 的弹性伸缩与金丝雀发布实践</title><link>https://openksc.github.io/zh/blogs/changhong-kubernetes-autoscaling-canaryrelease/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/changhong-kubernetes-autoscaling-canaryrelease/</guid><description>&lt;h2 id="背景">背景&lt;/h2>
&lt;p>2013 年四川长虹电器发布了公司面向互联网时代的全新战略规划和产业布局，首次提出将智能化、网络化和协同化做为新的三坐标体系的发力方向。为了推进智能化战略落地，加快黑电转型升级的计划，长虹电器于 2015 年组建了互联网事业部。&lt;/p>
&lt;p>2019 年初，我们运维的各类系统产生的数据日均接近三亿条，服务器、虚拟机多种多样，零散分布在 AWS、腾讯云、阿里云、华为云及本地机房中，运行的应用数以千计，而且各个应用依赖的运行环境差异极大。&lt;/p>
&lt;p>同一年集团提出了新的运营计划，要求智能电视增值运营平台要按照用户规模翻倍的目标进行业务规划，若要实现这个目标，当时的基础平台支撑起来难度颇大，甚至可以断言无法支撑，所以我们亟需对整个基础架构进行改造。&lt;/p>
&lt;h2 id="为什么使用-kubesphere">为什么使用 KubeSphere？&lt;/h2>
&lt;h3 id="面临问题">面临问题&lt;/h3>
&lt;p>在当时，我们面临的问题大致有这六点：&lt;/p>
&lt;ol>
&lt;li>系统初始化和运行环境部署复杂：开通资源的方式非常落后，审批通过后由运维手动建立虚拟机，之后在虚拟机上配置检验项，之后再按照这台虚拟机需要运行的应用去安装运行环境和依赖包。&lt;/li>
&lt;li>迭代时要求服务短中断甚至零中断：由于我们的服务全是面向最终用户的，集团提出的新要求是短中断甚至零中断，避免引起用户投诉，坦率而言，依照当时我们基础平台的能力，是无法满足这个要求的。&lt;/li>
&lt;li>应用需要多版本共存并且可以基于这些版本提供A/B测试：在当时还是以虚机的方式来响应资源，已经跟不上需求了。&lt;/li>
&lt;li>项目间物理隔离，资源使用率低：因为我们部门同时运维自己开发的项目和其他部门、公司开发的智能电视应用，基于公司的权责规定，项目需要进行隔离，就出现了部分项目资源使用率非常低的问题，导致了成本浪费。&lt;/li>
&lt;li>不具备自动扩缩容能力：由于电视业务的特性，其实在大多数时候我们的资源都是空置的，而基于当时的状况，我们也无法提供自动扩缩容的能力。&lt;/li>
&lt;li>项目团队规模不大，无法投入过多时间成本：事实上哪怕到了今天，我们部门研发、测试、运维加起来都不到 40 人，所以人力成本和时间成本控制对于我们部门来讲，是重中之重。&lt;/li>
&lt;/ol>
&lt;h3 id="方案选择">方案选择&lt;/h3>
&lt;p>基于对上述问题的思考，我们开始寻找图形化的管理工具作为突破口，在寻找图形化工具的时候我接触到了 KubeSphere，然后进行了试用。我们发现 KubeSphere 的设计思路和使用体验等方面都非常棒，有很多功能如&lt;strong>企业空间隔离&lt;/strong>、&lt;strong>弹性伸缩&lt;/strong>、&lt;strong>应用治理&lt;/strong>、&lt;strong>日志落盘&lt;/strong>等极大的降低了改造后的使用难度。&lt;/p>
&lt;p>KubeSphere 在我们需求中的优势：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>图形化操作&lt;/strong>，极大的降低了开发、测试、运维同事的学习成本。&lt;/li>
&lt;li>&lt;strong>多租户隔离&lt;/strong>，满足了我们项目资源隔离的需求。&lt;/li>
&lt;li>&lt;strong>不绑定云厂商&lt;/strong>，方便我们后续整体迁移和资源集中。&lt;/li>
&lt;li>常用功能的集成减少了诸如 API Gateway 等模块的开发工作量。&lt;/li>
&lt;li>&lt;strong>多集群集中管理&lt;/strong>的特性，使得我们通过一个平台集中管理当时散落在各处的服务器资源具备了可能性。&lt;/li>
&lt;/ul>
&lt;p>经过一个多月的试用，我们最终决定，使用 KubeSphere 作为我们云端服务基础设施平台的建设方案。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/advantages-KubeSphere.png" alt="KubeSphere 在我们需求中的优势">&lt;/p>
&lt;h2 id="kubesphere-的落地实践">KubeSphere 的落地实践&lt;/h2>
&lt;h3 id="业务流程总结">业务流程总结&lt;/h3>
&lt;p>在确定方案选型后，我们梳理了当时开发和维护的项目，总结了共性部分，提取了一个流程图。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/workflow-changhong.png" alt="业务流程图">&lt;/p>
&lt;p>流程图并不涉及具体的业务逻辑，可以分解如下：&lt;/p>
&lt;ul>
&lt;li>电视终端发起 http 请求到负载均衡器&lt;/li>
&lt;li>负载均衡器分发请求到接口应用上&lt;/li>
&lt;li>接口应用将写请求通过 MQ 消峰填谷后转移至后端应用&lt;/li>
&lt;li>后端应用写入 MySQL 或者 NoSQL，同时将数据同步到缓存&lt;/li>
&lt;li>读请求则由接口应用读取缓存数据后直接返回电视终端&lt;/li>
&lt;li>运营活动则是由运营同事操作管理后台将数据写入数据库&lt;/li>
&lt;/ul>
&lt;p>对于运维而言，这样的结构简单清晰。那么我们落地的思路也变得清晰起来：&lt;/p>
&lt;p>以容器管理平台 KubeSphere 为基础来运行无状态服务（接口应用、后端应用），以及可视化管理 Kubernetes 和基础设施资源。而 PaaS 提供有状态的服务，比如消息中间件、数据库等。&lt;/p>
&lt;h3 id="落地方案设计">落地方案设计&lt;/h3>
&lt;p>我们使用 KubeShere 管理多个公有云集群，利用 KubeSphere 的多租户管理的便捷性，将我们部门自行开发的业务和其他部门开发的业务分成了两个企业空间，同时使用了 KubeSphere 多个可插拔组件来解决一些我们业务的痛点。&lt;/p></description></item><item><title>征服 Docker 镜像访问限制！KubeSphere v3.4.1 成功部署全攻略</title><link>https://openksc.github.io/zh/blogs/breaching-docker-image-limits-to-deploy-kubesphere-3.4.1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/breaching-docker-image-limits-to-deploy-kubesphere-3.4.1/</guid><description>&lt;p>&lt;strong>近期，KubeSphere 社区的讨论中频繁出现关于 Docker 官方镜像仓库访问受限的问题。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>本文旨在为您提供一个详细的指南，&lt;/strong> 展示在 Docker 官方镜像访问受限的情况下，如何通过 KubeKey &lt;strong>v3.1.2&lt;/strong> 一次性成功部署 KubeSphere &lt;strong>v3.4.1&lt;/strong> 以及 Kubernetes &lt;strong>v1.28.8&lt;/strong> 集群。这将帮助您克服访问限制，确保部署过程的顺利进行。&lt;/p>
&lt;p>&lt;strong>实战服务器配置(架构1:1复刻小规模生产环境，配置略有不同)&lt;/strong>&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">主机名&lt;/th>
 &lt;th style="text-align: center">IP&lt;/th>
 &lt;th style="text-align: center">CPU&lt;/th>
 &lt;th style="text-align: center">内存&lt;/th>
 &lt;th style="text-align: center">系统盘&lt;/th>
 &lt;th style="text-align: center">数据盘&lt;/th>
 &lt;th style="text-align: center">用途&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-1&lt;/td>
 &lt;td style="text-align: center">192.168.9.91&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane/Worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-2&lt;/td>
 &lt;td style="text-align: center">192.168.9.92&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane/Worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">ksp-control-3&lt;/td>
 &lt;td style="text-align: center">192.168.9.93&lt;/td>
 &lt;td style="text-align: center">8&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">40&lt;/td>
 &lt;td style="text-align: center">100&lt;/td>
 &lt;td style="text-align: center">KubeSphere/k8s-control-plane/Worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">合计&lt;/td>
 &lt;td style="text-align: center">3&lt;/td>
 &lt;td style="text-align: center">24&lt;/td>
 &lt;td style="text-align: center">48&lt;/td>
 &lt;td style="text-align: center">120&lt;/td>
 &lt;td style="text-align: center">300&lt;/td>
 &lt;td style="text-align: center">&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>实战环境涉及软件版本信息&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>操作系统：&lt;strong>openEuler 22.03 LTS SP3 x86_64&lt;/strong>&lt;/li>
&lt;li>KubeSphere：&lt;strong>v3.4.1&lt;/strong>&lt;/li>
&lt;li>Kubernetes：&lt;strong>v1.28.8&lt;/strong>&lt;/li>
&lt;li>KubeKey: &lt;strong>v3.1.2&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="1-前置条件">1. 前置条件&lt;/h2>
&lt;p>请参考 &lt;a href="https://mp.weixin.qq.com/s/YDnvnuTqYfmgvF3HGOJ4WQ" target="_blank" rel="noopener noreferrer">Kubernetes 集群节点 openEuler 22.03 LTS SP3 系统初始化指南&lt;/a>，完成操作系统初始化配置。&lt;/p></description></item><item><title>蜘点云原生之 KubeSphere 落地实践过程</title><link>https://openksc.github.io/zh/blogs/best-practices-of-kubesphere-in-zhidian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/best-practices-of-kubesphere-in-zhidian/</guid><description>&lt;h2 id="公司平台介绍">公司平台介绍&lt;/h2>
&lt;p>蜘点成立于 2016 年 4 月，致力于打造社区电商业务（解决最后 3 公里的配送问题）。当初通过自建直营渠道、自建仓库、自建大型社区仓、和采用加盟仓的方式，实现在社区的电商业务的发展，配送本地化。最多的时候在全国各个省都有分公司及下属子公司，在每个省都有省仓，在南北的主要城市都建有大型仓。&lt;/p>
&lt;p>后面随着电商行业的落幕，公司又转型做企业数字化整体解决方案（产业互联网方向）。整体发展如下图：&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/ks-zhidian-1.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/ks-zhidian-2.png" alt="">&lt;/p>
&lt;h2 id="平台背景介绍">平台背景介绍&lt;/h2>
&lt;p>公司通过购买服务器组建了一个内部云，托管在 IDC 机房中，一直使用 VMware 的虚拟化技术，来实现虚拟机的管理。随着业务增加，项目从单体架构向分布式架构演进，虚拟机数量也随着增加，给开发与运维管理来了不少问题，随着微服务技术的发展，采用容器化架构成为了解决公司底层架构的问题。&lt;/p>
&lt;ul>
&lt;li>业务快速发展，不新增虚拟主机，环境搭建复杂，早期通过虚拟机模板解决；&lt;/li>
&lt;li>各个项目组之间的业务调用，都是通过 HTTP 接口交互，效率不高；&lt;/li>
&lt;li>部署靠人工编译打包上传，测试/上线，无 CI/CD，开发效率低；&lt;/li>
&lt;li>运维压力大，运维资源缺乏，各个服务、中间件的监控不到位，虽有 Zabbix，但管理不过来，缺少统一的监控面板；&lt;/li>
&lt;li>虚拟机的资源难以动态分配利用，资源被固定化；&lt;/li>
&lt;li>缺少专业的运维人员，环境安装、监控不够完善，资源使用情况难可视化（运维人员就一个）；&lt;/li>
&lt;li>前端组也想采用容器化部署，不要在本地打包，通过 FTP 上传静态文件的方式；&lt;/li>
&lt;li>运维人员想减少虚拟机数量，新上线业务不需要创建很多虚拟机，只需要增加少量节点就可以。&lt;/li>
&lt;/ul>
&lt;h2 id="平台选型">平台选型&lt;/h2>
&lt;h3 id="业务痛点">业务痛点&lt;/h3>
&lt;p>从前面的介绍，在从单体架向分布式架构的演变过程中，伴随着业务的快速发现，与快速响应，基础模块及业务模块越来越多，团队都忙在打包部署的过程中。&lt;/p>
&lt;ul>
&lt;li>修 Bug 打包部署。&lt;/li>
&lt;li>上线打包部署。&lt;/li>
&lt;li>每次上线全团队 StayBy, 折腾至深夜。&lt;/li>
&lt;li>效率低下，版本延迟。&lt;/li>
&lt;/ul>
&lt;h3 id="引入-jenkins-半自动化部署">引入 Jenkins 半自动化部署&lt;/h3>
&lt;p>为解决团队的效率问题，首先引入 Jenkins，通过 Jenkins 解决大部分部署问题。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/ks-zhidian-3.png" alt="">&lt;/p>
&lt;h3 id="引入-kubernetesk8s">引入 Kubernetes（K8s）&lt;/h3>
&lt;p>Jenkins 的引入，已经能很大的提高效率，但还是存在一些问题：&lt;/p>
&lt;ul>
&lt;li>服务太多，每次部署要排队。&lt;/li>
&lt;li>虚拟机太多，维护 Shell 脚本成本高。&lt;/li>
&lt;li>资源利用率低，没有用到点上。&lt;/li>
&lt;/ul>
&lt;p>自建 K8s 集群，可以解决繁锁的 Shell 脚本问题，结合 Jenkins 的 K8s 的插件，通过 Dockerfile + yaml 的方式进行部署。&lt;/p></description></item><item><title>中通快递关键业务和复杂架构挑战下的 Kubernetes 集群服务暴露实践</title><link>https://openksc.github.io/zh/blogs/zto-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/zh/blogs/zto-kubernetes/</guid><description>&lt;blockquote>
&lt;p>本文是上海站 Meetup 讲师王文虎根据其分享内容整理的文章。&lt;a href="https://kubesphere.com.cn/live/zhongtong-shanghai/" target="_blank" rel="noopener noreferrer">点击查看视频回放&lt;/a>&lt;/p>&lt;/blockquote>
&lt;p>KubeSphere 社区的小伙伴们，大家好。我是中通快递容器云平台的研发工程师王文虎，主要负责中通快递容器云平台开发、应用容器化推广、容器平台运维等工作。非常感谢 KubeSphere 社区的邀请，让我有机会跟大家分享中通快递关键业务和复杂架构挑战下的 Kubernetes 集群服务暴露实践。&lt;/p>
&lt;h2 id="zke-容器管理平台">ZKE 容器管理平台&lt;/h2>
&lt;p>首先介绍一下中通的容器云管理平台 ZKE。ZKE 平台是基于 KubeSphere 开发的，现在管理的中通内部集群超过十个，包含开发、测试、预发布、生产等环境，所有用户都通过 ZKE 平台管理容器应用。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/ZKE.png" alt="">&lt;/p>
&lt;h2 id="kubernetes-集群服务暴露方案">Kubernetes 集群服务暴露方案&lt;/h2>
&lt;p>根据中通的实际业务需求和一些探索，梳理出了中通 Kubernetes 集群服务暴露的几种方案。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/%E6%9A%B4%E9%9C%B2%E6%96%B9%E6%A1%88.png" alt="">&lt;/p>
&lt;h3 id="dubbo-服务之间访问">Dubbo 服务之间访问&lt;/h3>
&lt;p>中通大部分应用都是基于 Java 语言开发的，使用的微服务框架为 Dubbo。在上容器之初，我们考虑到虚拟机和容器并存的场景可能会持续很长时间，所以在规划 Kubernetes 集群的时候，通过把容器网络和物理网络打通的方式，来解决 Dubbo 服务在容器和虚拟机混布的场景下互相调用的问题。&lt;/p>
&lt;ul>
&lt;li>如何打通 Kubernetes 容器网络和物理网络？&lt;/li>
&lt;/ul>
&lt;p>我们内部环境中 Kubernetes 集群网络组件使用 Calico BGP 模式，数据中心物理网络也开启了 BGP 路由协议，通过在物理网络上开启 BGP RR（Route Reflector），避免后期集群规模太大导致 BGP 宣告路由条目过多的问题。BGP RR 和 Kubernetes 集群节点建立 EBGP 邻居，互相学习路由。&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-community/images/%E6%9A%B4%E9%9C%B2%E6%96%B9%E6%A1%88-2.png" alt="">&lt;/p>
&lt;h3 id="泛域名方式访问">泛域名方式访问&lt;/h3>
&lt;p>在初步推广开发和测试容器化时，我们遇到最多的问题就是用户在应用发布到容器环境后如何访问。&lt;/p>
&lt;p>用户在 ZKE 平台上创建 Ingress 以后，该域名是不能访问的，必须要运维把域名指向集群 Ingress Controller，而且公司申请域名需要走 OA 流程，所以这就使得我们的容器环境在初始推广阶段进度很慢。&lt;/p></description></item></channel></rss>
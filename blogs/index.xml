<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Open Source Community - Blogs | KubeSphere on</title><link>https://openksc.github.io/blogs/</link><description>Recent content in Open Source Community - Blogs | KubeSphere on</description><generator>Hugo</generator><language>en-US</language><atom:link href="https://openksc.github.io/blogs/index.xml" rel="self" type="application/rss+xml"/><item><title>A Container Platform: What is the Value of KubeSphere</title><link>https://openksc.github.io/blogs/value-of-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/value-of-kubesphere/</guid><description>&lt;h2 id="a-promising-newcomer">A Promising Newcomer&lt;/h2>
&lt;p>As &lt;strong>a promising newcomer&lt;/strong> of the cloud native family, KubeSphere has gained widespread recognition among its users and developers since it joined the open source community nearly two years ago. This article illustrates the position and value of KubeSphere from scratch in a straightforward way and sheds light on why different teams have chosen KubeSphere.&lt;/p>
&lt;h2 id="for-enterprises">For Enterprises&lt;/h2>
&lt;p>KubeSphere is a &lt;strong>multi-tenant&lt;/strong> &lt;a href="https://kubesphere.io/" target="_blank" rel="noopener noreferrer">container platform built on Kubernetes&lt;/a> with applications at its core. It is capable of full stack IT automated operation and maintenance, streamlining the DevOps workflow for enterprises. KubeSphere not only helps enterprises quickly establish a Kubernetes cluster in public cloud or private data center, but also provides a set of multi-functional wizard interfaces.&lt;/p></description></item><item><title>Adding Master Nodes to Achieve HA: One of the Best Practices for Using KubeKey</title><link>https://openksc.github.io/blogs/add-master-for-ha-using-kubekey/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/add-master-for-ha-using-kubekey/</guid><description>&lt;p>As demonstrated in my &lt;a href="https://kubesphere.io/blogs/scale-kubernetes-cluster-using-kubekey/" target="_blank" rel="noopener noreferrer">last article&lt;/a>, you can use KubeKey to easily scale in and out your cluster. As I only had one master node in the example, the cluster did not feature high availability. In this post, I will continue to demonstrate how to scale out your cluster while by adding master nodes this time to achieve high availability.&lt;/p>
&lt;p>The steps are listed as follows:&lt;/p>
&lt;ol>
&lt;li>Download KubeKey.&lt;/li>
&lt;li>Use KubeKey to retrieve cluster information with a configuration file created automatically.&lt;/li>
&lt;li>Add your node and load balancer information in the file and apply the configuration.&lt;/li>
&lt;/ol>
&lt;h2 id="prepare-hosts">Prepare Hosts&lt;/h2>
&lt;p>Here is my node information of the existing Kubernetes cluster.&lt;/p></description></item><item><title>Argo CD: A Tool for Kubernetes DevOps</title><link>https://openksc.github.io/blogs/argo-cd-a-tool-for-devops/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/argo-cd-a-tool-for-devops/</guid><description>&lt;p>In this post, I'll show you how Argo CD betters &lt;a href="https://kubesphere.io/devops/" target="_blank" rel="noopener noreferrer">Kubernetes DevOps&lt;/a> process. Before we begin, let's look at some background information.&lt;/p>
&lt;h2 id="argo-cd-capability">Argo CD Capability&lt;/h2>
&lt;h3 id="gitops-the-origin">GitOps: The origin&lt;/h3>
&lt;p>As you may already know, Weaveworks published a post titled &lt;a href="https://www.weave.works/blog/gitops-operations-by-pull-request" target="_blank" rel="noopener noreferrer">GitOps - Operations by Pull Request&lt;/a> in 2017. Alexis, the author, introduced a way of deployment by using Git as source of truth.&lt;/p>
&lt;p>In GitOps practices, we need to define and manage software infrastructures in Git repositories. Software infrastructures involve not only IaaS and Kubernetes but also applications. Everyone can make modifications to software infrastructures by submitting a Pull Request, and an automated program will perform those modifications.&lt;/p></description></item><item><title>Canary Release in Kubernetes with Nginx Ingress</title><link>https://openksc.github.io/blogs/canary-release-with-nginx-ingress/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/canary-release-with-nginx-ingress/</guid><description>&lt;p>An increasing number of applications are running in cloud-native environments nowadays. Application developers have to effectively develop, build, test, and release their cloud-native applications while minimizing errors in code and reducing impacts on users. One way to address this challenge is to implement canary releases.&lt;/p>
&lt;p>This article gives a quick look on the concept of canary release, and then focuses on how to implement a canary release in Kubernetes with Nginx Ingress.&lt;/p></description></item><item><title>Come on! Your Exclusive Certificate is Ready</title><link>https://openksc.github.io/blogs/kubesphere-certificates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/kubesphere-certificates/</guid><description>&lt;p>Itâ€™s highly appreciated that the KubeSphere community has nearly 100 contributors by February 2021. Voted and approved by the KubeSphere Community Technical Committee and the KubeSphere Steering Committee, 10 Ambassadors, 15 Talented Speakers, 22 outstanding contributors, and 6 active contributors with remarkable contributions were selected throughout the year.&lt;/p>
&lt;h2 id="vote-of-thanks">Vote of Thanks&lt;/h2>
&lt;p>KubeSphere Community Steering Committee would like to express gratitude to KubeSphere Members, KubeSphere Ambassadors, KubeSphere Talented Speakers, and KubeSphere Contributors, and extend the sincerest greetings to all the members who participated in the open-source contribution to the KubeSphere community. Therefore, we prepare an exclusive gift for you.&lt;/p></description></item><item><title>Configure an NFS Storage Class on an Existing KubeSphere Cluster and Create a PersistentVolumeClaim</title><link>https://openksc.github.io/blogs/add-nfs-sc-and-create-nfs-pvc-on-kubesphere-console/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/add-nfs-sc-and-create-nfs-pvc-on-kubesphere-console/</guid><description>&lt;p>In my &lt;a href="https://kubesphere.io/blogs/install-nfs-server-client-for-kubesphere-cluster/" target="_blank" rel="noopener noreferrer">last article&lt;/a>, I talked about how to use KubeKey to create a Kubernetes and KubeSphere cluster together with NFS storage. In fact, KubeSphere provides you with great flexibility as you can use KubeKey to install NFS storage when you create a cluster while it can also be deployed separately on an existing cluster.&lt;/p>
&lt;p>KubeSphere features a highly interactive dashboard where virtually all the operations can be performed on it. In this article, I am going to demonstrate show to configure an NFS storage class on your existing KubeSphere cluster and create a PVC using the storage class.&lt;/p></description></item><item><title>Create a Highly Available Kubernetes Cluster Using Keepalived and HAproxy</title><link>https://openksc.github.io/blogs/set-up-ha-cluster-using-keepalived-haproxy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/set-up-ha-cluster-using-keepalived-haproxy/</guid><description>&lt;p>A highly available Kubernetes cluster ensures your applications run without outages which is required for production. In this connection, there are plenty of ways for you to choose from to achieve high availability. For example, if your cluster is deployed on cloud (e.g. Google Cloud and AWS), you can create load balancers on these platforms directly. At the same time, Keepalived, HAproxy and NGINX are also possible alternatives for you to achieve load balancing.&lt;/p></description></item><item><title>Deep Dive into Kubernetes Logging</title><link>https://openksc.github.io/blogs/deep-dive-into-kubernetes-logging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/deep-dive-into-kubernetes-logging/</guid><description>&lt;p>Logging is one of the &lt;a href="https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/ch04.html" target="_blank" rel="noopener noreferrer">three pillars of observability&lt;/a> in distributed systems. As such, we have seen an explosion of popular open-source (e.g. ELK stack) and mature commerical products (e.g. Splunk) to deal with logging at scale. However, in a complex system like Kubernetes, logging remains a hard problem, compounded by the continued growth of data driven by increased adoption of containerized system.&lt;/p>
&lt;p>In this post, we will look into the different types of Kubernetes logs needed for better observability, as well as approaches to collect, aggregate, and analyze those logs in Kubernetes. We will then introduce an open-source solution using fluentd and fluentbit to make logging easier.&lt;/p></description></item><item><title>Dockershim Deprecation: Is Docker Truly out of Game?</title><link>https://openksc.github.io/blogs/dockershim-out-of-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/dockershim-out-of-kubernetes/</guid><description>&lt;p>Recently, the Kubernetes community announced it is &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation" target="_blank" rel="noopener noreferrer">deprecating Docker&lt;/a> as a container runtime after v1.20.&lt;/p>
&lt;h3 id="is-docker-truly-out-of-the-game">Is Docker truly out of the game&lt;/h3>
&lt;p>Strictly speaking, whatâ€™s actually happening is that dockershim is being removed from Kubelet. In other words, Docker will not be used as the default container runtime. However, you may still integrate Docker into your environment. For more information, you can take a look at the official announcement of Kubernetes:&lt;/p></description></item><item><title>Embracing Microservices: What You Need to Know before Creating a Microservices Architecture</title><link>https://openksc.github.io/blogs/embracing-microservices-what-you-need-to-know/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/embracing-microservices-what-you-need-to-know/</guid><description>&lt;p>Traditionally, software developers use a monolithic architecture to build the entire system of a single application where all functions of the application are managed in one place. On the contrary, a microservices architecture allows an application to be separated into parts which work together. Some new design patterns have also taken shape (for example, sidecars) as developers adopt microservices architectures.&lt;/p>
&lt;p>Against this backdrop, creating a microservices-based application represents a grave challenge. Among other things, people are talking about how to categorize microservices or to what extent can they be considered as &amp;quot;fine-grained&amp;quot;. For beginners to the microservices world, they are eager to embark on the right path to microservices and are seeking for best practices for building a microservices-based application.&lt;/p></description></item><item><title>Guide to Kubernetes Ingress Controllers</title><link>https://openksc.github.io/blogs/guide-to-kubernetes-ingress-controllers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/guide-to-kubernetes-ingress-controllers/</guid><description>&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank" rel="noopener noreferrer">Ingress&lt;/a> is a Kubernetes API object that manages how external traffic is routed to services in a Kubernetes cluster. An ingress can be configured to define external URLs, load balance incoming traffic, terminate TLS, and route traffic based on path or prefix. An ingress controller is the component responsible for fulfilling ingress API requests.&lt;/p>
&lt;p>In this article, we will dive deeper into what an ingress is, give a brief overview of different types of ingress controllers, and examine how they are used in KubeSphere.&lt;/p></description></item><item><title>How to Deploy Kubernetes on AWS</title><link>https://openksc.github.io/blogs/aws-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/aws-kubernetes/</guid><description>&lt;p>The high availability (HA) of clusters in production environments should be taken seriously. Kubernetes and AWS EC2 instances are widely used in various production environments. However, running Kubernetes on AWS while ensuring HA can be complex for many users. In this article, we will demonstrate how KubeKey can help you easily deploy Kubernetes on AWS and ensure HA.&lt;/p>
&lt;p>To meet the HA service requirements of Kubernetes in AWS, we need to ensure the HA of kube-apiserver. You can use either of the following methods to meet the target:&lt;/p></description></item><item><title>How to Install Kubernetes the Easy Way Using KubeKey</title><link>https://openksc.github.io/blogs/install-kubernetes-using-kubekey/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/install-kubernetes-using-kubekey/</guid><description>&lt;p>As Kubernetes is the de-facto standard in container orchestration, the installation of Kubernetes has remained one of the top challenges facing Kubernetes users, especially neophytes. Apart from Kubernetes itself, they also need to figure out how to install different tools required for the installation, such as kubelet, kubeadm and kubectl. They have been wondering if there is a tool that contains all the stacks so that they can just run only few commands for the installation.&lt;/p></description></item><item><title>How to Use KubeSphere Project Gateways and Routes</title><link>https://openksc.github.io/blogs/how-to-use-kubesphere-project-gateways-and-routes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/how-to-use-kubesphere-project-gateways-and-routes/</guid><description>&lt;p>KubeSphere project gateways and Routes provide a method for aggregating Services, which allows you to expose multiple Services by using a single IP address in HTTP or HTTPS mode. You can configure routing rules by using a domain name and multiple paths in a Route. The routing rules map different paths to different Services. You can also configure options such as HTTPS offloading in a Route. Project gateways forward external requests to Services according to routing rules configured in Routes.&lt;/p></description></item><item><title>Install Kubernetes 1.22 and containerd the Easy Way</title><link>https://openksc.github.io/blogs/install-kubernetes-containerd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/install-kubernetes-containerd/</guid><description>&lt;p>&lt;img src="https://openksc.github.io/images/blogs/en/kubekey-containerd/k8s-containerd.png" alt="k8s-containerd">&lt;/p>
&lt;p>&lt;a href="https://github.com/kubesphere/kubekey" target="_blank" rel="noopener noreferrer">KubeKey&lt;/a> is a lightweight and turn-key installer that supports the installation of Kubernetes, KubeSphere and related add-ons. Writtent in Go, KubeKey enables you to set up a Kubernetes cluster within minutes. In this blog, we will install Kubernetes 1.22 and &lt;a href="https://containerd.io/" target="_blank" rel="noopener noreferrer">containerd&lt;/a> in one command with KubeKey.&lt;/p>
&lt;h2 id="step-1-prepare-a-linux-machine">Step 1: Prepare a Linux Machine&lt;/h2>
&lt;p>To get started with all-in-one installation, you only need to prepare one host according to the following requirements for hardware and operating system.&lt;/p></description></item><item><title>Install Kubernetes 1.23, containerd, and Multus CNI the Easy Way</title><link>https://openksc.github.io/blogs/install-kubernetes-containerd-multus/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/install-kubernetes-containerd-multus/</guid><description>&lt;p>&lt;img src="https://openksc.github.io/images/blogs/en/kubekey-containerd/kubernetes-containerd-banner.png" alt="k8s-containerd">&lt;/p>
&lt;p>&lt;a href="https://github.com/kubesphere/kubekey" target="_blank" rel="noopener noreferrer">KubeKey&lt;/a> is a lightweight and turn-key installer that supports the installation of Kubernetes, KubeSphere and related add-ons. Writtent in Go, KubeKey enables you to set up a Kubernetes cluster within minutes.&lt;/p>
&lt;p>Kubernetes 1.23 &lt;a href="https://kubernetes.io/blog/2021/12/07/kubernetes-1-23-release-announcement/" target="_blank" rel="noopener noreferrer">was released on Dec 7&lt;/a>. KubeKey has supported the installation of the latest version Kubernetes in its v2.0.0 alpha release, and also brought some new features such as support for Multus CNI, Feature Gates, and easy-to-use air-gapped installation, etc.&lt;/p></description></item><item><title>Integrate KubeSphere with Okta Authentication</title><link>https://openksc.github.io/blogs/integrate-okta/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/integrate-okta/</guid><description>&lt;p>KubeSphere, with &lt;a href="../kubesphere-3.2.0-ga-announcement/">its latest release of 3.2.0&lt;/a>, provides a built-in authentication service based on &lt;a href="https://openid.net/connect/" target="_blank" rel="noopener noreferrer">OpenID Connect&lt;/a> (OIDC) in addition to its support for AD/LDAP and OAuth 2.0 identity authentication systems. You can easily integrate your existing identify providers that support the OIDC standard.&lt;/p>
&lt;p>This article uses &lt;a href="https://www.okta.com/" target="_blank" rel="noopener noreferrer">Okta&lt;/a> as an example to look into the process of how to integrate KubeSphere with an OIDC identity provider.&lt;/p>
&lt;h2 id="what-is-openid-connect">What is OpenID Connect?&lt;/h2>
&lt;p>OpenID Connect (OIDC) is an identity layer built on top of the OAuth 2.0 framework. As an open authentication protocol, OIDC allows clients to verify the identity of an end user and to obtain basic user profile information.&lt;/p></description></item><item><title>KubeEye: An Automatic Diagnostic Tool that Provides a Holistic View of Your Kubernetes Cluster</title><link>https://openksc.github.io/blogs/kubeeye-automatic-cluster-diagnostic-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/kubeeye-automatic-cluster-diagnostic-tool/</guid><description>&lt;p>&lt;strong>&lt;a href="https://github.com/kubesphere/kubeeye" target="_blank" rel="noopener noreferrer">KubeEye&lt;/a> is an open-source diagnostic tool for identifying various Kubernetes cluster issues automatically, such as misconfigurations, unhealthy components and node failures&lt;/strong>. It empowers cluster operators to manage and troubleshoot clusters in a timely and graceful manner. Developed in Go on the basis of &lt;a href="https://github.com/FairwindsOps/polaris" target="_blank" rel="noopener noreferrer">Polaris&lt;/a> and &lt;a href="https://github.com/kubernetes/node-problem-detector" target="_blank" rel="noopener noreferrer">Node Problem Detector&lt;/a>, KubeEye is equipped with a series of built-in rules for exception detection. Besides pre-defined rules, KubeEye also supports customized rules.&lt;/p>
&lt;p>&lt;img src="https://openksc.github.io/images/blogs/en/kubeeye-diagnostics-tool-introduction/kubeeye-logo.png" alt="kubeeye-logo">&lt;/p></description></item><item><title>Kubernetes Backup and Restore with Kasten K10 on KubeSphere</title><link>https://openksc.github.io/blogs/kubernetes-backup-and-restore-with-kasten-k10-on-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/kubernetes-backup-and-restore-with-kasten-k10-on-kubesphere/</guid><description>&lt;h2 id="kasten-on-kubesphere">Kasten on KubeSphere&lt;/h2>
&lt;p>Purpose-built for Kubernetes, &lt;a href="https://docs.kasten.io/" target="_blank" rel="noopener noreferrer">Kasten 10 &lt;/a>provides enterprise operations teams an easy-to-use, scalable, and secure system for backup/restore, disaster recovery, and mobility of Kubernetes applications.&lt;/p>
&lt;p>&lt;img src="https://openksc.github.io/images/blogs/en/kastenk10image/KastenK10-architecture.png" alt="kasten k10 arch">
&lt;a href="https://kubesphere.io/docs/introduction/what-is-kubesphere/" target="_blank" rel="noopener noreferrer">KubeSphere&lt;/a> is a distributed operating system for cloud-native application management, using Kubernetes as its kernel. It provides a plug-and-play architecture, allowing third-party applications to be seamlessly integrated into its ecosystem.
&lt;img src="https://openksc.github.io/images/blogs/en/kastenk10image/kubesphere-architecture.png" alt="kubesphere arch">&lt;/p>
&lt;p>In this article, we will introduce the deployment of Kasten K10 on KubeSphere.&lt;/p></description></item><item><title>Kubernetes Fundamental 1: Pods, Nodes, Deployments and Ingress</title><link>https://openksc.github.io/blogs/kubernetes-fundamentals-part-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/kubernetes-fundamentals-part-1/</guid><description>&lt;p>&lt;img src="https://openksc.github.io/images/blogs/en/kubernetes-fundamentals-part-1/main-poster.png" alt="mainposter.png">&lt;/p>
&lt;p>Hi! Today we'll discuss something that everyone is familiar with if they've heard the term &amp;quot;Containers.&amp;quot; Yes, It's &amp;quot;Kubernetes&amp;quot;!!&lt;/p>
&lt;p>â€œKubernetes was born out of the necessity to make our sophisticated software more available, scalable, transportable, and deployable in small, independent modules.â€&lt;/p>
&lt;p>Kubernetes is gaining popularity as the future cloud software deployment and management standard. However, Kubernetes has a steep learning curve that comes with all of its capabilities. As a rookie, it can be tough to comprehend the concepts and core principles. There are a lot of pieces that make up the system, and determining which ones are vital for your scenario might be tough.&lt;/p></description></item><item><title>Kubernetes High Availability Essential Practices Simply Explained</title><link>https://openksc.github.io/blogs/k8s-ha-practices/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/k8s-ha-practices/</guid><description>&lt;p>The surge of containers has completely revolutionized the way we build and deploy software. Docker came along as a gift to software developers and helped enterprises gain control over their software. In addition, the introduction of microservices was also a big boost as containers allowed developers to package various microservices.&lt;/p>
&lt;p>As container adoption increases in an organization, the number of containers is surging. As a result, there comes a time for a container orchestration tool to automate the deployment, maintenance, and scaling of these containers. Thatâ€™s where Kubernetes comes in.&lt;/p></description></item><item><title>Kubernetes Multi-cluster Deployment: Kubernetes Cluster Federation and KubeSphere</title><link>https://openksc.github.io/blogs/multi-cluster-deployment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/multi-cluster-deployment/</guid><description>&lt;h2 id="scenarios-for-multi-cluster-deployment">Scenarios for Multi-cluster Deployment&lt;/h2>
&lt;p>As the container technology and Kubernetes see a surge in popularity among their users, it is not uncommon for enterprises to run multiple clusters for their business. In general, here are the main scenarios where multiple Kubernetes clusters can be adopted.&lt;/p>
&lt;h3 id="high-availability">High Availability&lt;/h3>
&lt;p>You can deploy workloads on multiple clusters by using a global VIP or DNS to send requests to corresponding backend clusters. When a cluster malfunctions or fails to handle requests, the VIP or DNS records can be transferred to a health cluster.&lt;/p></description></item><item><title>Kubernetes Multi-cluster Management and Application Deployment in Hybrid Cloud</title><link>https://openksc.github.io/blogs/kubernetes-multicluster-kubesphere/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/kubernetes-multicluster-kubesphere/</guid><description>&lt;blockquote>
&lt;p>This post introduces the development of Kubernetes multi-cluster management and existing multi-cluster solutions. It also shares how KubeSphere distributes and deploys applications in a unified manner using KubeFed in hybrid cloud for the purpose of achieving cross-region high availability and disaster recovery. Finally, it discusses the possibility of decentralized multi-cluster architecture.&lt;/p>&lt;/blockquote>
&lt;p>Before initiating KubeSphere v3.0, we made a survey in the community and found that most of the users called for multi-cluster management and application deployment in different cloud environments. To meet users' needs, we added the multi-cluster management feature in KubeSphere v3.0.&lt;/p></description></item><item><title>Kubernetes Operators You Need to Use</title><link>https://openksc.github.io/blogs/what-are-kubernetes-operators/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/what-are-kubernetes-operators/</guid><description>&lt;p>One of Kubernetesâ€™s key principles is the &lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/" target="_blank" rel="noopener noreferrer">control loop&lt;/a>, which aims to continuously match the current state of at least one Kubernetes resource type to its desired state. While this property unlocks Kubernetesâ€™s self-healing capabilities, as more complex applications are onboarded onto Kubernetes, managing and operating those applications requires features beyond just what Kubernetes provides out of the box. Such operations may include complex consensus logic for highly available, distributed databases, cross-regional failover, or automated backup and restore for critical information.&lt;/p></description></item><item><title>Kubernetes Resource Requests and Limits Deep Dive</title><link>https://openksc.github.io/blogs/understand-requests-and-limits-in-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/understand-requests-and-limits-in-kubernetes/</guid><description>&lt;p>As you create resources in a Kubernetes cluster, you may have encountered the following scenarios:&lt;/p>
&lt;ol>
&lt;li>No CPU requests or low CPU requests specified for workloads, which means more Pods &amp;quot;seem&amp;quot; to be able to work on the same node. During traffic bursts, your CPU is maxed out with a longer delay while some of your machines may have a CPU soft lockup.&lt;/li>
&lt;li>Likewise, no memory requests or low memory requests specified for workloads. Some Pods, especially those running Java business apps, will keep restarting while they can actually run normally in local tests.&lt;/li>
&lt;li>In a Kubernetes cluster, workloads are usually not scheduled evenly across nodes. In most cases, in particular, memory resources are unevenly distributed, which means some nodes can see much higher memory utilization than other nodes. As the de facto standard in container orchestration, Kubernetes should have an effective scheduler that ensures the even distribution of resources. But, is it really the case?&lt;/li>
&lt;/ol>
&lt;p>Generally, cluster administrators can do nothing but restart the cluster if the above issues happen amid traffic bursts when all of your machines hang and SSH login fails. In this article, we will dive deep into Kubernetes requests and limits by analyzing possible issues and discussing the best practices for them. If you are also interested in the underlying mechanism, you can also find the analysis from the perspective of source code. Hopefully, this article will be helpful for you to understand how Kubernetes requests and limits work, and why they can work in the expected way.&lt;/p></description></item><item><title>KubeSphere Recommendations for Responding to Apache Log4j 2 Vulnerabilities</title><link>https://openksc.github.io/blogs/apache-log4j2-vulnerability-solution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/apache-log4j2-vulnerability-solution/</guid><description>&lt;p>Apache Log4j 2 is an open-source logging tool that is used in a wide range of frameworks. Recently, Apache Log4j 2 vulnerabilities have been reported. This article provides KubeSphere users with recommendations for fixing the vulnerabilities.&lt;/p>
&lt;p>In Log4j 2, the lookup functionality allows developers to read specific environment configurations by using some protocols. However, it does not scrutinize the input during implementation, and this is where the vulnerabilities come in. A large number of Java-based applications have been affected, including Apache Solr, srping-boot-strater-log4j2, Apache Struts2, ElasticSearch, Dubbo, Redis, Logstash, Kafka, and so on. For more information, see &lt;a href="https://mvnrepository.com/artifact/org.apache.logging.log4j/log4j-core/usages?p=1" target="_blank" rel="noopener noreferrer">Log4j 2 Documentation&lt;/a>.&lt;/p></description></item><item><title>Monitoring Kubernetes Control Plane using KubeSphere</title><link>https://openksc.github.io/blogs/monitoring-k8s-control-plane/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/monitoring-k8s-control-plane/</guid><description>&lt;h2 id="introduction-to-kubernetes-control-plane">Introduction to Kubernetes Control Plane&lt;/h2>
&lt;p>In a Kubernetes cluster, there are two machines roles including master nodes and worker nodes. The master node runs the Kubernetes control plane, which is responsible for the management of the worker nodes, makes scheduling decisions, and implements changes to drive the cluster to a desired state. The worker nodes, as the name implies, they run the workloads and Pod on them.&lt;/p>
&lt;p>&lt;img src="https://pek3b.qingstor.com/kubesphere-docs/png/20200328170549.png" alt="">&lt;/p>
&lt;p>As is shown in the graph, there are four components running in the Kubernetes control plane, each of them is critical for running a healthy Kubernetes cluster, they act as the different roles within the cluster:&lt;/p></description></item><item><title>Monitoring X.509 Certificates Expiration in Kubernetes Clusters with a Prometheus Exporter</title><link>https://openksc.github.io/blogs/x509-certificate-exporter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/x509-certificate-exporter/</guid><description>&lt;p>KubeSphere offers a developer-friendly wizard that simplifies the operations &amp;amp; maintenance of Kubernetes, but it is essentially built on Kubernetes. Kubernetes' TLS certificates are valid for only one year, so we need to update the certificates every year, which is unavoidable even though the cluster is installed by the powerful and lightweight installation tool &lt;a href="https://github.com/kubesphere/kubekey" target="_blank" rel="noopener noreferrer">KubeKey&lt;/a>. To prevent possible risks arising from certificate expiration, we need to find a way to monitor certificate validity of Kubernetes components.&lt;/p></description></item><item><title>OpenELB Joins the CNCF Sandbox, Making Service Exposure in Private Environments Easier</title><link>https://openksc.github.io/blogs/openelb-joins-cncf-sandbox-project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/openelb-joins-cncf-sandbox-project/</guid><description>&lt;p>&lt;img src="https://kubesphere-community.pek3b.qingstor.com/images/4761636694917_.pic_hd.jpg" alt="Cover">&lt;/p>
&lt;p>On November 10, the Cloud Native Computing Foundation (CNCF) accepted OpenELB, a load balancer plugin open sourced by KubeSphere, into the CNCF Sandbox.&lt;/p>
&lt;p>&lt;img src="https://kubesphere-community.pek3b.qingstor.com/images/8471636692467_.pic_hd.jpg" alt="Diagram">&lt;/p>
&lt;p>OpenELB, formerly known as &amp;quot;PorterLB&amp;quot;, is a load balancer plugin designed for bare metal servers, edge devices, and private environments. It serves as an LB plugin for Kubernetes, K3s, and KubeSphere to expose LoadBalancer services to outside the cluster. OpenELB provides the following core functions:&lt;/p></description></item><item><title>OpenFunction: Build a Modern Cloud-Native Serverless Computing Platform</title><link>https://openksc.github.io/blogs/faas-openfunction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/faas-openfunction/</guid><description>&lt;p>&lt;strong>Serverless computing&lt;/strong>, commonly known as Serverless, has become a buzzword in the cloud-native field. It will fuel the next wave of development in cloud computing after IaaS and PaaS. Serverless emphasizes an architecture philosophy and service model that allows developers to focus on implementing business logics in applications rather than on managing infrastructures (e.g., servers). In its paper &lt;em>Cloud Programming Simplified: A Berkeley View on Serverless Computing&lt;/em>, the University of California at Berkeley presents two key viewpoints on Serverless:&lt;/p></description></item><item><title>Part 1: Explaining Container Runtimes: Docker, containerd and CRI-O</title><link>https://openksc.github.io/blogs/part-1-explaining-container-runtimes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/part-1-explaining-container-runtimes/</guid><description>&lt;p>&lt;img src="https://openksc.github.io/images/blogs/Part-1-Explaining-Container-Runtimes/1.png" alt="main.png">&lt;/p>
&lt;p>In this blog, we will introduce some of the upcoming updates in Kubernetesâ€™s 1.24 and different container runtimes!&lt;/p>
&lt;p>Before getting into our main topic, we need to understand what are containers?&lt;/p>
&lt;h2 id="what-are-containers">What are containers?&lt;/h2>
&lt;p>Containers virtualize the underlying infrastructure and address a major issue in application development. When we ess entially write code, the particular whole thing works properly in our development environment, but this is where issues develop when we are generally ready to move the code to production. In production, the code that worked properly on our machine does not work at all in a major way. This can happen pretty much due to different operating systems, dependencies, libraries, etc., basically contrary to popular belief. Containers solved this fundamental issue of portability by allowing you to segregate code from the infrastructure on which it runs. As long as the base image is consistent, you can deploy and run containers anywhere.&lt;/p></description></item><item><title>Part 2: How to migrate to containerd and CRI-O after Dockershim Deprecation in Kubernetes 1.24</title><link>https://openksc.github.io/blogs/part-2-how-to-migrate-to-containerd-and-cri-o-after-docker-deprecation-in-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/part-2-how-to-migrate-to-containerd-and-cri-o-after-docker-deprecation-in-kubernetes/</guid><description>&lt;p>&lt;img src="https://openksc.github.io/images/blogs/Part-2-How-to-migrate-to-containerd-and-CRI-O-after-Docker-Deprecation-in-Kubernetes/2.png" alt="main.png">&lt;/p>
&lt;p>In the last part of our series, we talked about what are CRI and OCI, differences between Docker, containerd, CRI-O and their architecture, etc. Recently, weâ€™ve got to know that Docker is going to be deprecated from kubernetes! (&lt;a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/" target="_blank" rel="noopener noreferrer">check out this article officially from kubernetes&lt;/a>) So letâ€™s talk more in depth about why is it happening so?&lt;/p>
&lt;h2 id="why-is-dokershim-deprecated-from-k8s-124">Why is dokershim deprecated from K8s 1.24?&lt;/h2>
&lt;p>In version 1.24, Kubernetes will no longer support Docker as a container runtime.
Docker is being phased out in favor of runtimes that use the Container Runtime Interface (CRI), which was built for Kubernetes.
If you're a Kubernetes end-user, you won't notice much of a difference. This does not imply that Docker is dead, nor does it imply that you can't or shouldn't use it as a development tool. Docker is still a helpful tool for creating containers, and the images generated by the &lt;code>docker build&lt;/code> may be used in your Kubernetes cluster.
If you wish to create your cluster, you'll have to make certain adjustments to avoid cluster failure. As Docker will be deprecated from K8s 1.24, you'll have to transition to one of the other compatible container runtimes, such as containerd or CRI-O. Simply ensure that the runtime you select supports the current settings of the Docker daemon (such as logging).&lt;/p></description></item><item><title>Porter: An Innovative Cloud Native Service Proxy in CNCF Landscape</title><link>https://openksc.github.io/blogs/porter-in-cncf-landscape/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/porter-in-cncf-landscape/</guid><description>&lt;p>&lt;a href="https://github.com/kubesphere/porter" target="_blank" rel="noopener noreferrer">Porter&lt;/a>, a load balancer developed for bare metal Kubernetes clusters, was officially included in CNCF Landscape last week. This marks a great milestone for its parent project KubeSphere as it continues to deliver cloud native technologies to the wider community.&lt;/p>
&lt;p>&lt;img src="https://ap3.qingstor.com/kubesphere-website/docs/cncf-include-porter.png" alt="cncf-include-porter">&lt;/p>
&lt;p>Cloud Native Computing Foundation, or CNCF, was built for the establishment of sustainable ecosystems for cloud native software. Its &lt;a href="https://landscape.cncf.io/" target="_blank" rel="noopener noreferrer">Interactive Landscape&lt;/a> is dynamically generated, serving as a technology roadmap for various industries. As Porter is now recognized by CNCF as one of the best cloud native practices, it represents another important solution to load balancing for developers. Moreover, it is expected to see a surge in popularity amid growing needs from its users faced with various challenges in this field.&lt;/p></description></item><item><title>Practical Steps to Delete Kubernetes Pods</title><link>https://openksc.github.io/blogs/delete-k8s-pods/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/delete-k8s-pods/</guid><description>&lt;p>If you are using Kubernetes to deploy containerized applications, you would probably have to delete pods from one or more worker nodes for various reasons, such as debugging node issues, upgrading nodes, or removing nodes from your cluster.&lt;/p>
&lt;p>With the kubectl commands, deleting one or more Kubernetes pods from a node is a straightforward process. In this article, let's look at how to delete pods in practice.&lt;/p>
&lt;h2 id="delete-kubernetes-pods">Delete Kubernetes Pods&lt;/h2>
&lt;h3 id="delete-a-specific-pod">Delete a specific pod&lt;/h3>
&lt;p>When you want to delete a specific pod, you should make sure that you make the deletion on the node where the pod runs.&lt;/p></description></item><item><title>Quick Deployment of High Availability Kubernetes 1.33.0 Cluster Based on KubeKey 3.1.9</title><link>https://openksc.github.io/blogs/quick-deployment-of-high-availability-kubernetes-1.33.0-cluster-based-on-kubekey-3.1.9/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/quick-deployment-of-high-availability-kubernetes-1.33.0-cluster-based-on-kubekey-3.1.9/</guid><description>&lt;h1 id="quick-deployment-of-high-availability-kubernetes-1330-cluster-using-kubekey-319">Quick Deployment of High-Availability Kubernetes 1.33.0 Cluster Using KubeKey 3.1.9&lt;/h1>
&lt;blockquote>
&lt;p>Author: Ding Xinlei, Cloud-Native Operations Engineer
Focused on deep integration of KubeSphere and Kubernetes (K8s), passionate about simplifying Kubernetes operations and enabling enterprise cloud-native transformation.&lt;/p>&lt;/blockquote>
&lt;hr>
&lt;h2 id="-compatibility-notice">ðŸŒ Compatibility Notice&lt;/h2>
&lt;ul>
&lt;li>This guide is optimized for &lt;strong>global users&lt;/strong>.&lt;/li>
&lt;li>Default deployment uses &lt;strong>public registries&lt;/strong> (&lt;code>docker.io&lt;/code>, &lt;code>quay.io&lt;/code>, &lt;code>ghcr.io&lt;/code>).&lt;/li>
&lt;li>Harbor (private registry) is &lt;strong>optional&lt;/strong>, only needed for &lt;strong>offline / air-gapped&lt;/strong> environments.&lt;/li>
&lt;li>Timezone is set to &lt;code>UTC&lt;/code>.&lt;/li>
&lt;li>NTP server is &lt;code>pool.ntp.org&lt;/code>.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ol>
&lt;li>&lt;a href="#1-background">Background&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-software-versions">Software Versions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-server-planning">Server Planning&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-host-initialization">Host Initialization&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-package-preparation">Package Preparation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-optional-harbor-setup-offline">Optional: Harbor Setup (Offline)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-kubernetes-cluster-installation">Kubernetes Cluster Installation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-kubesphere-installation">KubeSphere Installation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-conclusion">Conclusion&lt;/a>&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="1-background">1. Background&lt;/h2>
&lt;h3 id="11-kubekey-319-updates">1.1 KubeKey 3.1.9 Updates&lt;/h3>
&lt;ul>
&lt;li>Support for Kubernetes 1.33.0&lt;/li>
&lt;li>Bug fixes:
&lt;ul>
&lt;li>kubelet cgroup configuration&lt;/li>
&lt;li>UFW and IPVS issues&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="12-kubernetes-1330-highlights">1.2 Kubernetes 1.33.0 Highlights&lt;/h3>
&lt;ul>
&lt;li>In-place vertical scaling&lt;/li>
&lt;li>Sidecar GA&lt;/li>
&lt;li>Indexed Jobs GA&lt;/li>
&lt;li>Improved ServiceAccount token security&lt;/li>
&lt;li>&lt;code>kubectl&lt;/code> subresource support&lt;/li>
&lt;li>Dynamic Service CIDR expansion&lt;/li>
&lt;li>Enhanced User Namespaces&lt;/li>
&lt;li>OCI image mounting&lt;/li>
&lt;li>Ordered namespace deletion&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="2-software-versions">2. Software Versions&lt;/h2>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>Component&lt;/th>
 &lt;th>Version&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>OS&lt;/td>
 &lt;td>openEuler 22.03 (LTS-SP3) amd64&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>Docker&lt;/td>
 &lt;td>24.0.9&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>Kubernetes&lt;/td>
 &lt;td>v1.33.0&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>KubeSphere&lt;/td>
 &lt;td>v4.1.3&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>KubeKey&lt;/td>
 &lt;td>v3.1.9&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;hr>
&lt;h2 id="3-server-planning">3. Server Planning&lt;/h2>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>IP Address&lt;/th>
 &lt;th>Hostname&lt;/th>
 &lt;th>Role&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>192.168.118.180&lt;/td>
 &lt;td>k8s-master1&lt;/td>
 &lt;td>master&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>192.168.118.181&lt;/td>
 &lt;td>k8s-node01&lt;/td>
 &lt;td>worker&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>192.168.118.182&lt;/td>
 &lt;td>k8s-node02&lt;/td>
 &lt;td>worker&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;hr>
&lt;h2 id="4-host-initialization">4. Host Initialization&lt;/h2>
&lt;h3 id="41-configure-static-ip">4.1 Configure Static IP&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>vim /etc/sysconfig/network-scripts/ifcfg-ens33
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>TYPE&lt;span style="color:#f92672">=&lt;/span>Ethernet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PROXY_METHOD&lt;span style="color:#f92672">=&lt;/span>none
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BROWSER_ONLY&lt;span style="color:#f92672">=&lt;/span>no
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BOOTPROTO&lt;span style="color:#f92672">=&lt;/span>static
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IPADDR&lt;span style="color:#f92672">=&lt;/span>192.168.118.180
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NETMASK&lt;span style="color:#f92672">=&lt;/span>255.255.255.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>GATEWAY&lt;span style="color:#f92672">=&lt;/span>192.168.118.2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS1&lt;span style="color:#f92672">=&lt;/span>192.168.118.2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DEFROUTE&lt;span style="color:#f92672">=&lt;/span>yes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IPV4_FAILURE_FATAL&lt;span style="color:#f92672">=&lt;/span>no
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IPV6INIT&lt;span style="color:#f92672">=&lt;/span>yes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IPV6_AUTOCONF&lt;span style="color:#f92672">=&lt;/span>yes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IPV6_DEFROUTE&lt;span style="color:#f92672">=&lt;/span>yes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IPV6_FAILURE_FATAL&lt;span style="color:#f92672">=&lt;/span>no
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IPV6_ADDR_GEN_MODE&lt;span style="color:#f92672">=&lt;/span>stable-privacy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME&lt;span style="color:#f92672">=&lt;/span>ens33
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DEVICE&lt;span style="color:#f92672">=&lt;/span>ens33
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ONBOOT&lt;span style="color:#f92672">=&lt;/span>yes
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="42-disable-selinux">4.2 Disable SELinux&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>sed -i &lt;span style="color:#e6db74">&amp;#39;s/SELINUX=enforcing/SELINUX=disabled/g&amp;#39;&lt;/span> /etc/selinux/config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>setenforce &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="43-disable-swap-for-performance-improvement">4.3 Disable Swap for Performance Improvement&lt;/h3>
&lt;pre tabindex="0">&lt;code>swapoff -a
vim /etc/fstab
&lt;/code>&lt;/pre>&lt;h3 id="44-disable-firewalld">4.4 Disable Firewalld&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>systemctl stop firewalld
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>systemctl disable firewalld
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="45-generate-config-file">4.5 Generate config file&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kk create config
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It will generate a default configuration file as shown here: &lt;a href="https://github.com/kubesphere/kubekey/blob/master/docs/config-example.md" target="_blank" rel="noopener noreferrer">https://github.com/kubesphere/kubekey/blob/master/docs/config-example.md&lt;/a>.&lt;/p></description></item><item><title>Restart a Kubernetes Cluster in a Practical Way</title><link>https://openksc.github.io/blogs/restart-k8s-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/restart-k8s-cluster/</guid><description>&lt;p>As cloud-native technologies continue to gain momentum, developers are focusing more on transforming conventional applications into cloud-native applications, hoping to take advantage of the flexibility and scalability that cloud-native technologies like Kubernetes offer.&lt;/p>
&lt;p>Powerful as Kubernetes is, it could still bring difficulties in practice. For example, it might be a puzzle when it comes to restarting a Kubernetes cluster. In this article, we'll look into how to restart a Kubernetes cluster in a practical way.&lt;/p></description></item><item><title>Scaling a Kubernetes Cluster: One of the Best Practices for Using KubeKey</title><link>https://openksc.github.io/blogs/scale-kubernetes-cluster-using-kubekey/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/scale-kubernetes-cluster-using-kubekey/</guid><description>&lt;p>In my &lt;a href="https://kubesphere.io/blogs/install-kubernetes-using-kubekey/" target="_blank" rel="noopener noreferrer">last post&lt;/a>, I demonstrated how to set up a three-node Kubernetes cluster using &lt;a href="https://github.com/kubesphere/kubekey" target="_blank" rel="noopener noreferrer">KubeKey&lt;/a>. As I mentioned in the article, KubeKey is a lightweight, powerful installer that is able to install Kubernetes as well as its related add-ons in a quick and convenient way. In fact, KubeKey can do way more than that as it is also an efficient tool to scale your Kubernetes cluster.&lt;/p>
&lt;p>On some cloud platforms, you can directly scale your cluster by increasing or decreasing the number of nodes. Usually, this does not entail complex operations as these platforms will do almost everything for you and you only need to click a few buttons. However, in some on-premises environments, you may need to manually change the number of nodes. In this article, I am going to demonstrate how to scale out and scale in your cluster using KubeKey. The steps are listed as follows:&lt;/p></description></item><item><title>Serverless Use Case: Elastic Kubernetes Log Alerts with OpenFunction and Kafka</title><link>https://openksc.github.io/blogs/serverless-way-for-kubernetes-log-alert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/serverless-way-for-kubernetes-log-alert/</guid><description>&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>How do you handle container logs collected by the message server? You may face a dilemma: Deploying a dedicated log processing workload can be costly, and it is difficult to assess the number of standby log processing workloads required when the quantity of logs fluctuates sharply. This blog post offers ideas for serverless log processing, which reduces the link cost while improving flexibility.&lt;/p>
&lt;p>Our general design idea is to add a Kafka server as a log receiver, and then use the log input to the Kafka server as an event to drive the serverless workloads to handle logs. Roughly, the following steps are involved:&lt;/p></description></item><item><title>Serverless vs. Function-as-a-Service (FaaS): Which One to Choose?</title><link>https://openksc.github.io/blogs/serverless-vs-faas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/serverless-vs-faas/</guid><description>&lt;p>The constantly-expanding cloud computing landscape has encompassed many emerging technologies. Youâ€™ve probably heard of serverless and Function-as-a-Service (FaaS) if you are a practitioner in the cloud computing field. These two concepts are sometimes used interchangeably. However, we should also be aware of the differences between them so that we can choose the one that better suits our needs.&lt;/p>
&lt;p>In this article, we will take a look at these two technology concepts to figure out how to pick the better one for yourself.&lt;/p></description></item><item><title>TiDB on KubeSphere: Release a Cloud-Native Distributed Database to the KubeSphere App Store</title><link>https://openksc.github.io/blogs/tidb-on-kubesphere-upload-helm-chart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/tidb-on-kubesphere-upload-helm-chart/</guid><description>&lt;p>&lt;a href="https://kubesphere.io/blogs/tidb-on-kubesphere-using-qke/" target="_blank" rel="noopener noreferrer">My last blog&lt;/a> talked about how to deploy TiDB Operator and a TiDB cluster on KubeSphere. After you add an app repository to KubeSphere, apps within the repository are provided as app templates on the &lt;a href="https://kubesphere.io/" target="_blank" rel="noopener noreferrer">container platform&lt;/a>. Tenants in the same workspace can deploy these app templates if they have necessary permissions. However, if you want these apps to be available to all workspace tenants, I recommend you release apps to the public repository of KubeSphere, also known as the KubeSphere App Store.&lt;/p></description></item><item><title>TiDB on KubeSphere: Using Cloud-Native Distributed Database on Kubernetes Platform Tailored for Hybrid Cloud</title><link>https://openksc.github.io/blogs/tidb-on-kubesphere-using-qke/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/tidb-on-kubesphere-using-qke/</guid><description>&lt;p>&lt;img src="https://ap3.qingstor.com/kubesphere-website/docs/20201028212049.png" alt="TiDB on KubeSphere">&lt;/p>
&lt;p>In a world where Kubernetes has become the de facto standard to build application services that span multiple containers, running a cloud-native distributed database represents an important part of the experience of using Kubernetes. In this connection, &lt;a href="https://docs.pingcap.com/tidb/dev/overview" target="_blank" rel="noopener noreferrer">TiDB&lt;/a>, a cloud-native, open-source NewSQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads, meets those needs admirably. Its architecture is suitable for Kubernetes, and it is MySQL compatible. TiDB also features horizontal scalability, strong consistency, and high availability.&lt;/p></description></item><item><title>Transform Traditional Applications into Microservices to Enable the Traffic Monitoring Feature</title><link>https://openksc.github.io/blogs/transform-traditional-applications-into-microservices/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/transform-traditional-applications-into-microservices/</guid><description>&lt;h2 id="challenges">Challenges&lt;/h2>
&lt;p>When trying to use service mesh of Kubernetes, most of KubeSphere users only manage to deploy a Bookinfo sample on KubeSphere. They are also struggling in understanding how to explore the full capabilities of service mesh, let alone transform traditional applications into microservices.&lt;/p>
&lt;p>This article describes how to transform a traditional application into microservices to use service mesh features, such as grayscale release, traffic monitoring, and tracing.&lt;/p>
&lt;h2 id="kubesphere-microservices">KubeSphere Microservices&lt;/h2>
&lt;p>KubeSphere microservices use the application CRD to abstract associated resources into a concrete application, and provide traffic monitoring, grayscale release, and tracing features with the help of Istio's application features. Moreover, KubeSphere microservices shield complex DestinationRule and VirtualService of Istio and automatically update resources according to traffic monitoring settings and grayscale release policies.&lt;/p></description></item><item><title>Use KubeKey to Set up a Kubernetes and KubeSphere Cluster with NFS Storage</title><link>https://openksc.github.io/blogs/install-nfs-server-client-for-kubesphere-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/install-nfs-server-client-for-kubesphere-cluster/</guid><description>&lt;p>In my previous articles, I talked about how to use &lt;a href="https://github.com/kubesphere/kubekey" target="_blank" rel="noopener noreferrer">KubeKey&lt;/a> to set up and scale a Kubernetes cluster. As you may already know, KubeKey can do way more than that. You can also use KubeKey to install KubeSphere, a &lt;a href="https://kubesphere.io/" target="_blank" rel="noopener noreferrer">container platform&lt;/a> running on top of Kubernetes with streamlined DevOps workflows, unified multi-cluster management and more. Besides, KubeKey is able to install cloud-native add-ons by Chart or YAML files.&lt;/p>
&lt;p>Among other things, storage represents one of, if not the most important element as you set up a Kubernetes cluster. Kubernetes itself supports multiple storage solutions, such as NFS-client, Ceph CSI and GlusterFS (in-tree). In this article, I am going to show you how to use KubeKey to create a Kubernetes and KubeSphere cluster with &lt;a href="https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client" target="_blank" rel="noopener noreferrer">NFS-client Provisioner&lt;/a> providing external storage.&lt;/p></description></item><item><title>Use KubeSphere DevOps to Build an Automated Testing System</title><link>https://openksc.github.io/blogs/devops-automatic-testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/devops-automatic-testing/</guid><description>&lt;h2 id="layers-of-testing">Layers of Testing&lt;/h2>
&lt;p>The purpose of testing is to verify expected functionality and find potential defects. Testing enhances the confidence in delivering qualified products and also brings the possibility of agile iteration. It is safe to say that testing determines the product development progress.&lt;/p>
&lt;p>As you may already know, in networking, there are 7 layers of OSI model and 4 layers of TCP model. In development, we also have different patterns such as MTV, MVC, MVP, and MVVM. These technologies have become more mature featuring high cohesion, low coupling, and division of responsibilities, modules, and layers. They have helped us build architectures and standards.&lt;/p></description></item><item><title>Visualize Network Traffic: A Simple Way to Enable Cilium on Kubernetes</title><link>https://openksc.github.io/blogs/cilium-as-cni/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/cilium-as-cni/</guid><description>&lt;h2 id="what-is-cilium">What Is Cilium&lt;/h2>
&lt;p>&lt;a href="https://cilium.io/" target="_blank" rel="noopener noreferrer">Cilium&lt;/a> is an open-source project focusing on container network. It can be deployed on container platforms to transparently secure the network connection and load balancing between application workloads, such as application containers or processes.&lt;/p>
&lt;p>Running on Layer 3 and Layer 4, Cilium provides conventional network and security services. It also runs on Layer 7 to secure the use of modern application protocols, such as HTTP, gRPC, and Kafka. Cilium can be integrated into popular container orchestration frameworks such as Kubernetes and Mesos.&lt;/p></description></item><item><title>What is Helm in Kubernetes?</title><link>https://openksc.github.io/blogs/what-is-helm-in-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/what-is-helm-in-kubernetes/</guid><description>&lt;p>Helm is one of the most popular ways to package and share applications built for Kubernetes. Since joining the &lt;a href="https://www.cncf.io/" target="_blank" rel="noopener noreferrer">Cloud Native Computing Foundation (CNCF)&lt;/a> in 2016, Helm has become an important component in the Kubernetes ecosystem. In fact, in &lt;a href="https://www.cncf.io/wp-content/uploads/2020/11/CNCF_Survey_Report_2020.pdf" target="_blank" rel="noopener noreferrer">CNCF Survey 2020&lt;/a>, over 60% of respondents reported Helm as the preferred method for packaging Kubernetes applications. This popularity is also backed by the fact that most cloud-native CI/CD tools as well as open-source Kubernetes tools all support Helm as one way to deploy and manage their applications. In this article, weâ€™ll dive into the architecture, key concepts, and example usage of Helm.&lt;/p></description></item><item><title>Will Cloud-Native WebAssembly Replace Docker?</title><link>https://openksc.github.io/blogs/will-cloud-native-webassembly-replace-docker_/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openksc.github.io/blogs/will-cloud-native-webassembly-replace-docker_/</guid><description>&lt;p>Holding a brand new form, WebAssembly excels in its portability, small size, fast loading, and compatibility. Given its good security, compatibility, high efficiency, and lightweight, &lt;a href="https://webassembly.org/" target="_blank" rel="noopener noreferrer">WebAssembly&lt;/a> is an ideal technology for applications based on sandbox schemes. It has got noticed from communities of containers, function computing, IoT, and edge computing. What is WebAssembly? Is it capable of replacing Docker?&lt;/p>
&lt;p>This article is based on the share by Michael Yuan, the maintainer of &lt;a href="https://github.com/WasmEdge/WasmEdge" target="_blank" rel="noopener noreferrer">WasmEdge&lt;/a>(project under CNCF now), in the 2020 KubeSphere Meetup.&lt;/p></description></item></channel></rss>